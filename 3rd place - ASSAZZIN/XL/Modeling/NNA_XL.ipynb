{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 10277.819658,
      "end_time": "2021-09-29T08:37:54.584739",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-29T05:46:36.765081",
      "version": "2.3.3"
    },
    "colab": {
      "name": "NNA_XL.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc40ed0",
        "papermill": {
          "duration": 0.044827,
          "end_time": "2021-09-29T05:46:43.471126",
          "exception": false,
          "start_time": "2021-09-29T05:46:43.426299",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# SETUP"
      ],
      "id": "acc40ed0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LeWRivHps4k"
      },
      "source": [
        "# NOTE: Please Run this on  Tesla P100-PCIE [ 16280 Mib ]"
      ],
      "id": "2LeWRivHps4k"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:46:43.565588Z",
          "iopub.status.busy": "2021-09-29T05:46:43.564118Z",
          "iopub.status.idle": "2021-09-29T05:46:44.361428Z",
          "shell.execute_reply": "2021-09-29T05:46:44.360503Z",
          "shell.execute_reply.started": "2021-09-29T05:33:53.906505Z"
        },
        "papermill": {
          "duration": 0.844919,
          "end_time": "2021-09-29T05:46:44.361597",
          "exception": false,
          "start_time": "2021-09-29T05:46:43.516678",
          "status": "completed"
        },
        "tags": [],
        "id": "9146fab1",
        "outputId": "43038fa1-5d31-4628-9fad-da288d288b91"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "9146fab1",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Wed Sep 29 05:46:44 2021       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   33C    P0    25W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|  No running processes found                                                 |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5GTu7SrSpu3o"
      },
      "source": [
        "!pip install -r requirements_kaggle.txt -q"
      ],
      "id": "5GTu7SrSpu3o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EjK1ZY-l9pVa"
      },
      "source": [
        "> To speed up the review process , i provided the ***drive id*** of the data i've created from the Train creation folder noteboooks .\n",
        "---\n",
        "> I  also add each data drive link in the Readme Pdf file attached with this solution"
      ],
      "id": "EjK1ZY-l9pVa"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:46:44.463015Z",
          "iopub.status.busy": "2021-09-29T05:46:44.462240Z",
          "iopub.status.idle": "2021-09-29T05:47:03.449160Z",
          "shell.execute_reply": "2021-09-29T05:47:03.448592Z",
          "shell.execute_reply.started": "2021-09-29T05:34:16.100720Z"
        },
        "papermill": {
          "duration": 19.041918,
          "end_time": "2021-09-29T05:47:03.449286",
          "exception": false,
          "start_time": "2021-09-29T05:46:44.407368",
          "status": "completed"
        },
        "tags": [],
        "id": "37cb9962",
        "outputId": "1aa69c20-249c-45db-ce5a-e0084580d80a"
      },
      "source": [
        "!pip install -q gdown"
      ],
      "id": "37cb9962",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:03.557856Z",
          "iopub.status.busy": "2021-09-29T05:47:03.557293Z",
          "iopub.status.idle": "2021-09-29T05:47:32.307118Z",
          "shell.execute_reply": "2021-09-29T05:47:32.306518Z",
          "shell.execute_reply.started": "2021-09-29T05:34:34.289447Z"
        },
        "papermill": {
          "duration": 28.811774,
          "end_time": "2021-09-29T05:47:32.307260",
          "exception": false,
          "start_time": "2021-09-29T05:47:03.495486",
          "status": "completed"
        },
        "tags": [],
        "id": "c960ed16"
      },
      "source": [
        "!gdown --id 1-Q7vaYnSEMwQ3jOrm3KwkcqWNI8iEzXn\n",
        "!gdown --id 1-cl7HvhPuIywmu8lPaK6B1Qb4M6RCIxb\n",
        "!gdown --id 1hNRbtcqd9F6stMOK1xAZApDITwAjiSDJ\n",
        "!gdown --id 1-QCmWsNGREXuWArifN0nD_Sp4hJxf0tu"
      ],
      "id": "c960ed16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:32.422988Z",
          "iopub.status.busy": "2021-09-29T05:47:32.418654Z",
          "iopub.status.idle": "2021-09-29T05:47:48.767601Z",
          "shell.execute_reply": "2021-09-29T05:47:48.767129Z",
          "shell.execute_reply.started": "2021-09-29T05:35:04.167348Z"
        },
        "papermill": {
          "duration": 16.408707,
          "end_time": "2021-09-29T05:47:48.767731",
          "exception": false,
          "start_time": "2021-09-29T05:47:32.359024",
          "status": "completed"
        },
        "tags": [],
        "id": "c911e3b8"
      },
      "source": [
        "!gdown --id 10-rLRxkX5Lyf92oM0xtUz1yaTXN3FBGO\n",
        "!gdown --id 1-9IVDUWu6lhY-DJnfzrjrJgQkafxKS8f\n",
        "!gdown --id 1-47L_1NKLeVgW1vWmqXXXCuWZ3gwZWsS\n",
        "!gdown --id 1-aO4FEtv5CF-ZOcxDSO3jGEzPcIFdxgP"
      ],
      "id": "c911e3b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:48.886560Z",
          "iopub.status.busy": "2021-09-29T05:47:48.886023Z",
          "iopub.status.idle": "2021-09-29T05:48:15.872715Z",
          "shell.execute_reply": "2021-09-29T05:48:15.872240Z",
          "shell.execute_reply.started": "2021-09-29T05:35:20.268672Z"
        },
        "papermill": {
          "duration": 27.052166,
          "end_time": "2021-09-29T05:48:15.872858",
          "exception": false,
          "start_time": "2021-09-29T05:47:48.820692",
          "status": "completed"
        },
        "tags": [],
        "id": "e99f42ee"
      },
      "source": [
        "!gdown --id 1-zEJ24ORxrK-RXxxoj770VNHOnGpB0dv\n",
        "!gdown --id 1-_GnYjOVN8qUB8k1alLBnf3p8_8-GCJj\n",
        "!gdown --id 1-8J_xFgI0WKT5UXFnfH4q1KUw_KgNY37\n",
        "!gdown --id 1-a55a7N6a4SoqolPF_wI4C6Q70u_d7Hj"
      ],
      "id": "e99f42ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:48:27.606024Z",
          "iopub.status.busy": "2021-09-29T05:48:27.601042Z",
          "iopub.status.idle": "2021-09-29T05:48:54.178968Z",
          "shell.execute_reply": "2021-09-29T05:48:54.178406Z",
          "shell.execute_reply.started": "2021-09-29T05:35:56.055800Z"
        },
        "papermill": {
          "duration": 26.648835,
          "end_time": "2021-09-29T05:48:54.179100",
          "exception": false,
          "start_time": "2021-09-29T05:48:27.530265",
          "status": "completed"
        },
        "tags": [],
        "id": "c0b89fc9"
      },
      "source": [
        "!gdown --id 1-kzjYxxLepIC8slA9WryJe3ahShANXzt\n",
        "!gdown --id 1-_-Ps0FoxEWEVCrdYSrCUTLPAFK5CFcs\n",
        "!gdown --id 1-BgXQwmXqBuk_P8VtvLfdLqy83dv56Kz\n",
        "!gdown --id 1-hQGF2TNBbsy3jsGNtndmK55egbdFDjs"
      ],
      "id": "c0b89fc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38e50227",
        "papermill": {
          "duration": 0.069612,
          "end_time": "2021-09-29T05:49:44.573383",
          "exception": false,
          "start_time": "2021-09-29T05:49:44.503771",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# LIBRARIES"
      ],
      "id": "38e50227"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:44.719111Z",
          "iopub.status.busy": "2021-09-29T05:49:44.718298Z",
          "iopub.status.idle": "2021-09-29T05:49:49.733238Z",
          "shell.execute_reply": "2021-09-29T05:49:49.732694Z",
          "shell.execute_reply.started": "2021-09-29T05:37:19.206869Z"
        },
        "papermill": {
          "duration": 5.090552,
          "end_time": "2021-09-29T05:49:49.733371",
          "exception": false,
          "start_time": "2021-09-29T05:49:44.642819",
          "status": "completed"
        },
        "tags": [],
        "id": "0fa6bc39",
        "outputId": "74d2fdc1-1879-4f95-a4ba-4381b722e52a"
      },
      "source": [
        "#import necessary dependecies\n",
        "import os\n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm \n",
        "import copy\n",
        "\n",
        "from sklearn.preprocessing import  LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "# tf \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate , Input ,concatenate ,add\n",
        "from tensorflow.keras.models import Sequential , Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.initializers import glorot_normal , l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# fix seed\n",
        "tf.random.set_seed(111)\n",
        "np.random.seed(111)\n",
        "random.seed(111)"
      ],
      "id": "0fa6bc39",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-29 05:49:46.178613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c5bff71",
        "papermill": {
          "duration": 0.068251,
          "end_time": "2021-09-29T05:49:49.872081",
          "exception": false,
          "start_time": "2021-09-29T05:49:49.803830",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Train Creation"
      ],
      "id": "3c5bff71"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.017797Z",
          "iopub.status.busy": "2021-09-29T05:49:50.017205Z",
          "iopub.status.idle": "2021-09-29T05:49:50.020564Z",
          "shell.execute_reply": "2021-09-29T05:49:50.021004Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.466881Z"
        },
        "id": "72195d39",
        "papermill": {
          "duration": 0.080916,
          "end_time": "2021-09-29T05:49:50.021134",
          "exception": false,
          "start_time": "2021-09-29T05:49:49.940218",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def create_train():\n",
        "  train =pd.read_csv(\"S2TrainObs1.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs1.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "72195d39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.164557Z",
          "iopub.status.busy": "2021-09-29T05:49:50.164048Z",
          "iopub.status.idle": "2021-09-29T05:49:50.167259Z",
          "shell.execute_reply": "2021-09-29T05:49:50.167639Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.487108Z"
        },
        "papermill": {
          "duration": 0.076427,
          "end_time": "2021-09-29T05:49:50.167780",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.091353",
          "status": "completed"
        },
        "tags": [],
        "id": "0de1ee06"
      },
      "source": [
        "def create_test():\n",
        "  test =pd.read_csv(\"S2TestObs1.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs1.csv\" )\n",
        "  return  pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "0de1ee06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.310623Z",
          "iopub.status.busy": "2021-09-29T05:49:50.309868Z",
          "iopub.status.idle": "2021-09-29T05:49:50.311830Z",
          "shell.execute_reply": "2021-09-29T05:49:50.312272Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.504827Z"
        },
        "papermill": {
          "duration": 0.076141,
          "end_time": "2021-09-29T05:49:50.312391",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.236250",
          "status": "completed"
        },
        "tags": [],
        "id": "0769b007"
      },
      "source": [
        "def createObs2_train():\n",
        "  train =pd.read_csv(\"S2TrainObs2.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs2.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "0769b007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.457237Z",
          "iopub.status.busy": "2021-09-29T05:49:50.456391Z",
          "iopub.status.idle": "2021-09-29T05:49:50.458520Z",
          "shell.execute_reply": "2021-09-29T05:49:50.458955Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.527947Z"
        },
        "papermill": {
          "duration": 0.079916,
          "end_time": "2021-09-29T05:49:50.459086",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.379170",
          "status": "completed"
        },
        "tags": [],
        "id": "785e1fe8"
      },
      "source": [
        "def createObs2_test():\n",
        "  test =pd.read_csv(\"S2TestObs2.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs2.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "785e1fe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.611725Z",
          "iopub.status.busy": "2021-09-29T05:49:50.610582Z",
          "iopub.status.idle": "2021-09-29T05:49:50.612948Z",
          "shell.execute_reply": "2021-09-29T05:49:50.613479Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.538500Z"
        },
        "papermill": {
          "duration": 0.082047,
          "end_time": "2021-09-29T05:49:50.613629",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.531582",
          "status": "completed"
        },
        "tags": [],
        "id": "1fe4205f"
      },
      "source": [
        "def createObs3_train():\n",
        "  train =pd.read_csv(\"S2TrainObs3.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs3.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "1fe4205f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.759023Z",
          "iopub.status.busy": "2021-09-29T05:49:50.757441Z",
          "iopub.status.idle": "2021-09-29T05:49:50.759724Z",
          "shell.execute_reply": "2021-09-29T05:49:50.760140Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.560105Z"
        },
        "papermill": {
          "duration": 0.075295,
          "end_time": "2021-09-29T05:49:50.760261",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.684966",
          "status": "completed"
        },
        "tags": [],
        "id": "607489b4"
      },
      "source": [
        "def createObs3_test():\n",
        "  test =pd.read_csv(\"S2TestObs3.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs3.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "607489b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.903576Z",
          "iopub.status.busy": "2021-09-29T05:49:50.902117Z",
          "iopub.status.idle": "2021-09-29T05:49:50.904242Z",
          "shell.execute_reply": "2021-09-29T05:49:50.904649Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.572479Z"
        },
        "papermill": {
          "duration": 0.077087,
          "end_time": "2021-09-29T05:49:50.904783",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.827696",
          "status": "completed"
        },
        "tags": [],
        "id": "b44c406a"
      },
      "source": [
        "def createObs4_train():\n",
        "  train =pd.read_csv(\"S2TrainObs4.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs4.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "b44c406a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.046418Z",
          "iopub.status.busy": "2021-09-29T05:49:51.045740Z",
          "iopub.status.idle": "2021-09-29T05:49:51.047953Z",
          "shell.execute_reply": "2021-09-29T05:49:51.048324Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.593332Z"
        },
        "papermill": {
          "duration": 0.076241,
          "end_time": "2021-09-29T05:49:51.048446",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.972205",
          "status": "completed"
        },
        "tags": [],
        "id": "f9661328"
      },
      "source": [
        "def createObs4_test():\n",
        "  test =pd.read_csv(\"S2TestObs4.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs4.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "f9661328",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33868ef5",
        "papermill": {
          "duration": 0.066967,
          "end_time": "2021-09-29T05:49:51.184266",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.117299",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Feature Engineering"
      ],
      "id": "33868ef5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.359660Z",
          "iopub.status.busy": "2021-09-29T05:49:51.354106Z",
          "iopub.status.idle": "2021-09-29T05:49:51.363494Z",
          "shell.execute_reply": "2021-09-29T05:49:51.363988Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.616268Z"
        },
        "id": "288161bc",
        "papermill": {
          "duration": 0.111819,
          "end_time": "2021-09-29T05:49:51.364121",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.252302",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def process(T) :\n",
        "\n",
        "  # process bands\n",
        "  Bcols = T.filter(like='B').columns.tolist()\n",
        "  Vcols = T.filter(like='V').columns.tolist()\n",
        "  Obs1 = T.filter(like='Month4').columns.tolist()\n",
        "  Obs2 = T.filter(like='Month5').columns.tolist()\n",
        "  Obs3 = T.filter(like='Month6').columns.tolist()\n",
        "  Obs4 = T.filter(like='Month7').columns.tolist()\n",
        "  Obs5 = T.filter(like='Month8').columns.tolist()\n",
        "  Obs6 = T.filter(like='Month9').columns.tolist()\n",
        "  Obs7 = T.filter(like='Month10').columns.tolist()\n",
        "  Obs8 = T.filter(like='Month11').columns.tolist()\n",
        "\n",
        "\n",
        "  # vegetation indexes \n",
        "  B8cols = T.filter(like='B8_').columns.tolist()\n",
        "  B8cols = [x for x in B8cols if 'std' not in x]\n",
        "  \n",
        "  B4cols = T.filter(like='B4_').columns.tolist()\n",
        "  B4cols = [x for x in B4cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B5cols = T.filter(like='B5_').columns.tolist()\n",
        "  B5cols = [x for x in B5cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B2cols = T.filter(like='B2_').columns.tolist()\n",
        "  B2cols = [x for x in B2cols if 'std' not in x]\n",
        "\n",
        "  B7cols = T.filter(like='B7_').columns.tolist()\n",
        "  B7cols = [x for x in B7cols if 'std' not in x]\n",
        "\n",
        "  B8Acols = T.filter(like='B8A_').columns.tolist()\n",
        "  B8Acols = [x for x in B8Acols if 'std' not in x]\n",
        "\n",
        "  B6cols = T.filter(like='B6_').columns.tolist()\n",
        "  B6cols = [x for x in B6cols if 'std' not in x]\n",
        "  \n",
        "  B12cols = T.filter(like='B12_').columns.tolist()\n",
        "  B12cols = [x for x in B12cols if 'std' not in x]\n",
        "\n",
        "  B11cols = T.filter(like='B11_').columns.tolist()\n",
        "  B11cols = [x for x in B11cols if 'std' not in x]\n",
        "\n",
        "  B1cols = T.filter(like='B1_').columns.tolist()\n",
        "  B1cols = [x for x in B1cols if 'std' not in x]\n",
        "\n",
        "  B9cols = T.filter(like='B9_').columns.tolist()\n",
        "  B9cols = [x for x in B9cols if 'std' not in x]\n",
        "\n",
        "  L = 0.725\n",
        "  for b1,b2 ,b3 ,b4, b5 , b6, b7, b8 ,b8a ,b9,b11,b12 in zip(B1cols,B2cols,B3cols,B4cols,B5cols,B6cols,B7cols,B8cols,B8Acols,B9cols,B11cols,B12cols) :\n",
        "    T[f'NDVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4])).values.clip(-5,5)\n",
        "    T[f'SAVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4]+L) * (1.0 + L)).values.clip(-5,5)\n",
        "    # T[f'CCCI_{b8.split(\"_\")[1]}']   =  ((T[b8] - T[b5]) /  (T[b8] + T[b5])) / ((T[b8] - T[b4]) /  (T[b8] + T[b4]))\n",
        "    T[f'GRNDVI_{b8.split(\"_\")[1]}'] = ((T[b8] - (T[b3]+T[b4])) /  (T[b8] + (T[b3]+T[b4]))).values.clip(-5,5)\n",
        "    T[f'GNDVI_{b8.split(\"_\")[1]}']  = ((T[b8] - T[b3] ) /  (T[b8] + T[b3])).values.clip(-5,5)\n",
        "    # T[f'WDRVI_{b8.split(\"_\")[1]}']  = (0.1 * T[b5] - T[b4])/ (0.1 * T[b5] + T[b4])\n",
        "    T[f'NDRE_{b8.split(\"_\")[1]}']   = ((T[b5] - T[b4])/ (T[b5] + T[b4])).values.clip(-5,5)\n",
        "    T[f'EVI_{b8.split(\"_\")[1]}']    = (2.5 * (T[b8]  - T[b4] ) / ((T[b8]  + 6.0 * T[b4]  - 7.5 * T[b2]) + 1.0)).values.clip(min=-5,max=5)\n",
        "    T[f'WDRVI_{b8.split(\"_\")[1]}']  = (((8 * T[b8]) - T[b4])/ ((8* T[b8]) + T[b4])).values.clip(-5,5)\n",
        "    T[f'ExBlue_{b8.split(\"_\")[1]}']  = ((2 * T[b2]) - (T[b3]+T[b4]))\n",
        "    T[f'ExGreen_{b8.split(\"_\")[1]}']  = ((2 * T[b3]) - (T[b2]+T[b4]) )\n",
        "    T[f'NDRE7_{b8.split(\"_\")[1]}']   = ((T[b7] - T[b4])/ (T[b7] + T[b4])).values.clip(-5,5)\n",
        "    T[f'MTCI_{b8.split(\"_\")[1]}']   = ((T[b8a] - T[b6])/ (T[b7] + T[b6])).values.clip(-5,5)\n",
        "    T[f'VARI_{b8.split(\"_\")[1]}']   = ((T[b3] - T[b4])/ (T[b3] + T[b4] - T[b2])).values.clip(-5,5)\n",
        "    T[f'b3b1_{b8.split(\"_\")[1]}']  = (T[b3] - T[b1])/ (T[b3] + T[b1])    # B7  / B3\n",
        "    T[f'b11b8_{b8.split(\"_\")[1]}']  = (T[b11] - T[b8])/ (T[b11] + T[b8])    # B7  / B3\n",
        "    T[f'b12b11_{b8.split(\"_\")[1]}']  = (T[b12] - T[b11])/ (T[b12] + T[b11])    # B7  / B3\n",
        "    T[f'b3b4_{b8.split(\"_\")[1]}']  = (T[b3] - T[b4])/ (T[b3] + T[b4])    # B7  / B3\n",
        "    T[f'b9b4_{b8.split(\"_\")[1]}']  = (T[b9] - T[b4])/ (T[b9] + T[b4])    # B7  / B3\n",
        "    T[f'b5b3_{b8.split(\"_\")[1]}']  = (T[b5] - T[b3])/ (T[b5] + T[b3])    # B7  / B3\n",
        "    T[f'b12b3_{b8.split(\"_\")[1]}']  = (T[b12] - T[b3])/ (T[b12] + T[b3])    # B7  / B3\n",
        "    \n",
        "    T[f'b2b1_{b8.split(\"_\")[1]}']  = (T[b2] - T[b1])/ (T[b2] + T[b1])    # B7  / B3\n",
        "    T[f'b4b1_{b8.split(\"_\")[1]}']  = (T[b4] - T[b1])/ (T[b4] + T[b1])    # B7  / B3\n",
        "    T[f'b11b3_{b8.split(\"_\")[1]}']  = (T[b11] - T[b3])/ (T[b11] + T[b3]) \n",
        "    \n",
        "  for col in Bcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "  \n",
        "  for col in Vcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "    \n",
        "  for col1,col2,col3,col4,col5,col6,col7,col8 in zip(Obs1,Obs2,Obs3,Obs4,Obs5,Obs6,Obs7,Obs8) :\n",
        "    T[f'{col1.split(\"_\")[0]}_std'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].std(axis=1)\n",
        "    T[f'{col1.split(\"_\")[0]}_mean'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].mean(axis=1)\n",
        "\n",
        "    \n",
        "  # NDVI \n",
        "  T['water']       = T['NDVI_Month4'].apply(lambda x :1 if x<0 else 0)\n",
        "  T['dense_green'] = T['NDVI_Month4'].apply(lambda x :1 if x>=0.5 else 0)\n",
        "  T['not_green']   = T['NDVI_Month4'].apply(lambda x :1 if( (x>0) & (x<0.5)) else 0)\n",
        "\n",
        "  T['high_green'] = T['SAVI_Month4'].apply(lambda x :1 if x<0.1 else 0)\n",
        "  T['low_green']  = T['SAVI_Month4'].apply(lambda x :1 if x>=0.8 else 0)\n",
        "  T['chlorophyll_EVI'] = T['EVI_Month4'].apply(lambda x :1 if( (x>0.2) & (x<0.8)) else 0)\n",
        "\n",
        "  # process Vegetation indexes\n",
        "\n",
        "  ObsN   = T.filter(like='NDVI_').columns.tolist()\n",
        "  ObsSA  = T.filter(like='SAVI_').columns.tolist()\n",
        "  ObsCC  = T.filter(like='CCCI_').columns.tolist()\n",
        "  ObsWDR = T.filter(like='WDRVI_').columns.tolist()\n",
        "  ObsNDRE7 = T.filter(like='NDRE7_').columns.tolist()\n",
        "\n",
        "  return T"
      ],
      "id": "288161bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.504510Z",
          "iopub.status.busy": "2021-09-29T05:49:51.503666Z",
          "iopub.status.idle": "2021-09-29T05:49:54.401618Z",
          "shell.execute_reply": "2021-09-29T05:49:54.400980Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.707822Z"
        },
        "id": "1348f1c3",
        "papermill": {
          "duration": 2.970586,
          "end_time": "2021-09-29T05:49:54.401773",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.431187",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train = create_train()\n",
        "Test = create_test()"
      ],
      "id": "1348f1c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:54.557186Z",
          "iopub.status.busy": "2021-09-29T05:49:54.556405Z",
          "iopub.status.idle": "2021-09-29T05:49:54.559626Z",
          "shell.execute_reply": "2021-09-29T05:49:54.560129Z",
          "shell.execute_reply.started": "2021-09-29T05:37:27.533875Z"
        },
        "id": "dd617c55",
        "papermill": {
          "duration": 0.079682,
          "end_time": "2021-09-29T05:49:54.560279",
          "exception": false,
          "start_time": "2021-09-29T05:49:54.480597",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c58ca82a-d949-4f73-c29a-b3f225fe5974"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "dd617c55",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:54.704352Z",
          "iopub.status.busy": "2021-09-29T05:49:54.703594Z",
          "iopub.status.idle": "2021-09-29T05:49:57.327280Z",
          "shell.execute_reply": "2021-09-29T05:49:57.326719Z",
          "shell.execute_reply.started": "2021-09-29T05:37:27.544882Z"
        },
        "papermill": {
          "duration": 2.698541,
          "end_time": "2021-09-29T05:49:57.327415",
          "exception": false,
          "start_time": "2021-09-29T05:49:54.628874",
          "status": "completed"
        },
        "tags": [],
        "id": "71925909"
      },
      "source": [
        "Train2 = createObs2_train()\n",
        "Test2 = createObs2_test()"
      ],
      "id": "71925909",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:57.474367Z",
          "iopub.status.busy": "2021-09-29T05:49:57.473601Z",
          "iopub.status.idle": "2021-09-29T05:49:57.476987Z",
          "shell.execute_reply": "2021-09-29T05:49:57.476569Z",
          "shell.execute_reply.started": "2021-09-29T05:37:30.207715Z"
        },
        "papermill": {
          "duration": 0.075297,
          "end_time": "2021-09-29T05:49:57.477105",
          "exception": false,
          "start_time": "2021-09-29T05:49:57.401808",
          "status": "completed"
        },
        "tags": [],
        "id": "3fa7820f",
        "outputId": "2d28ecb7-e507-4d4e-b9c5-1ba0ca404a2e"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "3fa7820f",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:57.618966Z",
          "iopub.status.busy": "2021-09-29T05:49:57.618204Z",
          "iopub.status.idle": "2021-09-29T05:50:00.161553Z",
          "shell.execute_reply": "2021-09-29T05:50:00.161042Z",
          "shell.execute_reply.started": "2021-09-29T05:37:30.215520Z"
        },
        "papermill": {
          "duration": 2.616429,
          "end_time": "2021-09-29T05:50:00.161682",
          "exception": false,
          "start_time": "2021-09-29T05:49:57.545253",
          "status": "completed"
        },
        "tags": [],
        "id": "b151a1db"
      },
      "source": [
        "Train3 = createObs3_train()\n",
        "Test3 = createObs3_test()"
      ],
      "id": "b151a1db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:00.308970Z",
          "iopub.status.busy": "2021-09-29T05:50:00.308289Z",
          "iopub.status.idle": "2021-09-29T05:50:00.311063Z",
          "shell.execute_reply": "2021-09-29T05:50:00.311472Z",
          "shell.execute_reply.started": "2021-09-29T05:37:32.702983Z"
        },
        "papermill": {
          "duration": 0.078987,
          "end_time": "2021-09-29T05:50:00.311595",
          "exception": false,
          "start_time": "2021-09-29T05:50:00.232608",
          "status": "completed"
        },
        "tags": [],
        "id": "ec556779",
        "outputId": "808c7b8d-d530-4ea1-9036-e28cfa95c88b"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "ec556779",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:00.452549Z",
          "iopub.status.busy": "2021-09-29T05:50:00.451637Z",
          "iopub.status.idle": "2021-09-29T05:50:02.848544Z",
          "shell.execute_reply": "2021-09-29T05:50:02.847431Z",
          "shell.execute_reply.started": "2021-09-29T05:37:32.710877Z"
        },
        "papermill": {
          "duration": 2.469373,
          "end_time": "2021-09-29T05:50:02.848675",
          "exception": false,
          "start_time": "2021-09-29T05:50:00.379302",
          "status": "completed"
        },
        "tags": [],
        "id": "72ff440b"
      },
      "source": [
        "Train4 = createObs4_train()\n",
        "Test4 = createObs4_test()"
      ],
      "id": "72ff440b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:02.991863Z",
          "iopub.status.busy": "2021-09-29T05:50:02.991157Z",
          "iopub.status.idle": "2021-09-29T05:50:02.994005Z",
          "shell.execute_reply": "2021-09-29T05:50:02.994443Z",
          "shell.execute_reply.started": "2021-09-29T05:37:35.104383Z"
        },
        "papermill": {
          "duration": 0.075587,
          "end_time": "2021-09-29T05:50:02.994589",
          "exception": false,
          "start_time": "2021-09-29T05:50:02.919002",
          "status": "completed"
        },
        "tags": [],
        "id": "d65352e2",
        "outputId": "69e427f0-c686-41ef-fb00-45c21a11d848"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "d65352e2",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:03.195813Z",
          "iopub.status.busy": "2021-09-29T05:50:03.195080Z",
          "iopub.status.idle": "2021-09-29T05:50:10.651486Z",
          "shell.execute_reply": "2021-09-29T05:50:10.650892Z",
          "shell.execute_reply.started": "2021-09-29T05:37:35.112575Z"
        },
        "papermill": {
          "duration": 7.587759,
          "end_time": "2021-09-29T05:50:10.651613",
          "exception": false,
          "start_time": "2021-09-29T05:50:03.063854",
          "status": "completed"
        },
        "tags": [],
        "id": "4a49e434"
      },
      "source": [
        "Train = process(Train)\n",
        "Test = process(Test)"
      ],
      "id": "4a49e434",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:10.796459Z",
          "iopub.status.busy": "2021-09-29T05:50:10.795352Z",
          "iopub.status.idle": "2021-09-29T05:50:10.798653Z",
          "shell.execute_reply": "2021-09-29T05:50:10.799103Z",
          "shell.execute_reply.started": "2021-09-29T05:37:42.588079Z"
        },
        "papermill": {
          "duration": 0.078599,
          "end_time": "2021-09-29T05:50:10.799229",
          "exception": false,
          "start_time": "2021-09-29T05:50:10.720630",
          "status": "completed"
        },
        "tags": [],
        "id": "006d62d7",
        "outputId": "fbcf256e-2d34-4690-be79-c89819f2756f"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "006d62d7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:10.999996Z",
          "iopub.status.busy": "2021-09-29T05:50:10.999363Z",
          "iopub.status.idle": "2021-09-29T05:50:18.712972Z",
          "shell.execute_reply": "2021-09-29T05:50:18.713398Z",
          "shell.execute_reply.started": "2021-09-29T05:37:42.595672Z"
        },
        "id": "3bad0300",
        "papermill": {
          "duration": 7.844996,
          "end_time": "2021-09-29T05:50:18.713561",
          "exception": false,
          "start_time": "2021-09-29T05:50:10.868565",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train2 = process(Train2)\n",
        "Test2 = process(Test2)"
      ],
      "id": "3bad0300",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:18.856992Z",
          "iopub.status.busy": "2021-09-29T05:50:18.856316Z",
          "iopub.status.idle": "2021-09-29T05:50:18.859165Z",
          "shell.execute_reply": "2021-09-29T05:50:18.859622Z",
          "shell.execute_reply.started": "2021-09-29T05:37:50.069577Z"
        },
        "papermill": {
          "duration": 0.077373,
          "end_time": "2021-09-29T05:50:18.859778",
          "exception": false,
          "start_time": "2021-09-29T05:50:18.782405",
          "status": "completed"
        },
        "tags": [],
        "id": "1b5bcb4b",
        "outputId": "6cf0ddf5-dccf-4190-c0d0-9bcbf4e7c709"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "1b5bcb4b",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:19.001800Z",
          "iopub.status.busy": "2021-09-29T05:50:19.001023Z",
          "iopub.status.idle": "2021-09-29T05:50:26.723386Z",
          "shell.execute_reply": "2021-09-29T05:50:26.723973Z",
          "shell.execute_reply.started": "2021-09-29T05:37:50.084768Z"
        },
        "papermill": {
          "duration": 7.793783,
          "end_time": "2021-09-29T05:50:26.724198",
          "exception": false,
          "start_time": "2021-09-29T05:50:18.930415",
          "status": "completed"
        },
        "tags": [],
        "id": "acfa2dc5"
      },
      "source": [
        "Train3 = process(Train3)\n",
        "Test3 = process(Test3)"
      ],
      "id": "acfa2dc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:26.973391Z",
          "iopub.status.busy": "2021-09-29T05:50:26.972458Z",
          "iopub.status.idle": "2021-09-29T05:50:26.977791Z",
          "shell.execute_reply": "2021-09-29T05:50:26.978417Z",
          "shell.execute_reply.started": "2021-09-29T05:37:57.626501Z"
        },
        "papermill": {
          "duration": 0.127686,
          "end_time": "2021-09-29T05:50:26.978620",
          "exception": false,
          "start_time": "2021-09-29T05:50:26.850934",
          "status": "completed"
        },
        "tags": [],
        "id": "040eda71",
        "outputId": "1d254cfb-0ff8-41d6-e9e0-5f14f3a68520"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "040eda71",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:27.210475Z",
          "iopub.status.busy": "2021-09-29T05:50:27.209388Z",
          "iopub.status.idle": "2021-09-29T05:50:34.677062Z",
          "shell.execute_reply": "2021-09-29T05:50:34.676585Z",
          "shell.execute_reply.started": "2021-09-29T05:37:57.634680Z"
        },
        "papermill": {
          "duration": 7.604812,
          "end_time": "2021-09-29T05:50:34.677201",
          "exception": false,
          "start_time": "2021-09-29T05:50:27.072389",
          "status": "completed"
        },
        "tags": [],
        "id": "f9d8cde4"
      },
      "source": [
        "Train4 = process(Train4)\n",
        "Test4 = process(Test4)"
      ],
      "id": "f9d8cde4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:34.823730Z",
          "iopub.status.busy": "2021-09-29T05:50:34.823099Z",
          "iopub.status.idle": "2021-09-29T05:50:34.825744Z",
          "shell.execute_reply": "2021-09-29T05:50:34.826172Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.127535Z"
        },
        "papermill": {
          "duration": 0.077485,
          "end_time": "2021-09-29T05:50:34.826298",
          "exception": false,
          "start_time": "2021-09-29T05:50:34.748813",
          "status": "completed"
        },
        "tags": [],
        "id": "14b2df9d",
        "outputId": "c01b8d73-585b-4816-a8c1-bd7203a355c2"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "14b2df9d",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:35.173178Z",
          "iopub.status.busy": "2021-09-29T05:50:35.172142Z",
          "iopub.status.idle": "2021-09-29T05:50:35.490909Z",
          "shell.execute_reply": "2021-09-29T05:50:35.490428Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.135660Z"
        },
        "papermill": {
          "duration": 0.594109,
          "end_time": "2021-09-29T05:50:35.491030",
          "exception": false,
          "start_time": "2021-09-29T05:50:34.896921",
          "status": "completed"
        },
        "tags": [],
        "id": "79e018c1",
        "outputId": "6c3f34d3-4924-4372-ef2f-b3ee2a4b7e9f"
      },
      "source": [
        "Train = pd.concat([Train,Train2.drop(columns=['field_id','label']),Train3.drop(columns=['field_id','label']),Train4.drop(columns=['field_id','label'])],axis=1)\n",
        "Train.shape"
      ],
      "id": "79e018c1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(87114, 1290)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:35.694139Z",
          "iopub.status.busy": "2021-09-29T05:50:35.693499Z",
          "iopub.status.idle": "2021-09-29T05:50:36.512342Z",
          "shell.execute_reply": "2021-09-29T05:50:36.511917Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.666689Z"
        },
        "id": "39df5209",
        "papermill": {
          "duration": 0.950552,
          "end_time": "2021-09-29T05:50:36.512462",
          "exception": false,
          "start_time": "2021-09-29T05:50:35.561910",
          "status": "completed"
        },
        "tags": [],
        "outputId": "14bef70b-b3d5-4ffb-d20e-983e85885217"
      },
      "source": [
        "Test = pd.concat([Test,Test2.drop(columns=['field_id']),Test3.drop(columns=['field_id'])],axis=1)\n",
        "Test = pd.merge(Test,Test4,on='field_id',how='left')\n",
        "Test.shape"
      ],
      "id": "39df5209",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35295, 1289)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9bd0b7de",
        "papermill": {
          "duration": 0.070201,
          "end_time": "2021-09-29T05:50:36.654370",
          "exception": false,
          "start_time": "2021-09-29T05:50:36.584169",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODELING"
      ],
      "id": "9bd0b7de"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:36.949039Z",
          "iopub.status.busy": "2021-09-29T05:50:36.948489Z",
          "iopub.status.idle": "2021-09-29T05:50:39.760047Z",
          "shell.execute_reply": "2021-09-29T05:50:39.759559Z",
          "shell.execute_reply.started": "2021-09-29T05:38:06.549064Z"
        },
        "id": "ab6ae39f",
        "papermill": {
          "duration": 2.886861,
          "end_time": "2021-09-29T05:50:39.760189",
          "exception": false,
          "start_time": "2021-09-29T05:50:36.873328",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "X    = Train.fillna(-999).replace(np.inf,9999).drop(['field_id','label'], axis=1)\n",
        "y    = Train.label\n",
        "TEST = Test.fillna(-999).replace(np.inf,9999).drop(['field_id'], axis=1)"
      ],
      "id": "ab6ae39f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:39.916633Z",
          "iopub.status.busy": "2021-09-29T05:50:39.914973Z",
          "iopub.status.idle": "2021-09-29T05:50:39.917337Z",
          "shell.execute_reply": "2021-09-29T05:50:39.917848Z",
          "shell.execute_reply.started": "2021-09-29T05:38:53.274125Z"
        },
        "papermill": {
          "duration": 0.08268,
          "end_time": "2021-09-29T05:50:39.917987",
          "exception": false,
          "start_time": "2021-09-29T05:50:39.835307",
          "status": "completed"
        },
        "tags": [],
        "id": "f45e32b1"
      },
      "source": [
        "TEST.columns = X.columns.tolist()"
      ],
      "id": "f45e32b1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:40.066832Z",
          "iopub.status.busy": "2021-09-29T05:50:40.065866Z",
          "iopub.status.idle": "2021-09-29T05:51:29.296475Z",
          "shell.execute_reply": "2021-09-29T05:51:29.296998Z",
          "shell.execute_reply.started": "2021-09-29T05:38:54.057802Z"
        },
        "papermill": {
          "duration": 49.306534,
          "end_time": "2021-09-29T05:51:29.297186",
          "exception": false,
          "start_time": "2021-09-29T05:50:39.990652",
          "status": "completed"
        },
        "tags": [],
        "id": "66c40756"
      },
      "source": [
        "data = pd.concat([X,TEST])\n",
        "qt=QuantileTransformer(output_distribution=\"normal\",random_state=42)\n",
        "data= pd.DataFrame(qt.fit_transform(data),columns=X.columns)"
      ],
      "id": "66c40756",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:29.484879Z",
          "iopub.status.busy": "2021-09-29T05:51:29.484052Z",
          "iopub.status.idle": "2021-09-29T05:51:29.489890Z",
          "shell.execute_reply": "2021-09-29T05:51:29.489361Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.215981Z"
        },
        "papermill": {
          "duration": 0.082725,
          "end_time": "2021-09-29T05:51:29.490003",
          "exception": false,
          "start_time": "2021-09-29T05:51:29.407278",
          "status": "completed"
        },
        "tags": [],
        "id": "c5b49b9a",
        "outputId": "214ea9d1-851d-4631-8730-e83f65ba2eca"
      },
      "source": [
        "X.shape , TEST.shape"
      ],
      "id": "c5b49b9a",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 1288), (35295, 1288))"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:29.938299Z",
          "iopub.status.busy": "2021-09-29T05:51:29.937511Z",
          "iopub.status.idle": "2021-09-29T05:51:29.939708Z",
          "shell.execute_reply": "2021-09-29T05:51:29.940171Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.235075Z"
        },
        "papermill": {
          "duration": 0.0843,
          "end_time": "2021-09-29T05:51:29.940314",
          "exception": false,
          "start_time": "2021-09-29T05:51:29.856014",
          "status": "completed"
        },
        "tags": [],
        "id": "b28d1fcc"
      },
      "source": [
        "def attention_3d_block(inputs, name):\n",
        "  # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "  TIME_STEPS = inputs.shape[1]\n",
        "  SINGLE_ATTENTION_VECTOR = False\n",
        "  input_dim = inputs.shape[2]\n",
        "  a = Permute((2, 1))(inputs)\n",
        "  a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "  if SINGLE_ATTENTION_VECTOR:  \n",
        "    a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
        "    a = RepeatVector(input_dim)(a)\n",
        "  a_probs = Permute((2, 1), name=name)(a)\n",
        "  output_attention_mul = Multiply()([inputs, a_probs])\n",
        "  return output_attention_mul"
      ],
      "id": "b28d1fcc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.098119Z",
          "iopub.status.busy": "2021-09-29T05:51:30.097289Z",
          "iopub.status.idle": "2021-09-29T05:51:30.099197Z",
          "shell.execute_reply": "2021-09-29T05:51:30.099887Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.245865Z"
        },
        "papermill": {
          "duration": 0.085846,
          "end_time": "2021-09-29T05:51:30.100019",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.014173",
          "status": "completed"
        },
        "tags": [],
        "id": "7a322cf3"
      },
      "source": [
        "def get_model():\n",
        "  tf.random.set_seed(1)\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "    \n",
        "  input_tensor = Input(shape=(1,X.shape[2]))\n",
        "  \n",
        "  x = BatchNormalization()(input_tensor)\n",
        "  x = Conv1D(256, 5,strides=5,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_1')\n",
        "  x = Dropout(0.1)(x)\n",
        "  \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv1D(128, 7,strides=3,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_2')\n",
        "  x = Dropout(0.15)(x)\n",
        "  \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv1D(512, 9,strides=3,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_3')\n",
        "  x = Dropout(0.15)(x)\n",
        "\n",
        "  x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "  x = Dropout(0.3)(x)\n",
        "\n",
        "  out = Dense(9,kernel_initializer=glorot_normal(seed=1),\n",
        "                bias_initializer=glorot_normal(seed=1),\n",
        "                activation=\"softmax\")(x)\n",
        "\n",
        "  model = Model(inputs=input_tensor,outputs =out)\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  return model"
      ],
      "id": "7a322cf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.258528Z",
          "iopub.status.busy": "2021-09-29T05:51:30.257678Z",
          "iopub.status.idle": "2021-09-29T05:51:30.259816Z",
          "shell.execute_reply": "2021-09-29T05:51:30.260264Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.263017Z"
        },
        "papermill": {
          "duration": 0.082672,
          "end_time": "2021-09-29T05:51:30.260412",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.177740",
          "status": "completed"
        },
        "tags": [],
        "id": "c1829e03"
      },
      "source": [
        "X = data[:X.shape[0]].values\n",
        "TEST = data[X.shape[0]:].values"
      ],
      "id": "c1829e03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.410780Z",
          "iopub.status.busy": "2021-09-29T05:51:30.409976Z",
          "iopub.status.idle": "2021-09-29T05:51:30.411992Z",
          "shell.execute_reply": "2021-09-29T05:51:30.412406Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.276556Z"
        },
        "papermill": {
          "duration": 0.078291,
          "end_time": "2021-09-29T05:51:30.412544",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.334253",
          "status": "completed"
        },
        "tags": [],
        "id": "92fe5a5f"
      },
      "source": [
        "X = X.reshape(X.shape[0], 1,X.shape[1])\n",
        "TEST = TEST.reshape(TEST.shape[0],1,TEST.shape[1])"
      ],
      "id": "92fe5a5f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.563823Z",
          "iopub.status.busy": "2021-09-29T05:51:30.563203Z",
          "iopub.status.idle": "2021-09-29T05:51:30.565890Z",
          "shell.execute_reply": "2021-09-29T05:51:30.566386Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.284856Z"
        },
        "papermill": {
          "duration": 0.08036,
          "end_time": "2021-09-29T05:51:30.566525",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.486165",
          "status": "completed"
        },
        "tags": [],
        "id": "2a2e350a",
        "outputId": "187c877a-54e2-46d3-f696-a7b6f2a70726"
      },
      "source": [
        "X.shape , TEST.shape"
      ],
      "id": "2a2e350a",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 1, 1288), (35295, 1, 1288))"
            ]
          },
          "execution_count": 49,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.717773Z",
          "iopub.status.busy": "2021-09-29T05:51:30.716948Z",
          "iopub.status.idle": "2021-09-29T05:51:30.718715Z",
          "shell.execute_reply": "2021-09-29T05:51:30.719170Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.294869Z"
        },
        "papermill": {
          "duration": 0.07969,
          "end_time": "2021-09-29T05:51:30.719292",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.639602",
          "status": "completed"
        },
        "tags": [],
        "id": "d634c267"
      },
      "source": [
        "# Function to seed everything\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "id": "d634c267",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:30.867456Z",
          "iopub.status.busy": "2021-09-29T05:51:30.866653Z",
          "iopub.status.idle": "2021-09-29T05:51:30.868572Z",
          "shell.execute_reply": "2021-09-29T05:51:30.869060Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.303819Z"
        },
        "papermill": {
          "duration": 0.078466,
          "end_time": "2021-09-29T05:51:30.869186",
          "exception": false,
          "start_time": "2021-09-29T05:51:30.790720",
          "status": "completed"
        },
        "tags": [],
        "id": "8927d720"
      },
      "source": [
        "seed_everything(42)"
      ],
      "id": "8927d720",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:31.172781Z",
          "iopub.status.busy": "2021-09-29T05:51:31.171614Z",
          "iopub.status.idle": "2021-09-29T05:51:31.182214Z",
          "shell.execute_reply": "2021-09-29T05:51:31.181625Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.318645Z"
        },
        "papermill": {
          "duration": 0.088758,
          "end_time": "2021-09-29T05:51:31.182331",
          "exception": false,
          "start_time": "2021-09-29T05:51:31.093573",
          "status": "completed"
        },
        "tags": [],
        "id": "cea17fa6"
      },
      "source": [
        "y_train = y.copy()\n",
        "n_labels = y.unique().shape[0]\n",
        "\n",
        "# we need to binarize the labels for the neural net\n",
        "LE = LabelEncoder()\n",
        "ytrain_enc = pd.get_dummies(y_train).values\n",
        "TARGETS = pd.get_dummies(y_train).columns\n",
        "\n",
        "y_oof = np.zeros([X.shape[0], n_labels])\n",
        "y_test = np.zeros([TEST.shape[0], n_labels])\n",
        "\n",
        "i = 0\n",
        "metrics = list()\n",
        "apply_aug = False"
      ],
      "id": "cea17fa6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "execution": {
          "iopub.execute_input": "2021-09-29T05:51:31.339948Z",
          "iopub.status.busy": "2021-09-29T05:51:31.334519Z",
          "iopub.status.idle": "2021-09-29T08:28:01.729152Z",
          "shell.execute_reply": "2021-09-29T08:28:01.730007Z",
          "shell.execute_reply.started": "2021-09-29T05:39:42.338301Z"
        },
        "jupyter": {
          "outputs_hidden": true
        },
        "papermill": {
          "duration": 9390.475714,
          "end_time": "2021-09-29T08:28:01.730173",
          "exception": false,
          "start_time": "2021-09-29T05:51:31.254459",
          "status": "completed"
        },
        "tags": [],
        "id": "9fd2b605",
        "outputId": "2d1f4457-20ea-42cb-ad1f-26590975bd42"
      },
      "source": [
        "seed_everything(42)\n",
        "n_splits = 10\n",
        "kf = StratifiedKFold(n_splits=n_splits, random_state=47, shuffle=True)\n",
        "\n",
        "for idx , (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    # Verbosity\n",
        "    VERBOSE = 0\n",
        "    seed_everything(42)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
        "                                                      mode = 'min',\n",
        "                                                      patience = 35,\n",
        "                                                      restore_best_weights = True,\n",
        "                                                      verbose = 1)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Radiant_NN_{idx}.h5',\n",
        "                                                    monitor = 'val_loss',\n",
        "                                                    verbose = 1,\n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True)\n",
        "\n",
        "    X_tr, X_vl = X[tr_idx, :], X[val_idx, :]\n",
        "    y_tr, y_vl = ytrain_enc[tr_idx], ytrain_enc[val_idx]\n",
        "    y_train_, y_vld_ = y[tr_idx], y[val_idx]\n",
        "\n",
        "    # 1 CNN ANN\n",
        "    seed_everything(seed=1)\n",
        "    model = get_model()\n",
        "    model.fit( \n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        validation_data =(X_vl, y_vl),\n",
        "        callbacks = [early_stopping,  checkpoint],\n",
        "        epochs = 300,\n",
        "        batch_size = 64)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    model = get_model()\n",
        "    model.load_weights(f'Radiant_NN_{idx}.h5')\n",
        "    y_pred_cnn_nn = model.predict(X_vl)\n",
        "    test_pred_cnn_nn = model.predict(TEST, batch_size= 1024)\n",
        "    \n",
        "    \n",
        "    y_oof[val_idx, :] = y_pred_cnn_nn\n",
        "    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "    metric = log_loss(y_vl, y_pred_cnn_nn)\n",
        "    print(\"fold #{} Log Loss: {}\".format(i, metric))\n",
        "    \n",
        "    i += 1\n",
        "    test_pred = test_pred_cnn_nn \n",
        "    y_test += test_pred / n_splits\n",
        "    metrics.append(metric)\n",
        "    \n",
        "metrics = np.array(metrics).mean()\n",
        "print(f'Full Log loss {metrics}') "
      ],
      "id": "9fd2b605",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-29 05:51:31.921587: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-29 05:51:31.924888: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-09-29 05:51:31.965828: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:31.966486: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-29 05:51:31.966562: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-29 05:51:31.994342: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-29 05:51:31.994451: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-29 05:51:32.019085: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-29 05:51:32.027419: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-29 05:51:32.051436: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-29 05:51:32.059218: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-29 05:51:32.061875: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-29 05:51:32.062066: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:32.062791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:32.064178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-29 05:51:32.064677: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-09-29 05:51:32.064904: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-29 05:51:32.065096: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:32.065691: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-29 05:51:32.065744: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-29 05:51:32.065786: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-29 05:51:32.065806: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-29 05:51:32.065824: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-29 05:51:32.065841: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-29 05:51:32.065859: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-29 05:51:32.065878: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-29 05:51:32.065897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-29 05:51:32.065975: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:32.066607: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:32.067263: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-29 05:51:32.068653: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-29 05:51:33.530273: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-29 05:51:33.530328: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-29 05:51:33.530339: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-29 05:51:33.532805: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:33.533568: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:33.534242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-29 05:51:33.534819: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2021-09-29 05:51:34.562416: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-09-29 05:51:34.572936: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000175000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-29 05:51:35.981193: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-29 05:51:36.782904: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-29 05:51:36.815558: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1226/1226 [==============================] - 20s 10ms/step - loss: 1.1433 - val_loss: 0.9191\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91915, saving model to Radiant_NN_0.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.9250 - val_loss: 0.8633\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91915 to 0.86328, saving model to Radiant_NN_0.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8660 - val_loss: 0.8275\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86328 to 0.82749, saving model to Radiant_NN_0.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8320 - val_loss: 0.8005\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82749 to 0.80048, saving model to Radiant_NN_0.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8027 - val_loss: 0.7873\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80048 to 0.78732, saving model to Radiant_NN_0.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7804 - val_loss: 0.7675\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78732 to 0.76752, saving model to Radiant_NN_0.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7556 - val_loss: 0.7607\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76752 to 0.76067, saving model to Radiant_NN_0.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7372 - val_loss: 0.7504\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76067 to 0.75037, saving model to Radiant_NN_0.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7154 - val_loss: 0.7576\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.75037\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7057 - val_loss: 0.7401\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.75037 to 0.74005, saving model to Radiant_NN_0.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6915 - val_loss: 0.7382\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.74005 to 0.73821, saving model to Radiant_NN_0.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6737 - val_loss: 0.7250\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73821 to 0.72498, saving model to Radiant_NN_0.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6673 - val_loss: 0.7289\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.72498\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6475 - val_loss: 0.7253\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.72498\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6442 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.72498 to 0.71719, saving model to Radiant_NN_0.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6380 - val_loss: 0.7127\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71719 to 0.71275, saving model to Radiant_NN_0.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6246 - val_loss: 0.7070\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.71275 to 0.70698, saving model to Radiant_NN_0.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6084 - val_loss: 0.7077\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70698\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6031 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.70698 to 0.70514, saving model to Radiant_NN_0.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5949 - val_loss: 0.7068\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70514\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5861 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70514 to 0.69200, saving model to Radiant_NN_0.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5828 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69200\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5657 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69200\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5677 - val_loss: 0.7124\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69200\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5632 - val_loss: 0.7069\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69200\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.5475 - val_loss: 0.6976\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69200\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5490 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69200\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5420 - val_loss: 0.6996\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69200\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.5386 - val_loss: 0.6989\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69200\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5338 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69200\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5235 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69200\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5262 - val_loss: 0.7021\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69200\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5129 - val_loss: 0.7135\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69200\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5129 - val_loss: 0.6921\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69200\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5061 - val_loss: 0.6975\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69200\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4922 - val_loss: 0.6852\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.69200 to 0.68522, saving model to Radiant_NN_0.h5\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4982 - val_loss: 0.7067\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68522\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4964 - val_loss: 0.7042\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68522\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4897 - val_loss: 0.6924\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68522\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4907 - val_loss: 0.6865\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68522\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4875 - val_loss: 0.6967\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68522\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4781 - val_loss: 0.6808\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.68522 to 0.68077, saving model to Radiant_NN_0.h5\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4711 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68077\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4730 - val_loss: 0.7086\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68077\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4730 - val_loss: 0.6970\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68077\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4634 - val_loss: 0.6987\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68077\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4647 - val_loss: 0.7098\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68077\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4610 - val_loss: 0.7131\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68077\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4579 - val_loss: 0.6935\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68077\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4555 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68077\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4456 - val_loss: 0.7056\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68077\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4512 - val_loss: 0.7257\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68077\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4397 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68077\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4437 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68077\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4449 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.68077 to 0.67851, saving model to Radiant_NN_0.h5\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4359 - val_loss: 0.7150\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67851\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4310 - val_loss: 0.7201\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67851\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4301 - val_loss: 0.6994\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67851\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4246 - val_loss: 0.6976\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67851\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4304 - val_loss: 0.7112\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67851\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4251 - val_loss: 0.6832\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67851\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4236 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67851\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4209 - val_loss: 0.7019\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67851\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4139 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67851\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4160 - val_loss: 0.7193\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67851\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4180 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67851\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4159 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67851\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4139 - val_loss: 0.7203\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67851\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4094 - val_loss: 0.7104\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67851\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4003 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67851\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4053 - val_loss: 0.7198\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67851\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4020 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67851\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3942 - val_loss: 0.7151\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67851\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3934 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67851\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3955 - val_loss: 0.7215\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67851\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.3927 - val_loss: 0.7192\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67851\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3963 - val_loss: 0.6940\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67851\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3897 - val_loss: 0.7207\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67851\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3843 - val_loss: 0.7472\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67851\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3894 - val_loss: 0.7075\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67851\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3889 - val_loss: 0.7105\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67851\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3879 - val_loss: 0.7190\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67851\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3792 - val_loss: 0.6975\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67851\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3859 - val_loss: 0.7134\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67851\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3851 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67851\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3836 - val_loss: 0.7259\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67851\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3805 - val_loss: 0.7324\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67851\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.3784 - val_loss: 0.7000\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67851\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3751 - val_loss: 0.7245\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67851\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3747 - val_loss: 0.7135\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67851\n",
            "Epoch 00090: early stopping\n",
            "fold #0 Log Loss: 0.6785108114991141\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 1.1439 - val_loss: 0.8865\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.88649, saving model to Radiant_NN_1.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9206 - val_loss: 0.8353\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.88649 to 0.83534, saving model to Radiant_NN_1.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8703 - val_loss: 0.8019\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83534 to 0.80186, saving model to Radiant_NN_1.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8329 - val_loss: 0.7905\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.80186 to 0.79047, saving model to Radiant_NN_1.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8028 - val_loss: 0.7764\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79047 to 0.77636, saving model to Radiant_NN_1.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7849 - val_loss: 0.7581\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77636 to 0.75809, saving model to Radiant_NN_1.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7658 - val_loss: 0.7365\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.75809 to 0.73655, saving model to Radiant_NN_1.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7516 - val_loss: 0.7303\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.73655 to 0.73025, saving model to Radiant_NN_1.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7198 - val_loss: 0.7239\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.73025 to 0.72385, saving model to Radiant_NN_1.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7074 - val_loss: 0.7184\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.72385 to 0.71842, saving model to Radiant_NN_1.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6980 - val_loss: 0.7154\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.71842 to 0.71541, saving model to Radiant_NN_1.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6861 - val_loss: 0.7072\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.71541 to 0.70717, saving model to Radiant_NN_1.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6721 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.70717 to 0.70173, saving model to Radiant_NN_1.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6518 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.70173 to 0.69990, saving model to Radiant_NN_1.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6422 - val_loss: 0.6871\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.69990 to 0.68711, saving model to Radiant_NN_1.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6366 - val_loss: 0.6872\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.68711\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6221 - val_loss: 0.6858\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68711 to 0.68580, saving model to Radiant_NN_1.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6080 - val_loss: 0.6836\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.68580 to 0.68360, saving model to Radiant_NN_1.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6043 - val_loss: 0.6795\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68360 to 0.67954, saving model to Radiant_NN_1.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6020 - val_loss: 0.6865\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67954\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5930 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.67954 to 0.67217, saving model to Radiant_NN_1.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5781 - val_loss: 0.6741\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67217\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5747 - val_loss: 0.6690\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.67217 to 0.66898, saving model to Radiant_NN_1.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5669 - val_loss: 0.6674\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.66898 to 0.66740, saving model to Radiant_NN_1.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5615 - val_loss: 0.6739\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66740\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5518 - val_loss: 0.6705\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66740\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5529 - val_loss: 0.6746\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66740\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5410 - val_loss: 0.6824\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66740\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5353 - val_loss: 0.6753\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.66740\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5322 - val_loss: 0.6732\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66740\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5291 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66740\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5209 - val_loss: 0.6771\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66740\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5101 - val_loss: 0.6754\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66740\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5078 - val_loss: 0.6638\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.66740 to 0.66380, saving model to Radiant_NN_1.h5\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5072 - val_loss: 0.6646\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66380\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5025 - val_loss: 0.6641\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66380\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5053 - val_loss: 0.6611\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.66380 to 0.66110, saving model to Radiant_NN_1.h5\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4917 - val_loss: 0.6643\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66110\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4871 - val_loss: 0.6553\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.66110 to 0.65526, saving model to Radiant_NN_1.h5\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4895 - val_loss: 0.6776\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65526\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4771 - val_loss: 0.6716\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65526\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4764 - val_loss: 0.6912\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65526\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4776 - val_loss: 0.6754\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65526\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4741 - val_loss: 0.6644\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65526\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4684 - val_loss: 0.6760\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65526\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4717 - val_loss: 0.6685\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65526\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4609 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65526\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4551 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65526\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4583 - val_loss: 0.6797\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65526\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4527 - val_loss: 0.6704\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65526\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4507 - val_loss: 0.6627\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65526\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4463 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65526\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4494 - val_loss: 0.6821\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65526\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4439 - val_loss: 0.6762\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65526\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4397 - val_loss: 0.6760\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.65526\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4343 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.65526\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4364 - val_loss: 0.6780\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.65526\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4247 - val_loss: 0.6790\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.65526\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4245 - val_loss: 0.6745\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.65526\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4226 - val_loss: 0.6764\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.65526\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4272 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.65526\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4259 - val_loss: 0.6990\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.65526\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4150 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.65526\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4145 - val_loss: 0.6860\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.65526\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4185 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.65526\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4097 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.65526\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4093 - val_loss: 0.6748\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.65526\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4017 - val_loss: 0.6800\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.65526\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4092 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.65526\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4011 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.65526\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4095 - val_loss: 0.7022\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.65526\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4032 - val_loss: 0.7102\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.65526\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4012 - val_loss: 0.7015\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.65526\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4002 - val_loss: 0.6851\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.65526\n",
            "Epoch 00074: early stopping\n",
            "fold #1 Log Loss: 0.6552569190258327\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 14s 10ms/step - loss: 1.1431 - val_loss: 0.8900\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89002, saving model to Radiant_NN_2.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.9281 - val_loss: 0.8339\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89002 to 0.83394, saving model to Radiant_NN_2.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8685 - val_loss: 0.7981\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83394 to 0.79815, saving model to Radiant_NN_2.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8285 - val_loss: 0.7854\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.79815 to 0.78538, saving model to Radiant_NN_2.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8115 - val_loss: 0.7626\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.78538 to 0.76260, saving model to Radiant_NN_2.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7877 - val_loss: 0.7532\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.76260 to 0.75324, saving model to Radiant_NN_2.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7634 - val_loss: 0.7363\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.75324 to 0.73630, saving model to Radiant_NN_2.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7360 - val_loss: 0.7291\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.73630 to 0.72910, saving model to Radiant_NN_2.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7151 - val_loss: 0.7203\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.72910 to 0.72035, saving model to Radiant_NN_2.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7115 - val_loss: 0.7151\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.72035 to 0.71510, saving model to Radiant_NN_2.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6975 - val_loss: 0.7156\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.71510\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6790 - val_loss: 0.7119\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.71510 to 0.71192, saving model to Radiant_NN_2.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6665 - val_loss: 0.6934\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71192 to 0.69342, saving model to Radiant_NN_2.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6607 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69342 to 0.68998, saving model to Radiant_NN_2.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6467 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68998 to 0.68809, saving model to Radiant_NN_2.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6353 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68809 to 0.67890, saving model to Radiant_NN_2.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6239 - val_loss: 0.6768\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.67890 to 0.67683, saving model to Radiant_NN_2.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6190 - val_loss: 0.6914\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67683\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6089 - val_loss: 0.6818\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67683\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6043 - val_loss: 0.6755\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.67683 to 0.67550, saving model to Radiant_NN_2.h5\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5906 - val_loss: 0.6730\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.67550 to 0.67301, saving model to Radiant_NN_2.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5782 - val_loss: 0.6772\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67301\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5720 - val_loss: 0.6771\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.67301\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5662 - val_loss: 0.6671\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.67301 to 0.66709, saving model to Radiant_NN_2.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5585 - val_loss: 0.6736\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66709\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5569 - val_loss: 0.6759\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66709\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5473 - val_loss: 0.6749\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66709\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5502 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66709\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5440 - val_loss: 0.6646\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.66709 to 0.66459, saving model to Radiant_NN_2.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5326 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66459\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5292 - val_loss: 0.6634\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.66459 to 0.66345, saving model to Radiant_NN_2.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5185 - val_loss: 0.6673\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66345\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5137 - val_loss: 0.6752\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66345\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5129 - val_loss: 0.6688\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66345\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5182 - val_loss: 0.6614\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.66345 to 0.66141, saving model to Radiant_NN_2.h5\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5071 - val_loss: 0.6608\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.66141 to 0.66077, saving model to Radiant_NN_2.h5\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5065 - val_loss: 0.6709\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66077\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4932 - val_loss: 0.6618\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66077\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4938 - val_loss: 0.6689\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66077\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4955 - val_loss: 0.6662\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66077\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4903 - val_loss: 0.6638\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66077\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4780 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66077\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4788 - val_loss: 0.6679\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66077\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4826 - val_loss: 0.6698\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66077\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4743 - val_loss: 0.6718\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66077\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4714 - val_loss: 0.6619\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66077\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4658 - val_loss: 0.6850\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66077\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4614 - val_loss: 0.6653\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66077\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4612 - val_loss: 0.6876\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66077\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4605 - val_loss: 0.6674\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66077\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4516 - val_loss: 0.6752\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66077\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4490 - val_loss: 0.6747\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66077\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4451 - val_loss: 0.6746\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66077\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4477 - val_loss: 0.6793\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66077\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4437 - val_loss: 0.6747\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66077\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4378 - val_loss: 0.6676\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66077\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4330 - val_loss: 0.6703\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.66077\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 16s 13ms/step - loss: 0.4415 - val_loss: 0.6650\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.66077\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4286 - val_loss: 0.6691\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.66077\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4363 - val_loss: 0.6706\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.66077\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4332 - val_loss: 0.6648\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.66077\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4209 - val_loss: 0.6752\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.66077\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4226 - val_loss: 0.6888\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.66077\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4222 - val_loss: 0.6612\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.66077\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4214 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.66077\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4220 - val_loss: 0.6680\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.66077\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4130 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.66077\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4146 - val_loss: 0.6701\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.66077\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4157 - val_loss: 0.6954\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.66077\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4091 - val_loss: 0.6980\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66077\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4066 - val_loss: 0.6795\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66077\n",
            "Epoch 00071: early stopping\n",
            "fold #2 Log Loss: 0.6607722144814044\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 1.1492 - val_loss: 0.9099\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90988, saving model to Radiant_NN_3.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.9299 - val_loss: 0.8440\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90988 to 0.84399, saving model to Radiant_NN_3.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8662 - val_loss: 0.8153\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84399 to 0.81533, saving model to Radiant_NN_3.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8357 - val_loss: 0.8132\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.81533 to 0.81323, saving model to Radiant_NN_3.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8043 - val_loss: 0.7791\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81323 to 0.77905, saving model to Radiant_NN_3.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7846 - val_loss: 0.7672\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77905 to 0.76723, saving model to Radiant_NN_3.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7485 - val_loss: 0.7573\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76723 to 0.75735, saving model to Radiant_NN_3.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7370 - val_loss: 0.7474\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75735 to 0.74735, saving model to Radiant_NN_3.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7261 - val_loss: 0.7323\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74735 to 0.73232, saving model to Radiant_NN_3.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7038 - val_loss: 0.7354\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.73232\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6830 - val_loss: 0.7436\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.73232\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.6800 - val_loss: 0.7262\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73232 to 0.72619, saving model to Radiant_NN_3.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6642 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72619 to 0.71160, saving model to Radiant_NN_3.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6513 - val_loss: 0.7099\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71160 to 0.70985, saving model to Radiant_NN_3.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6481 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70985\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6291 - val_loss: 0.7029\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70985 to 0.70289, saving model to Radiant_NN_3.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6237 - val_loss: 0.7034\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70289\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6105 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70289\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6075 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.70289 to 0.70100, saving model to Radiant_NN_3.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5941 - val_loss: 0.7030\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70100\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5855 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70100 to 0.69846, saving model to Radiant_NN_3.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5813 - val_loss: 0.7145\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69846\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5762 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69846 to 0.69296, saving model to Radiant_NN_3.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5688 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69296\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5565 - val_loss: 0.6984\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69296\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5521 - val_loss: 0.6961\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69296\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5477 - val_loss: 0.7015\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69296\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5360 - val_loss: 0.6905\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.69296 to 0.69050, saving model to Radiant_NN_3.h5\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5300 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.69050 to 0.68769, saving model to Radiant_NN_3.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5303 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68769\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5260 - val_loss: 0.7085\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68769\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5229 - val_loss: 0.6950\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68769\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5222 - val_loss: 0.6989\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68769\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5125 - val_loss: 0.6983\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68769\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5065 - val_loss: 0.7040\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68769\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5044 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68769\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4916 - val_loss: 0.7018\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68769\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4949 - val_loss: 0.6989\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68769\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4939 - val_loss: 0.6893\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68769\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4888 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68769\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4790 - val_loss: 0.6998\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68769\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4775 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68769\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4750 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68769\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4710 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68769\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4729 - val_loss: 0.7001\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68769\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4633 - val_loss: 0.7041\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68769\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4643 - val_loss: 0.7100\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68769\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4621 - val_loss: 0.7191\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68769\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4541 - val_loss: 0.6938\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68769\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4580 - val_loss: 0.6961\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68769\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4460 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68769\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4502 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68769\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4480 - val_loss: 0.7248\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68769\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4407 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68769\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4421 - val_loss: 0.7132\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68769\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4413 - val_loss: 0.6799\n",
            "\n",
            "Epoch 00056: val_loss improved from 0.68769 to 0.67985, saving model to Radiant_NN_3.h5\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4358 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67985\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4337 - val_loss: 0.7115\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67985\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4424 - val_loss: 0.7159\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67985\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4242 - val_loss: 0.7044\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67985\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4328 - val_loss: 0.7084\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67985\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4254 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67985\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4220 - val_loss: 0.7273\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67985\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4212 - val_loss: 0.7135\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67985\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4165 - val_loss: 0.7074\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67985\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4051 - val_loss: 0.7157\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67985\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4188 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67985\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4063 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67985\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4033 - val_loss: 0.7296\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67985\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4010 - val_loss: 0.7104\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67985\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4049 - val_loss: 0.7111\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67985\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4068 - val_loss: 0.7081\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67985\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3953 - val_loss: 0.7047\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67985\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4005 - val_loss: 0.7376\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67985\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3924 - val_loss: 0.7119\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67985\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4020 - val_loss: 0.7028\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67985\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3946 - val_loss: 0.7132\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67985\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3927 - val_loss: 0.7283\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67985\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3944 - val_loss: 0.7408\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67985\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3879 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67985\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3877 - val_loss: 0.7192\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67985\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3898 - val_loss: 0.7482\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67985\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3837 - val_loss: 0.7251\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67985\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3921 - val_loss: 0.7298\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67985\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3867 - val_loss: 0.7254\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67985\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3769 - val_loss: 0.7283\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67985\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3761 - val_loss: 0.7400\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67985\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3742 - val_loss: 0.7538\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67985\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3755 - val_loss: 0.7410\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67985\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3762 - val_loss: 0.7324\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67985\n",
            "Epoch 91/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3742 - val_loss: 0.7553\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67985\n",
            "Epoch 00091: early stopping\n",
            "fold #3 Log Loss: 0.6798537820650108\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 17s 12ms/step - loss: 1.1494 - val_loss: 0.9147\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91470, saving model to Radiant_NN_4.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9177 - val_loss: 0.8532\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91470 to 0.85317, saving model to Radiant_NN_4.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8731 - val_loss: 0.8262\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85317 to 0.82620, saving model to Radiant_NN_4.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8403 - val_loss: 0.8076\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82620 to 0.80763, saving model to Radiant_NN_4.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8040 - val_loss: 0.7981\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80763 to 0.79808, saving model to Radiant_NN_4.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7676 - val_loss: 0.7680\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79808 to 0.76799, saving model to Radiant_NN_4.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7538 - val_loss: 0.7542\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76799 to 0.75418, saving model to Radiant_NN_4.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7399 - val_loss: 0.7640\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.75418\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7209 - val_loss: 0.7361\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75418 to 0.73609, saving model to Radiant_NN_4.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7032 - val_loss: 0.7366\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.73609\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6932 - val_loss: 0.7246\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73609 to 0.72457, saving model to Radiant_NN_4.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6745 - val_loss: 0.7236\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72457 to 0.72357, saving model to Radiant_NN_4.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6649 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72357 to 0.71032, saving model to Radiant_NN_4.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6626 - val_loss: 0.7186\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.71032\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6391 - val_loss: 0.7186\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.71032\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6355 - val_loss: 0.7022\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71032 to 0.70220, saving model to Radiant_NN_4.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6180 - val_loss: 0.6949\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.70220 to 0.69488, saving model to Radiant_NN_4.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6225 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69488\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6112 - val_loss: 0.7062\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69488\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5964 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.69488 to 0.69395, saving model to Radiant_NN_4.h5\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5912 - val_loss: 0.6994\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69395\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5840 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69395\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5770 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69395\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5691 - val_loss: 0.6927\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69395 to 0.69267, saving model to Radiant_NN_4.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5556 - val_loss: 0.6980\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69267\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5636 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69267\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5556 - val_loss: 0.6844\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.69267 to 0.68440, saving model to Radiant_NN_4.h5\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5465 - val_loss: 0.6947\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68440\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5350 - val_loss: 0.6840\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.68440 to 0.68396, saving model to Radiant_NN_4.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5351 - val_loss: 0.6840\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68396\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5238 - val_loss: 0.6913\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68396\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5245 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68396\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5116 - val_loss: 0.6862\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68396\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5214 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.68396 to 0.68131, saving model to Radiant_NN_4.h5\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5092 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68131\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5073 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68131\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5034 - val_loss: 0.6948\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68131\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5036 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68131\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4873 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68131\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4871 - val_loss: 0.6898\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68131\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4867 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68131\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4945 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68131\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4745 - val_loss: 0.6940\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68131\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4717 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68131\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4668 - val_loss: 0.6951\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68131\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4666 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68131\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4669 - val_loss: 0.6849\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68131\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4639 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68131\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4630 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68131\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4587 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68131\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4578 - val_loss: 0.6948\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68131\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4517 - val_loss: 0.7113\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68131\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4577 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68131\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4470 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68131\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4439 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68131\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4427 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68131\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4410 - val_loss: 0.7024\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68131\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4426 - val_loss: 0.7001\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68131\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4305 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68131\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4318 - val_loss: 0.7046\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68131\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4282 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68131\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4346 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68131\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4318 - val_loss: 0.6984\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68131\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4198 - val_loss: 0.7099\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68131\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4184 - val_loss: 0.6879\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68131\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4271 - val_loss: 0.7045\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68131\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4187 - val_loss: 0.7114\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.68131\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4132 - val_loss: 0.7036\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68131\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4120 - val_loss: 0.7160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.68131\n",
            "Epoch 00069: early stopping\n",
            "fold #4 Log Loss: 0.681310189385195\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 17s 13ms/step - loss: 1.1435 - val_loss: 0.8931\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89309, saving model to Radiant_NN_5.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9239 - val_loss: 0.8456\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89309 to 0.84561, saving model to Radiant_NN_5.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8660 - val_loss: 0.8189\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84561 to 0.81895, saving model to Radiant_NN_5.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8413 - val_loss: 0.7955\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.81895 to 0.79550, saving model to Radiant_NN_5.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8086 - val_loss: 0.7764\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79550 to 0.77643, saving model to Radiant_NN_5.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7839 - val_loss: 0.7571\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77643 to 0.75709, saving model to Radiant_NN_5.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7667 - val_loss: 0.7419\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.75709 to 0.74194, saving model to Radiant_NN_5.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7455 - val_loss: 0.7297\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74194 to 0.72972, saving model to Radiant_NN_5.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7164 - val_loss: 0.7229\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.72972 to 0.72289, saving model to Radiant_NN_5.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7017 - val_loss: 0.7292\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72289\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6936 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72289 to 0.71720, saving model to Radiant_NN_5.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6767 - val_loss: 0.7218\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.71720\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6664 - val_loss: 0.7124\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71720 to 0.71236, saving model to Radiant_NN_5.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6525 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71236 to 0.70314, saving model to Radiant_NN_5.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6439 - val_loss: 0.7025\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70314 to 0.70250, saving model to Radiant_NN_5.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6291 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.70250\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6235 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.70250 to 0.68665, saving model to Radiant_NN_5.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6089 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68665\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6074 - val_loss: 0.7002\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68665\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5955 - val_loss: 0.6955\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68665\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5917 - val_loss: 0.6857\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68665 to 0.68569, saving model to Radiant_NN_5.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5765 - val_loss: 0.6862\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68569\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5785 - val_loss: 0.6861\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68569\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5647 - val_loss: 0.6686\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68569 to 0.66862, saving model to Radiant_NN_5.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5537 - val_loss: 0.6818\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66862\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5542 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66862\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5458 - val_loss: 0.6705\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66862\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5530 - val_loss: 0.6749\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66862\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5370 - val_loss: 0.6803\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.66862\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5344 - val_loss: 0.6826\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66862\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5260 - val_loss: 0.6711\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66862\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5178 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66862\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5178 - val_loss: 0.6765\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66862\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5121 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66862\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5100 - val_loss: 0.6809\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66862\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5036 - val_loss: 0.6723\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66862\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5083 - val_loss: 0.6786\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66862\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4957 - val_loss: 0.6749\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66862\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4877 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66862\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4950 - val_loss: 0.6674\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.66862 to 0.66745, saving model to Radiant_NN_5.h5\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4832 - val_loss: 0.6750\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66745\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4840 - val_loss: 0.6780\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66745\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4754 - val_loss: 0.6735\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66745\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4738 - val_loss: 0.6739\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66745\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4764 - val_loss: 0.6787\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66745\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4629 - val_loss: 0.6682\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66745\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4664 - val_loss: 0.6691\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66745\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4670 - val_loss: 0.6894\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66745\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4674 - val_loss: 0.6820\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66745\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4478 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66745\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4564 - val_loss: 0.6870\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66745\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4546 - val_loss: 0.6737\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66745\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4558 - val_loss: 0.6723\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66745\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4447 - val_loss: 0.6786\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66745\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4507 - val_loss: 0.6804\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66745\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4394 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66745\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4450 - val_loss: 0.6836\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.66745\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4333 - val_loss: 0.6859\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.66745\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4304 - val_loss: 0.7080\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.66745\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4344 - val_loss: 0.6695\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.66745\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4270 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.66745\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4294 - val_loss: 0.6989\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.66745\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4252 - val_loss: 0.6961\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.66745\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4282 - val_loss: 0.7048\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.66745\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4227 - val_loss: 0.6904\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.66745\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4231 - val_loss: 0.6824\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.66745\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4166 - val_loss: 0.6944\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.66745\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4174 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.66745\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4059 - val_loss: 0.6947\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.66745\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4026 - val_loss: 0.7107\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66745\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4080 - val_loss: 0.6968\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66745\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4030 - val_loss: 0.7083\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66745\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4086 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.66745\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4044 - val_loss: 0.7263\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.66745\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4046 - val_loss: 0.6777\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.66745\n",
            "Epoch 00075: early stopping\n",
            "fold #5 Log Loss: 0.6674488973976093\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 14s 10ms/step - loss: 1.1501 - val_loss: 0.9050\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90497, saving model to Radiant_NN_6.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9318 - val_loss: 0.8564\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90497 to 0.85640, saving model to Radiant_NN_6.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8813 - val_loss: 0.8254\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85640 to 0.82542, saving model to Radiant_NN_6.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8483 - val_loss: 0.7944\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82542 to 0.79445, saving model to Radiant_NN_6.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8054 - val_loss: 0.7715\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79445 to 0.77151, saving model to Radiant_NN_6.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7845 - val_loss: 0.7617\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77151 to 0.76170, saving model to Radiant_NN_6.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7696 - val_loss: 0.7630\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.76170\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7483 - val_loss: 0.7470\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76170 to 0.74701, saving model to Radiant_NN_6.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7234 - val_loss: 0.7388\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74701 to 0.73880, saving model to Radiant_NN_6.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7180 - val_loss: 0.7352\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.73880 to 0.73522, saving model to Radiant_NN_6.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7042 - val_loss: 0.7267\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73522 to 0.72666, saving model to Radiant_NN_6.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6875 - val_loss: 0.7214\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72666 to 0.72136, saving model to Radiant_NN_6.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6854 - val_loss: 0.7219\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.72136\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6605 - val_loss: 0.7159\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72136 to 0.71594, saving model to Radiant_NN_6.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6517 - val_loss: 0.7081\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71594 to 0.70812, saving model to Radiant_NN_6.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6397 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70812 to 0.69774, saving model to Radiant_NN_6.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6250 - val_loss: 0.6942\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69774 to 0.69422, saving model to Radiant_NN_6.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6205 - val_loss: 0.7040\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69422\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6104 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69422 to 0.69021, saving model to Radiant_NN_6.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5964 - val_loss: 0.6935\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69021\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.6029 - val_loss: 0.6892\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69021 to 0.68916, saving model to Radiant_NN_6.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5955 - val_loss: 0.6905\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68916\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5750 - val_loss: 0.6779\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68916 to 0.67793, saving model to Radiant_NN_6.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5677 - val_loss: 0.6864\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67793\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5654 - val_loss: 0.6798\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67793\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5655 - val_loss: 0.6795\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67793\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5562 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67793\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5568 - val_loss: 0.6975\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67793\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5449 - val_loss: 0.6871\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67793\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5402 - val_loss: 0.6949\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67793\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5224 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67793\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5198 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67793\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5186 - val_loss: 0.6747\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.67793 to 0.67473, saving model to Radiant_NN_6.h5\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5157 - val_loss: 0.6847\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67473\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5206 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67473\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5060 - val_loss: 0.6863\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67473\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4960 - val_loss: 0.6711\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.67473 to 0.67112, saving model to Radiant_NN_6.h5\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4956 - val_loss: 0.6847\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67112\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4968 - val_loss: 0.6783\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67112\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4906 - val_loss: 0.6792\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67112\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4898 - val_loss: 0.6736\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67112\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4889 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67112\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4857 - val_loss: 0.6859\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67112\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4693 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67112\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4693 - val_loss: 0.6864\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67112\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.4682 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67112\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4655 - val_loss: 0.6876\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67112\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4633 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67112\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4553 - val_loss: 0.6802\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67112\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4585 - val_loss: 0.6745\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67112\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4574 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67112\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4481 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67112\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4503 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67112\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4554 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67112\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4442 - val_loss: 0.6857\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67112\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4391 - val_loss: 0.6913\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67112\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4367 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67112\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4400 - val_loss: 0.6942\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67112\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4485 - val_loss: 0.6888\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67112\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4233 - val_loss: 0.6808\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67112\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4287 - val_loss: 0.6991\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67112\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4252 - val_loss: 0.6873\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67112\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4291 - val_loss: 0.7239\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67112\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4229 - val_loss: 0.7034\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67112\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4176 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67112\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4209 - val_loss: 0.7025\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67112\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4147 - val_loss: 0.7081\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67112\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4095 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67112\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4126 - val_loss: 0.7036\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67112\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4079 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67112\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4111 - val_loss: 0.7223\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67112\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4061 - val_loss: 0.6892\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67112\n",
            "Epoch 00072: early stopping\n",
            "fold #6 Log Loss: 0.6711224257964408\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 17s 12ms/step - loss: 1.1534 - val_loss: 0.9119\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91189, saving model to Radiant_NN_7.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.9255 - val_loss: 0.8627\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91189 to 0.86270, saving model to Radiant_NN_7.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8702 - val_loss: 0.8142\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86270 to 0.81422, saving model to Radiant_NN_7.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.8379 - val_loss: 0.8134\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.81422 to 0.81344, saving model to Radiant_NN_7.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8117 - val_loss: 0.7703\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81344 to 0.77033, saving model to Radiant_NN_7.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.7787 - val_loss: 0.7740\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.77033\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7601 - val_loss: 0.7572\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77033 to 0.75718, saving model to Radiant_NN_7.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7423 - val_loss: 0.7410\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75718 to 0.74100, saving model to Radiant_NN_7.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7258 - val_loss: 0.7344\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74100 to 0.73436, saving model to Radiant_NN_7.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7014 - val_loss: 0.7274\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.73436 to 0.72744, saving model to Radiant_NN_7.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6982 - val_loss: 0.7334\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.72744\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6789 - val_loss: 0.7320\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.72744\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6688 - val_loss: 0.7193\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72744 to 0.71926, saving model to Radiant_NN_7.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6526 - val_loss: 0.7071\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71926 to 0.70706, saving model to Radiant_NN_7.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6526 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70706 to 0.69927, saving model to Radiant_NN_7.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6355 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69927\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6274 - val_loss: 0.7038\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69927\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6154 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69927\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6117 - val_loss: 0.6964\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69927 to 0.69641, saving model to Radiant_NN_7.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6066 - val_loss: 0.6997\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69641\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5999 - val_loss: 0.6936\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69641 to 0.69358, saving model to Radiant_NN_7.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5864 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69358\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5819 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69358\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5719 - val_loss: 0.6972\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.69358\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5636 - val_loss: 0.7040\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69358\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5633 - val_loss: 0.7069\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69358\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5461 - val_loss: 0.6952\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69358\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5443 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.69358 to 0.69007, saving model to Radiant_NN_7.h5\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5451 - val_loss: 0.6954\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69007\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5355 - val_loss: 0.6885\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.69007 to 0.68847, saving model to Radiant_NN_7.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5305 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.68847 to 0.67771, saving model to Radiant_NN_7.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5251 - val_loss: 0.6852\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67771\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5260 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67771\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5106 - val_loss: 0.6846\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67771\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5113 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67771\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5103 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67771\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5020 - val_loss: 0.6837\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67771\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4944 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67771\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4945 - val_loss: 0.6947\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67771\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4937 - val_loss: 0.7079\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67771\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4853 - val_loss: 0.6858\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67771\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4776 - val_loss: 0.6959\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67771\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4802 - val_loss: 0.6832\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67771\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4792 - val_loss: 0.6839\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67771\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4712 - val_loss: 0.6899\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67771\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4654 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67771\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4630 - val_loss: 0.6950\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67771\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4670 - val_loss: 0.6889\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67771\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4649 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67771\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4619 - val_loss: 0.6753\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.67771 to 0.67527, saving model to Radiant_NN_7.h5\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4589 - val_loss: 0.6975\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67527\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4482 - val_loss: 0.7180\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67527\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4442 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67527\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4420 - val_loss: 0.7063\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67527\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4444 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67527\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4319 - val_loss: 0.7000\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67527\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4375 - val_loss: 0.7144\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67527\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4380 - val_loss: 0.7052\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67527\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4352 - val_loss: 0.6947\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67527\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4323 - val_loss: 0.6888\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67527\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4232 - val_loss: 0.7091\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67527\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4318 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67527\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4136 - val_loss: 0.6934\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67527\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4171 - val_loss: 0.6824\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67527\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4190 - val_loss: 0.6926\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67527\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4138 - val_loss: 0.7187\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67527\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4144 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67527\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4127 - val_loss: 0.7186\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67527\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4107 - val_loss: 0.7094\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67527\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4042 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67527\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4028 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67527\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4122 - val_loss: 0.6991\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67527\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4054 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67527\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4089 - val_loss: 0.7219\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67527\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3980 - val_loss: 0.6949\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67527\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4018 - val_loss: 0.7194\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67527\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3962 - val_loss: 0.7035\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67527\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3891 - val_loss: 0.7144\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67527\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.3951 - val_loss: 0.7059\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67527\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3923 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67527\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3857 - val_loss: 0.7228\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67527\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3880 - val_loss: 0.7137\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67527\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3946 - val_loss: 0.6980\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67527\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.3913 - val_loss: 0.7194\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67527\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3885 - val_loss: 0.7408\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67527\n",
            "Epoch 00085: early stopping\n",
            "fold #7 Log Loss: 0.6752729144931392\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 1.1531 - val_loss: 0.8898\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.88981, saving model to Radiant_NN_8.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9281 - val_loss: 0.8400\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.88981 to 0.83998, saving model to Radiant_NN_8.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8691 - val_loss: 0.8058\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83998 to 0.80576, saving model to Radiant_NN_8.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.8377 - val_loss: 0.7918\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.80576 to 0.79176, saving model to Radiant_NN_8.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8115 - val_loss: 0.7649\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79176 to 0.76493, saving model to Radiant_NN_8.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.7805 - val_loss: 0.7570\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.76493 to 0.75696, saving model to Radiant_NN_8.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7632 - val_loss: 0.7460\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.75696 to 0.74597, saving model to Radiant_NN_8.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7474 - val_loss: 0.7338\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74597 to 0.73383, saving model to Radiant_NN_8.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.7276 - val_loss: 0.7251\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.73383 to 0.72505, saving model to Radiant_NN_8.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7085 - val_loss: 0.7194\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.72505 to 0.71936, saving model to Radiant_NN_8.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6949 - val_loss: 0.7109\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.71936 to 0.71095, saving model to Radiant_NN_8.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6853 - val_loss: 0.7072\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.71095 to 0.70724, saving model to Radiant_NN_8.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6682 - val_loss: 0.7041\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.70724 to 0.70412, saving model to Radiant_NN_8.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6610 - val_loss: 0.7104\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70412\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6505 - val_loss: 0.6911\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70412 to 0.69114, saving model to Radiant_NN_8.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6282 - val_loss: 0.6909\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.69114 to 0.69093, saving model to Radiant_NN_8.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6305 - val_loss: 0.6914\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69093\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6123 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.69093 to 0.68810, saving model to Radiant_NN_8.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6086 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68810 to 0.68543, saving model to Radiant_NN_8.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6052 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68543\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5918 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68543 to 0.67846, saving model to Radiant_NN_8.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5842 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67846\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5784 - val_loss: 0.6755\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.67846 to 0.67552, saving model to Radiant_NN_8.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5649 - val_loss: 0.6753\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.67552 to 0.67529, saving model to Radiant_NN_8.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5655 - val_loss: 0.6684\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.67529 to 0.66836, saving model to Radiant_NN_8.h5\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5604 - val_loss: 0.6748\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66836\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5534 - val_loss: 0.6827\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66836\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5470 - val_loss: 0.6705\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66836\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5381 - val_loss: 0.6762\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.66836\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5369 - val_loss: 0.6680\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.66836 to 0.66804, saving model to Radiant_NN_8.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5289 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66804\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5262 - val_loss: 0.6694\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66804\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5220 - val_loss: 0.6743\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66804\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5155 - val_loss: 0.6817\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66804\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5150 - val_loss: 0.6715\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66804\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5085 - val_loss: 0.6755\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66804\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5063 - val_loss: 0.6626\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.66804 to 0.66261, saving model to Radiant_NN_8.h5\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5012 - val_loss: 0.6764\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66261\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4984 - val_loss: 0.6718\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66261\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4889 - val_loss: 0.6657\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66261\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4914 - val_loss: 0.6664\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66261\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4845 - val_loss: 0.6582\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.66261 to 0.65819, saving model to Radiant_NN_8.h5\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4769 - val_loss: 0.6739\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65819\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4785 - val_loss: 0.6770\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65819\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4689 - val_loss: 0.6701\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65819\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4748 - val_loss: 0.6866\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65819\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4626 - val_loss: 0.6821\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65819\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4628 - val_loss: 0.6700\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65819\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4635 - val_loss: 0.6683\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65819\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4636 - val_loss: 0.6738\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65819\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4499 - val_loss: 0.6775\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65819\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4563 - val_loss: 0.6672\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65819\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4574 - val_loss: 0.6671\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65819\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4434 - val_loss: 0.6883\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65819\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4509 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.65819\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4452 - val_loss: 0.6880\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.65819\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4519 - val_loss: 0.6702\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.65819\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4384 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.65819\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4335 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.65819\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4300 - val_loss: 0.7119\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.65819\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4344 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.65819\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4279 - val_loss: 0.6792\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.65819\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4282 - val_loss: 0.6840\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.65819\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4227 - val_loss: 0.6786\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.65819\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4227 - val_loss: 0.6692\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.65819\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4154 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.65819\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4144 - val_loss: 0.6761\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.65819\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4155 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.65819\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4132 - val_loss: 0.6728\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.65819\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4146 - val_loss: 0.6778\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.65819\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4115 - val_loss: 0.6907\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.65819\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4118 - val_loss: 0.6862\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.65819\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4065 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.65819\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4027 - val_loss: 0.6743\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.65819\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3992 - val_loss: 0.6953\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.65819\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4020 - val_loss: 0.7125\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.65819\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4024 - val_loss: 0.6862\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.65819\n",
            "Epoch 00077: early stopping\n",
            "fold #8 Log Loss: 0.6581919161939235\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 1.1525 - val_loss: 0.8836\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.88359, saving model to Radiant_NN_9.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.9253 - val_loss: 0.8484\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.88359 to 0.84844, saving model to Radiant_NN_9.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8743 - val_loss: 0.8294\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84844 to 0.82944, saving model to Radiant_NN_9.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.8316 - val_loss: 0.7956\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82944 to 0.79559, saving model to Radiant_NN_9.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8072 - val_loss: 0.7664\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79559 to 0.76644, saving model to Radiant_NN_9.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7858 - val_loss: 0.7694\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.76644\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7653 - val_loss: 0.7546\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76644 to 0.75457, saving model to Radiant_NN_9.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7339 - val_loss: 0.7464\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75457 to 0.74643, saving model to Radiant_NN_9.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7163 - val_loss: 0.7305\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74643 to 0.73052, saving model to Radiant_NN_9.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7093 - val_loss: 0.7270\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.73052 to 0.72701, saving model to Radiant_NN_9.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6930 - val_loss: 0.7180\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72701 to 0.71801, saving model to Radiant_NN_9.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6800 - val_loss: 0.7266\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.71801\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.6671 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71801 to 0.71719, saving model to Radiant_NN_9.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6525 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71719 to 0.71162, saving model to Radiant_NN_9.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6402 - val_loss: 0.7114\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71162 to 0.71144, saving model to Radiant_NN_9.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6310 - val_loss: 0.6995\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71144 to 0.69952, saving model to Radiant_NN_9.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6199 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69952 to 0.69564, saving model to Radiant_NN_9.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6164 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.69564 to 0.69255, saving model to Radiant_NN_9.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6046 - val_loss: 0.7087\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69255\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5915 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.69255 to 0.68999, saving model to Radiant_NN_9.h5\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.5913 - val_loss: 0.7015\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68999\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5813 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68999\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5690 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68999 to 0.68423, saving model to Radiant_NN_9.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5672 - val_loss: 0.6928\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68423\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5650 - val_loss: 0.6863\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68423\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5555 - val_loss: 0.6863\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68423\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5496 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.68423 to 0.67894, saving model to Radiant_NN_9.h5\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5429 - val_loss: 0.6880\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67894\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5345 - val_loss: 0.6968\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67894\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5386 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67894\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5250 - val_loss: 0.6827\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67894\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5204 - val_loss: 0.6931\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67894\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5125 - val_loss: 0.6799\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67894\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5111 - val_loss: 0.6858\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67894\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5041 - val_loss: 0.6953\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67894\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5027 - val_loss: 0.6909\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67894\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4892 - val_loss: 0.6931\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67894\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4993 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67894\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4865 - val_loss: 0.6775\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.67894 to 0.67748, saving model to Radiant_NN_9.h5\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4820 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67748\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4823 - val_loss: 0.6944\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67748\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4764 - val_loss: 0.6899\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67748\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4705 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67748\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4728 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67748\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4734 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67748\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4605 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67748\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4672 - val_loss: 0.6989\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67748\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4649 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67748\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4520 - val_loss: 0.7020\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67748\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4571 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67748\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4495 - val_loss: 0.6997\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67748\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4472 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67748\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4429 - val_loss: 0.7071\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67748\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4380 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67748\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4359 - val_loss: 0.7011\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67748\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4276 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67748\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4364 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67748\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4361 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67748\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4352 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67748\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4327 - val_loss: 0.7185\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67748\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4316 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67748\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4255 - val_loss: 0.7048\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67748\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4221 - val_loss: 0.7069\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67748\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4177 - val_loss: 0.6990\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67748\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4145 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67748\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4176 - val_loss: 0.7164\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67748\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4102 - val_loss: 0.7209\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67748\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4107 - val_loss: 0.7208\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67748\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4026 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67748\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4075 - val_loss: 0.7290\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67748\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4186 - val_loss: 0.7049\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67748\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4196 - val_loss: 0.7121\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67748\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4058 - val_loss: 0.7189\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67748\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3960 - val_loss: 0.7014\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67748\n",
            "Epoch 00074: early stopping\n",
            "fold #9 Log Loss: 0.677476375094605\n",
            "Full Log loss 0.6705216445432275\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T08:31:02.892833Z",
          "iopub.status.busy": "2021-09-29T08:31:02.892047Z",
          "iopub.status.idle": "2021-09-29T08:31:02.959385Z",
          "shell.execute_reply": "2021-09-29T08:31:02.960393Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.936064Z"
        },
        "id": "a5951e52",
        "papermill": {
          "duration": 45.514666,
          "end_time": "2021-09-29T08:31:02.960609",
          "exception": false,
          "start_time": "2021-09-29T08:30:17.445943",
          "status": "completed"
        },
        "tags": [],
        "outputId": "a550112e-2325-4679-d7f5-168385e5acc9"
      },
      "source": [
        "print('NN LOG LOSS :',log_loss(y_train,y_oof)) "
      ],
      "id": "a5951e52",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN LOG LOSS : 0.6705215626569802\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T08:34:03.944891Z",
          "iopub.status.busy": "2021-09-29T08:34:03.944074Z",
          "iopub.status.idle": "2021-09-29T08:34:03.967018Z",
          "shell.execute_reply": "2021-09-29T08:34:03.967431Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.939863Z"
        },
        "id": "bda383f5",
        "papermill": {
          "duration": 45.099608,
          "end_time": "2021-09-29T08:34:03.967572",
          "exception": false,
          "start_time": "2021-09-29T08:33:18.867964",
          "status": "completed"
        },
        "tags": [],
        "outputId": "40599105-d725-4abe-d8da-5304a35489f3"
      },
      "source": [
        "# In this part we format the DataFrame to have column names and order similar to the sample submission file. \n",
        "pred_df = pd.DataFrame(y_test)\n",
        "pred_df = pred_df.rename(columns={\n",
        "    0:'Crop_ID_1',\n",
        "    1:'Crop_ID_2', \n",
        "    2:'Crop_ID_3',\n",
        "    3:'Crop_ID_4',\n",
        "    4:'Crop_ID_5',\n",
        "    5:'Crop_ID_6',\n",
        "    6:'Crop_ID_7',\n",
        "    7:'Crop_ID_8',\n",
        "    8:'Crop_ID_9'\n",
        "})\n",
        "pred_df['field_id'] = Test['field_id'].astype('int').values\n",
        "pred_df = pred_df[['field_id', 'Crop_ID_1', 'Crop_ID_2', 'Crop_ID_3', 'Crop_ID_4', 'Crop_ID_5', 'Crop_ID_6', 'Crop_ID_7', 'Crop_ID_8', 'Crop_ID_9']]\n",
        "pred_df.head()"
      ],
      "id": "bda383f5",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>field_id</th>\n",
              "      <th>Crop_ID_1</th>\n",
              "      <th>Crop_ID_2</th>\n",
              "      <th>Crop_ID_3</th>\n",
              "      <th>Crop_ID_4</th>\n",
              "      <th>Crop_ID_5</th>\n",
              "      <th>Crop_ID_6</th>\n",
              "      <th>Crop_ID_7</th>\n",
              "      <th>Crop_ID_8</th>\n",
              "      <th>Crop_ID_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.055811</td>\n",
              "      <td>0.297380</td>\n",
              "      <td>0.011131</td>\n",
              "      <td>0.000204</td>\n",
              "      <td>0.173487</td>\n",
              "      <td>2.992985e-01</td>\n",
              "      <td>1.538131e-01</td>\n",
              "      <td>7.317784e-03</td>\n",
              "      <td>1.557601e-03</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39</td>\n",
              "      <td>0.768878</td>\n",
              "      <td>0.168540</td>\n",
              "      <td>0.015459</td>\n",
              "      <td>0.015719</td>\n",
              "      <td>0.021324</td>\n",
              "      <td>9.927223e-03</td>\n",
              "      <td>1.141302e-04</td>\n",
              "      <td>2.858439e-05</td>\n",
              "      <td>1.057026e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49</td>\n",
              "      <td>0.008941</td>\n",
              "      <td>0.397148</td>\n",
              "      <td>0.000055</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.000510</td>\n",
              "      <td>5.465559e-01</td>\n",
              "      <td>3.995302e-02</td>\n",
              "      <td>6.780147e-03</td>\n",
              "      <td>1.868593e-05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>0.000034</td>\n",
              "      <td>0.000092</td>\n",
              "      <td>0.999768</td>\n",
              "      <td>0.000104</td>\n",
              "      <td>5.213484e-07</td>\n",
              "      <td>3.247506e-07</td>\n",
              "      <td>1.512979e-10</td>\n",
              "      <td>2.743625e-08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>0.884511</td>\n",
              "      <td>0.015084</td>\n",
              "      <td>0.000476</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000644</td>\n",
              "      <td>2.719114e-02</td>\n",
              "      <td>6.365343e-02</td>\n",
              "      <td>8.380454e-03</td>\n",
              "      <td>6.204341e-06</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   field_id  Crop_ID_1  Crop_ID_2  Crop_ID_3  Crop_ID_4  Crop_ID_5  \\\n",
              "0        30   0.055811   0.297380   0.011131   0.000204   0.173487   \n",
              "1        39   0.768878   0.168540   0.015459   0.015719   0.021324   \n",
              "2        49   0.008941   0.397148   0.000055   0.000038   0.000510   \n",
              "3        54   0.000001   0.000034   0.000092   0.999768   0.000104   \n",
              "4        56   0.884511   0.015084   0.000476   0.000054   0.000644   \n",
              "\n",
              "      Crop_ID_6     Crop_ID_7     Crop_ID_8     Crop_ID_9  \n",
              "0  2.992985e-01  1.538131e-01  7.317784e-03  1.557601e-03  \n",
              "1  9.927223e-03  1.141302e-04  2.858439e-05  1.057026e-05  \n",
              "2  5.465559e-01  3.995302e-02  6.780147e-03  1.868593e-05  \n",
              "3  5.213484e-07  3.247506e-07  1.512979e-10  2.743625e-08  \n",
              "4  2.719114e-02  6.365343e-02  8.380454e-03  6.204341e-06  "
            ]
          },
          "execution_count": 58,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T08:35:34.782082Z",
          "iopub.status.busy": "2021-09-29T08:35:34.781389Z",
          "iopub.status.idle": "2021-09-29T08:35:35.415037Z",
          "shell.execute_reply": "2021-09-29T08:35:35.415458Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.941611Z"
        },
        "id": "b42db2c8",
        "papermill": {
          "duration": 45.812822,
          "end_time": "2021-09-29T08:35:35.415643",
          "exception": false,
          "start_time": "2021-09-29T08:34:49.602821",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Write the predicted probabilites to a csv for submission\n",
        "pred_df.to_csv('S1_NNAttention.csv', index=False)"
      ],
      "id": "b42db2c8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0b6e2b80",
        "papermill": {
          "duration": 45.272972,
          "end_time": "2021-09-29T08:37:06.273617",
          "exception": false,
          "start_time": "2021-09-29T08:36:21.000645",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "np.save('S1_oof_NNAttention.npy',y_oof)"
      ],
      "id": "0b6e2b80",
      "execution_count": null,
      "outputs": []
    }
  ]
}