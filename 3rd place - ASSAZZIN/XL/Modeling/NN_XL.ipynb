{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 9182.351162,
      "end_time": "2021-09-29T09:20:46.524792",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-29T06:47:44.173630",
      "version": "2.3.3"
    },
    "colab": {
      "name": "NN_XL.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "acc40ed0",
        "papermill": {
          "duration": 0.044827,
          "end_time": "2021-09-29T05:46:43.471126",
          "exception": false,
          "start_time": "2021-09-29T05:46:43.426299",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# SETUP"
      ],
      "id": "acc40ed0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eljNtsFQoozD"
      },
      "source": [
        "!pip install -r requirements_kaggle.txt -q"
      ],
      "id": "eljNtsFQoozD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9LH6_Vs9qkt"
      },
      "source": [
        "> To speed up the review process , i provided the ***drive id*** of the data i've created from the Train creation folder noteboooks .\n",
        "---\n",
        "> I  also add each data drive link in the Readme Pdf file attached with this solution\n"
      ],
      "id": "D9LH6_Vs9qkt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:46:44.463015Z",
          "iopub.status.busy": "2021-09-29T05:46:44.462240Z",
          "iopub.status.idle": "2021-09-29T05:47:03.449160Z",
          "shell.execute_reply": "2021-09-29T05:47:03.448592Z",
          "shell.execute_reply.started": "2021-09-29T05:34:16.100720Z"
        },
        "papermill": {
          "duration": 19.041918,
          "end_time": "2021-09-29T05:47:03.449286",
          "exception": false,
          "start_time": "2021-09-29T05:46:44.407368",
          "status": "completed"
        },
        "tags": [],
        "id": "37cb9962",
        "outputId": "1aa69c20-249c-45db-ce5a-e0084580d80a"
      },
      "source": [
        "!pip install -q gdown"
      ],
      "id": "37cb9962",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:03.557856Z",
          "iopub.status.busy": "2021-09-29T05:47:03.557293Z",
          "iopub.status.idle": "2021-09-29T05:47:32.307118Z",
          "shell.execute_reply": "2021-09-29T05:47:32.306518Z",
          "shell.execute_reply.started": "2021-09-29T05:34:34.289447Z"
        },
        "papermill": {
          "duration": 28.811774,
          "end_time": "2021-09-29T05:47:32.307260",
          "exception": false,
          "start_time": "2021-09-29T05:47:03.495486",
          "status": "completed"
        },
        "tags": [],
        "id": "c960ed16"
      },
      "source": [
        "!gdown --id 1-Q7vaYnSEMwQ3jOrm3KwkcqWNI8iEzXn\n",
        "!gdown --id 1-cl7HvhPuIywmu8lPaK6B1Qb4M6RCIxb\n",
        "!gdown --id 1hNRbtcqd9F6stMOK1xAZApDITwAjiSDJ\n",
        "!gdown --id 1-QCmWsNGREXuWArifN0nD_Sp4hJxf0tu"
      ],
      "id": "c960ed16",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:32.422988Z",
          "iopub.status.busy": "2021-09-29T05:47:32.418654Z",
          "iopub.status.idle": "2021-09-29T05:47:48.767601Z",
          "shell.execute_reply": "2021-09-29T05:47:48.767129Z",
          "shell.execute_reply.started": "2021-09-29T05:35:04.167348Z"
        },
        "papermill": {
          "duration": 16.408707,
          "end_time": "2021-09-29T05:47:48.767731",
          "exception": false,
          "start_time": "2021-09-29T05:47:32.359024",
          "status": "completed"
        },
        "tags": [],
        "id": "c911e3b8"
      },
      "source": [
        "!gdown --id 10-rLRxkX5Lyf92oM0xtUz1yaTXN3FBGO\n",
        "!gdown --id 1-9IVDUWu6lhY-DJnfzrjrJgQkafxKS8f\n",
        "!gdown --id 1-47L_1NKLeVgW1vWmqXXXCuWZ3gwZWsS\n",
        "!gdown --id 1-aO4FEtv5CF-ZOcxDSO3jGEzPcIFdxgP"
      ],
      "id": "c911e3b8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:47:48.886560Z",
          "iopub.status.busy": "2021-09-29T05:47:48.886023Z",
          "iopub.status.idle": "2021-09-29T05:48:15.872715Z",
          "shell.execute_reply": "2021-09-29T05:48:15.872240Z",
          "shell.execute_reply.started": "2021-09-29T05:35:20.268672Z"
        },
        "papermill": {
          "duration": 27.052166,
          "end_time": "2021-09-29T05:48:15.872858",
          "exception": false,
          "start_time": "2021-09-29T05:47:48.820692",
          "status": "completed"
        },
        "tags": [],
        "id": "e99f42ee"
      },
      "source": [
        "!gdown --id 1-zEJ24ORxrK-RXxxoj770VNHOnGpB0dv\n",
        "!gdown --id 1-_GnYjOVN8qUB8k1alLBnf3p8_8-GCJj\n",
        "!gdown --id 1-8J_xFgI0WKT5UXFnfH4q1KUw_KgNY37\n",
        "!gdown --id 1-a55a7N6a4SoqolPF_wI4C6Q70u_d7Hj"
      ],
      "id": "e99f42ee",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:48:27.606024Z",
          "iopub.status.busy": "2021-09-29T05:48:27.601042Z",
          "iopub.status.idle": "2021-09-29T05:48:54.178968Z",
          "shell.execute_reply": "2021-09-29T05:48:54.178406Z",
          "shell.execute_reply.started": "2021-09-29T05:35:56.055800Z"
        },
        "papermill": {
          "duration": 26.648835,
          "end_time": "2021-09-29T05:48:54.179100",
          "exception": false,
          "start_time": "2021-09-29T05:48:27.530265",
          "status": "completed"
        },
        "tags": [],
        "id": "c0b89fc9"
      },
      "source": [
        "!gdown --id 1-kzjYxxLepIC8slA9WryJe3ahShANXzt\n",
        "!gdown --id 1-_-Ps0FoxEWEVCrdYSrCUTLPAFK5CFcs\n",
        "!gdown --id 1-BgXQwmXqBuk_P8VtvLfdLqy83dv56Kz\n",
        "!gdown --id 1-hQGF2TNBbsy3jsGNtndmK55egbdFDjs"
      ],
      "id": "c0b89fc9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38e50227",
        "papermill": {
          "duration": 0.069612,
          "end_time": "2021-09-29T05:49:44.573383",
          "exception": false,
          "start_time": "2021-09-29T05:49:44.503771",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## LIBRARIES"
      ],
      "id": "38e50227"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:44.719111Z",
          "iopub.status.busy": "2021-09-29T05:49:44.718298Z",
          "iopub.status.idle": "2021-09-29T05:49:49.733238Z",
          "shell.execute_reply": "2021-09-29T05:49:49.732694Z",
          "shell.execute_reply.started": "2021-09-29T05:37:19.206869Z"
        },
        "papermill": {
          "duration": 5.090552,
          "end_time": "2021-09-29T05:49:49.733371",
          "exception": false,
          "start_time": "2021-09-29T05:49:44.642819",
          "status": "completed"
        },
        "tags": [],
        "id": "0fa6bc39",
        "outputId": "74d2fdc1-1879-4f95-a4ba-4381b722e52a"
      },
      "source": [
        "#import necessary dependecies\n",
        "import os\n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm \n",
        "import copy\n",
        "\n",
        "from sklearn.preprocessing import  LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "# tf \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate , Input ,concatenate ,add\n",
        "from tensorflow.keras.models import Sequential , Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.initializers import glorot_normal\n",
        "\n",
        "# fix seed\n",
        "tf.random.set_seed(111)\n",
        "np.random.seed(111)\n",
        "random.seed(111)"
      ],
      "id": "0fa6bc39",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-29 05:49:46.178613: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3c5bff71",
        "papermill": {
          "duration": 0.068251,
          "end_time": "2021-09-29T05:49:49.872081",
          "exception": false,
          "start_time": "2021-09-29T05:49:49.803830",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Train Creation"
      ],
      "id": "3c5bff71"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.017797Z",
          "iopub.status.busy": "2021-09-29T05:49:50.017205Z",
          "iopub.status.idle": "2021-09-29T05:49:50.020564Z",
          "shell.execute_reply": "2021-09-29T05:49:50.021004Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.466881Z"
        },
        "id": "72195d39",
        "papermill": {
          "duration": 0.080916,
          "end_time": "2021-09-29T05:49:50.021134",
          "exception": false,
          "start_time": "2021-09-29T05:49:49.940218",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def create_train():\n",
        "  train =pd.read_csv(\"S2TrainObs1.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs1.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "72195d39",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.164557Z",
          "iopub.status.busy": "2021-09-29T05:49:50.164048Z",
          "iopub.status.idle": "2021-09-29T05:49:50.167259Z",
          "shell.execute_reply": "2021-09-29T05:49:50.167639Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.487108Z"
        },
        "papermill": {
          "duration": 0.076427,
          "end_time": "2021-09-29T05:49:50.167780",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.091353",
          "status": "completed"
        },
        "tags": [],
        "id": "0de1ee06"
      },
      "source": [
        "def create_test():\n",
        "  test =pd.read_csv(\"S2TestObs1.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs1.csv\" )\n",
        "  return  pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "0de1ee06",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.310623Z",
          "iopub.status.busy": "2021-09-29T05:49:50.309868Z",
          "iopub.status.idle": "2021-09-29T05:49:50.311830Z",
          "shell.execute_reply": "2021-09-29T05:49:50.312272Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.504827Z"
        },
        "papermill": {
          "duration": 0.076141,
          "end_time": "2021-09-29T05:49:50.312391",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.236250",
          "status": "completed"
        },
        "tags": [],
        "id": "0769b007"
      },
      "source": [
        "def createObs2_train():\n",
        "  train =pd.read_csv(\"S2TrainObs2.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs2.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "0769b007",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.457237Z",
          "iopub.status.busy": "2021-09-29T05:49:50.456391Z",
          "iopub.status.idle": "2021-09-29T05:49:50.458520Z",
          "shell.execute_reply": "2021-09-29T05:49:50.458955Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.527947Z"
        },
        "papermill": {
          "duration": 0.079916,
          "end_time": "2021-09-29T05:49:50.459086",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.379170",
          "status": "completed"
        },
        "tags": [],
        "id": "785e1fe8"
      },
      "source": [
        "def createObs2_test():\n",
        "  test =pd.read_csv(\"S2TestObs2.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs2.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "785e1fe8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.611725Z",
          "iopub.status.busy": "2021-09-29T05:49:50.610582Z",
          "iopub.status.idle": "2021-09-29T05:49:50.612948Z",
          "shell.execute_reply": "2021-09-29T05:49:50.613479Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.538500Z"
        },
        "papermill": {
          "duration": 0.082047,
          "end_time": "2021-09-29T05:49:50.613629",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.531582",
          "status": "completed"
        },
        "tags": [],
        "id": "1fe4205f"
      },
      "source": [
        "def createObs3_train():\n",
        "  train =pd.read_csv(\"S2TrainObs3.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs3.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "1fe4205f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.759023Z",
          "iopub.status.busy": "2021-09-29T05:49:50.757441Z",
          "iopub.status.idle": "2021-09-29T05:49:50.759724Z",
          "shell.execute_reply": "2021-09-29T05:49:50.760140Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.560105Z"
        },
        "papermill": {
          "duration": 0.075295,
          "end_time": "2021-09-29T05:49:50.760261",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.684966",
          "status": "completed"
        },
        "tags": [],
        "id": "607489b4"
      },
      "source": [
        "def createObs3_test():\n",
        "  test =pd.read_csv(\"S2TestObs3.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs3.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "607489b4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:50.903576Z",
          "iopub.status.busy": "2021-09-29T05:49:50.902117Z",
          "iopub.status.idle": "2021-09-29T05:49:50.904242Z",
          "shell.execute_reply": "2021-09-29T05:49:50.904649Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.572479Z"
        },
        "papermill": {
          "duration": 0.077087,
          "end_time": "2021-09-29T05:49:50.904783",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.827696",
          "status": "completed"
        },
        "tags": [],
        "id": "b44c406a"
      },
      "source": [
        "def createObs4_train():\n",
        "  train =pd.read_csv(\"S2TrainObs4.csv\" )\n",
        "  trainS1 =pd.read_csv(\"S1TrainObs4.csv\" )\n",
        "\n",
        "  train = pd.merge(train,trainS1.drop('label',axis=1),on='field_id',how='left')\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "b44c406a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.046418Z",
          "iopub.status.busy": "2021-09-29T05:49:51.045740Z",
          "iopub.status.idle": "2021-09-29T05:49:51.047953Z",
          "shell.execute_reply": "2021-09-29T05:49:51.048324Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.593332Z"
        },
        "papermill": {
          "duration": 0.076241,
          "end_time": "2021-09-29T05:49:51.048446",
          "exception": false,
          "start_time": "2021-09-29T05:49:50.972205",
          "status": "completed"
        },
        "tags": [],
        "id": "f9661328"
      },
      "source": [
        "def createObs4_test():\n",
        "  test =pd.read_csv(\"S2TestObs4.csv\" )\n",
        "  testS1 =pd.read_csv(\"S1TestObs4.csv\" )\n",
        "  return pd.merge(test,testS1,on='field_id',how='left')"
      ],
      "id": "f9661328",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33868ef5",
        "papermill": {
          "duration": 0.066967,
          "end_time": "2021-09-29T05:49:51.184266",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.117299",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Feature Engineering"
      ],
      "id": "33868ef5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.359660Z",
          "iopub.status.busy": "2021-09-29T05:49:51.354106Z",
          "iopub.status.idle": "2021-09-29T05:49:51.363494Z",
          "shell.execute_reply": "2021-09-29T05:49:51.363988Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.616268Z"
        },
        "id": "288161bc",
        "papermill": {
          "duration": 0.111819,
          "end_time": "2021-09-29T05:49:51.364121",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.252302",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def process(T) :\n",
        "\n",
        "  # process bands\n",
        "  Bcols = T.filter(like='B').columns.tolist()\n",
        "  Vcols = T.filter(like='V').columns.tolist()\n",
        "  Obs1 = T.filter(like='Month4').columns.tolist()\n",
        "  Obs2 = T.filter(like='Month5').columns.tolist()\n",
        "  Obs3 = T.filter(like='Month6').columns.tolist()\n",
        "  Obs4 = T.filter(like='Month7').columns.tolist()\n",
        "  Obs5 = T.filter(like='Month8').columns.tolist()\n",
        "  Obs6 = T.filter(like='Month9').columns.tolist()\n",
        "  Obs7 = T.filter(like='Month10').columns.tolist()\n",
        "  Obs8 = T.filter(like='Month11').columns.tolist()\n",
        "\n",
        "\n",
        "  # vegetation indexes \n",
        "  B8cols = T.filter(like='B8_').columns.tolist()\n",
        "  B8cols = [x for x in B8cols if 'std' not in x]\n",
        "  \n",
        "  B4cols = T.filter(like='B4_').columns.tolist()\n",
        "  B4cols = [x for x in B4cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B5cols = T.filter(like='B5_').columns.tolist()\n",
        "  B5cols = [x for x in B5cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B2cols = T.filter(like='B2_').columns.tolist()\n",
        "  B2cols = [x for x in B2cols if 'std' not in x]\n",
        "\n",
        "  B7cols = T.filter(like='B7_').columns.tolist()\n",
        "  B7cols = [x for x in B7cols if 'std' not in x]\n",
        "\n",
        "  B8Acols = T.filter(like='B8A_').columns.tolist()\n",
        "  B8Acols = [x for x in B8Acols if 'std' not in x]\n",
        "\n",
        "  B6cols = T.filter(like='B6_').columns.tolist()\n",
        "  B6cols = [x for x in B6cols if 'std' not in x]\n",
        "  \n",
        "  B12cols = T.filter(like='B12_').columns.tolist()\n",
        "  B12cols = [x for x in B12cols if 'std' not in x]\n",
        "\n",
        "  B11cols = T.filter(like='B11_').columns.tolist()\n",
        "  B11cols = [x for x in B11cols if 'std' not in x]\n",
        "\n",
        "  B1cols = T.filter(like='B1_').columns.tolist()\n",
        "  B1cols = [x for x in B1cols if 'std' not in x]\n",
        "\n",
        "  B9cols = T.filter(like='B9_').columns.tolist()\n",
        "  B9cols = [x for x in B9cols if 'std' not in x]\n",
        "\n",
        "  L = 0.725\n",
        "  for b1,b2 ,b3 ,b4, b5 , b6, b7, b8 ,b8a ,b9,b11,b12 in zip(B1cols,B2cols,B3cols,B4cols,B5cols,B6cols,B7cols,B8cols,B8Acols,B9cols,B11cols,B12cols) :\n",
        "    T[f'NDVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4])).values.clip(-5,5)\n",
        "    T[f'SAVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4]+L) * (1.0 + L)).values.clip(-5,5)\n",
        "    T[f'GRNDVI_{b8.split(\"_\")[1]}'] = ((T[b8] - (T[b3]+T[b4])) /  (T[b8] + (T[b3]+T[b4]))).values.clip(-5,5)\n",
        "    T[f'GNDVI_{b8.split(\"_\")[1]}']  = ((T[b8] - T[b3] ) /  (T[b8] + T[b3])).values.clip(-5,5)\n",
        "    T[f'NDRE_{b8.split(\"_\")[1]}']   = ((T[b5] - T[b4])/ (T[b5] + T[b4])).values.clip(-5,5)\n",
        "    T[f'EVI_{b8.split(\"_\")[1]}']    = (2.5 * (T[b8]  - T[b4] ) / ((T[b8]  + 6.0 * T[b4]  - 7.5 * T[b2]) + 1.0)).values.clip(min=-5,max=5)\n",
        "    T[f'WDRVI_{b8.split(\"_\")[1]}']  = (((8 * T[b8]) - T[b4])/ ((8* T[b8]) + T[b4])).values.clip(-5,5)\n",
        "    T[f'ExBlue_{b8.split(\"_\")[1]}']  = ((2 * T[b2]) - (T[b3]+T[b4]))\n",
        "    T[f'ExGreen_{b8.split(\"_\")[1]}']  = ((2 * T[b3]) - (T[b2]+T[b4]) )\n",
        "    T[f'NDRE7_{b8.split(\"_\")[1]}']   = ((T[b7] - T[b4])/ (T[b7] + T[b4])).values.clip(-5,5)\n",
        "    T[f'MTCI_{b8.split(\"_\")[1]}']   = ((T[b8a] - T[b6])/ (T[b7] + T[b6])).values.clip(-5,5)\n",
        "    T[f'VARI_{b8.split(\"_\")[1]}']   = ((T[b3] - T[b4])/ (T[b3] + T[b4] - T[b2])).values.clip(-5,5)\n",
        "    T[f'b3b1_{b8.split(\"_\")[1]}']  = (T[b3] - T[b1])/ (T[b3] + T[b1])    # B7  / B3\n",
        "    T[f'b11b8_{b8.split(\"_\")[1]}']  = (T[b11] - T[b8])/ (T[b11] + T[b8])    # B7  / B3\n",
        "    T[f'b12b11_{b8.split(\"_\")[1]}']  = (T[b12] - T[b11])/ (T[b12] + T[b11])    # B7  / B3\n",
        "    T[f'b3b4_{b8.split(\"_\")[1]}']  = (T[b3] - T[b4])/ (T[b3] + T[b4])    # B7  / B3\n",
        "    T[f'b9b4_{b8.split(\"_\")[1]}']  = (T[b9] - T[b4])/ (T[b9] + T[b4])    # B7  / B3\n",
        "    T[f'b5b3_{b8.split(\"_\")[1]}']  = (T[b5] - T[b3])/ (T[b5] + T[b3])    # B7  / B3\n",
        "    T[f'b12b3_{b8.split(\"_\")[1]}']  = (T[b12] - T[b3])/ (T[b12] + T[b3])    # B7  / B3\n",
        "    \n",
        "    T[f'b2b1_{b8.split(\"_\")[1]}']  = (T[b2] - T[b1])/ (T[b2] + T[b1])    # B7  / B3\n",
        "    T[f'b4b1_{b8.split(\"_\")[1]}']  = (T[b4] - T[b1])/ (T[b4] + T[b1])    # B7  / B3\n",
        "    T[f'b11b3_{b8.split(\"_\")[1]}']  = (T[b11] - T[b3])/ (T[b11] + T[b3]) \n",
        "    \n",
        "  for col in Bcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "  \n",
        "  for col in Vcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "    \n",
        "  for col1,col2,col3,col4,col5,col6,col7,col8 in zip(Obs1,Obs2,Obs3,Obs4,Obs5,Obs6,Obs7,Obs8) :\n",
        "    T[f'{col1.split(\"_\")[0]}_std'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].std(axis=1)\n",
        "    T[f'{col1.split(\"_\")[0]}_mean'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].mean(axis=1)\n",
        "\n",
        "    \n",
        "  # NDVI \n",
        "  T['water']       = T['NDVI_Month4'].apply(lambda x :1 if x<0 else 0)\n",
        "  T['dense_green'] = T['NDVI_Month4'].apply(lambda x :1 if x>=0.5 else 0)\n",
        "  T['not_green']   = T['NDVI_Month4'].apply(lambda x :1 if( (x>0) & (x<0.5)) else 0)\n",
        "\n",
        "  T['high_green'] = T['SAVI_Month4'].apply(lambda x :1 if x<0.1 else 0)\n",
        "  T['low_green']  = T['SAVI_Month4'].apply(lambda x :1 if x>=0.8 else 0)\n",
        "  T['chlorophyll_EVI'] = T['EVI_Month4'].apply(lambda x :1 if( (x>0.2) & (x<0.8)) else 0)\n",
        "\n",
        "  # process Vegetation indexes\n",
        "\n",
        "  ObsN   = T.filter(like='NDVI_').columns.tolist()\n",
        "  ObsSA  = T.filter(like='SAVI_').columns.tolist()\n",
        "  ObsCC  = T.filter(like='CCCI_').columns.tolist()\n",
        "  ObsWDR = T.filter(like='WDRVI_').columns.tolist()\n",
        "  ObsNDRE7 = T.filter(like='NDRE7_').columns.tolist()\n",
        "\n",
        "  return T"
      ],
      "id": "288161bc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:51.504510Z",
          "iopub.status.busy": "2021-09-29T05:49:51.503666Z",
          "iopub.status.idle": "2021-09-29T05:49:54.401618Z",
          "shell.execute_reply": "2021-09-29T05:49:54.400980Z",
          "shell.execute_reply.started": "2021-09-29T05:37:24.707822Z"
        },
        "id": "1348f1c3",
        "papermill": {
          "duration": 2.970586,
          "end_time": "2021-09-29T05:49:54.401773",
          "exception": false,
          "start_time": "2021-09-29T05:49:51.431187",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train = create_train()\n",
        "Test = create_test()"
      ],
      "id": "1348f1c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:54.557186Z",
          "iopub.status.busy": "2021-09-29T05:49:54.556405Z",
          "iopub.status.idle": "2021-09-29T05:49:54.559626Z",
          "shell.execute_reply": "2021-09-29T05:49:54.560129Z",
          "shell.execute_reply.started": "2021-09-29T05:37:27.533875Z"
        },
        "id": "dd617c55",
        "papermill": {
          "duration": 0.079682,
          "end_time": "2021-09-29T05:49:54.560279",
          "exception": false,
          "start_time": "2021-09-29T05:49:54.480597",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c58ca82a-d949-4f73-c29a-b3f225fe5974"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "dd617c55",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:54.704352Z",
          "iopub.status.busy": "2021-09-29T05:49:54.703594Z",
          "iopub.status.idle": "2021-09-29T05:49:57.327280Z",
          "shell.execute_reply": "2021-09-29T05:49:57.326719Z",
          "shell.execute_reply.started": "2021-09-29T05:37:27.544882Z"
        },
        "papermill": {
          "duration": 2.698541,
          "end_time": "2021-09-29T05:49:57.327415",
          "exception": false,
          "start_time": "2021-09-29T05:49:54.628874",
          "status": "completed"
        },
        "tags": [],
        "id": "71925909"
      },
      "source": [
        "Train2 = createObs2_train()\n",
        "Test2 = createObs2_test()"
      ],
      "id": "71925909",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:57.474367Z",
          "iopub.status.busy": "2021-09-29T05:49:57.473601Z",
          "iopub.status.idle": "2021-09-29T05:49:57.476987Z",
          "shell.execute_reply": "2021-09-29T05:49:57.476569Z",
          "shell.execute_reply.started": "2021-09-29T05:37:30.207715Z"
        },
        "papermill": {
          "duration": 0.075297,
          "end_time": "2021-09-29T05:49:57.477105",
          "exception": false,
          "start_time": "2021-09-29T05:49:57.401808",
          "status": "completed"
        },
        "tags": [],
        "id": "3fa7820f",
        "outputId": "2d28ecb7-e507-4d4e-b9c5-1ba0ca404a2e"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "3fa7820f",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:49:57.618966Z",
          "iopub.status.busy": "2021-09-29T05:49:57.618204Z",
          "iopub.status.idle": "2021-09-29T05:50:00.161553Z",
          "shell.execute_reply": "2021-09-29T05:50:00.161042Z",
          "shell.execute_reply.started": "2021-09-29T05:37:30.215520Z"
        },
        "papermill": {
          "duration": 2.616429,
          "end_time": "2021-09-29T05:50:00.161682",
          "exception": false,
          "start_time": "2021-09-29T05:49:57.545253",
          "status": "completed"
        },
        "tags": [],
        "id": "b151a1db"
      },
      "source": [
        "Train3 = createObs3_train()\n",
        "Test3 = createObs3_test()"
      ],
      "id": "b151a1db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:00.308970Z",
          "iopub.status.busy": "2021-09-29T05:50:00.308289Z",
          "iopub.status.idle": "2021-09-29T05:50:00.311063Z",
          "shell.execute_reply": "2021-09-29T05:50:00.311472Z",
          "shell.execute_reply.started": "2021-09-29T05:37:32.702983Z"
        },
        "papermill": {
          "duration": 0.078987,
          "end_time": "2021-09-29T05:50:00.311595",
          "exception": false,
          "start_time": "2021-09-29T05:50:00.232608",
          "status": "completed"
        },
        "tags": [],
        "id": "ec556779",
        "outputId": "808c7b8d-d530-4ea1-9036-e28cfa95c88b"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "ec556779",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:00.452549Z",
          "iopub.status.busy": "2021-09-29T05:50:00.451637Z",
          "iopub.status.idle": "2021-09-29T05:50:02.848544Z",
          "shell.execute_reply": "2021-09-29T05:50:02.847431Z",
          "shell.execute_reply.started": "2021-09-29T05:37:32.710877Z"
        },
        "papermill": {
          "duration": 2.469373,
          "end_time": "2021-09-29T05:50:02.848675",
          "exception": false,
          "start_time": "2021-09-29T05:50:00.379302",
          "status": "completed"
        },
        "tags": [],
        "id": "72ff440b"
      },
      "source": [
        "Train4 = createObs4_train()\n",
        "Test4 = createObs4_test()"
      ],
      "id": "72ff440b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:02.991863Z",
          "iopub.status.busy": "2021-09-29T05:50:02.991157Z",
          "iopub.status.idle": "2021-09-29T05:50:02.994005Z",
          "shell.execute_reply": "2021-09-29T05:50:02.994443Z",
          "shell.execute_reply.started": "2021-09-29T05:37:35.104383Z"
        },
        "papermill": {
          "duration": 0.075587,
          "end_time": "2021-09-29T05:50:02.994589",
          "exception": false,
          "start_time": "2021-09-29T05:50:02.919002",
          "status": "completed"
        },
        "tags": [],
        "id": "d65352e2",
        "outputId": "69e427f0-c686-41ef-fb00-45c21a11d848"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "d65352e2",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 114), (35295, 113))"
            ]
          },
          "execution_count": 28,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:03.195813Z",
          "iopub.status.busy": "2021-09-29T05:50:03.195080Z",
          "iopub.status.idle": "2021-09-29T05:50:10.651486Z",
          "shell.execute_reply": "2021-09-29T05:50:10.650892Z",
          "shell.execute_reply.started": "2021-09-29T05:37:35.112575Z"
        },
        "papermill": {
          "duration": 7.587759,
          "end_time": "2021-09-29T05:50:10.651613",
          "exception": false,
          "start_time": "2021-09-29T05:50:03.063854",
          "status": "completed"
        },
        "tags": [],
        "id": "4a49e434"
      },
      "source": [
        "Train = process(Train)\n",
        "Test = process(Test)"
      ],
      "id": "4a49e434",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:10.796459Z",
          "iopub.status.busy": "2021-09-29T05:50:10.795352Z",
          "iopub.status.idle": "2021-09-29T05:50:10.798653Z",
          "shell.execute_reply": "2021-09-29T05:50:10.799103Z",
          "shell.execute_reply.started": "2021-09-29T05:37:42.588079Z"
        },
        "papermill": {
          "duration": 0.078599,
          "end_time": "2021-09-29T05:50:10.799229",
          "exception": false,
          "start_time": "2021-09-29T05:50:10.720630",
          "status": "completed"
        },
        "tags": [],
        "id": "006d62d7",
        "outputId": "fbcf256e-2d34-4690-be79-c89819f2756f"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "006d62d7",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:10.999996Z",
          "iopub.status.busy": "2021-09-29T05:50:10.999363Z",
          "iopub.status.idle": "2021-09-29T05:50:18.712972Z",
          "shell.execute_reply": "2021-09-29T05:50:18.713398Z",
          "shell.execute_reply.started": "2021-09-29T05:37:42.595672Z"
        },
        "id": "3bad0300",
        "papermill": {
          "duration": 7.844996,
          "end_time": "2021-09-29T05:50:18.713561",
          "exception": false,
          "start_time": "2021-09-29T05:50:10.868565",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train2 = process(Train2)\n",
        "Test2 = process(Test2)"
      ],
      "id": "3bad0300",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:18.856992Z",
          "iopub.status.busy": "2021-09-29T05:50:18.856316Z",
          "iopub.status.idle": "2021-09-29T05:50:18.859165Z",
          "shell.execute_reply": "2021-09-29T05:50:18.859622Z",
          "shell.execute_reply.started": "2021-09-29T05:37:50.069577Z"
        },
        "papermill": {
          "duration": 0.077373,
          "end_time": "2021-09-29T05:50:18.859778",
          "exception": false,
          "start_time": "2021-09-29T05:50:18.782405",
          "status": "completed"
        },
        "tags": [],
        "id": "1b5bcb4b",
        "outputId": "6cf0ddf5-dccf-4190-c0d0-9bcbf4e7c709"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "1b5bcb4b",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:19.001800Z",
          "iopub.status.busy": "2021-09-29T05:50:19.001023Z",
          "iopub.status.idle": "2021-09-29T05:50:26.723386Z",
          "shell.execute_reply": "2021-09-29T05:50:26.723973Z",
          "shell.execute_reply.started": "2021-09-29T05:37:50.084768Z"
        },
        "papermill": {
          "duration": 7.793783,
          "end_time": "2021-09-29T05:50:26.724198",
          "exception": false,
          "start_time": "2021-09-29T05:50:18.930415",
          "status": "completed"
        },
        "tags": [],
        "id": "acfa2dc5"
      },
      "source": [
        "Train3 = process(Train3)\n",
        "Test3 = process(Test3)"
      ],
      "id": "acfa2dc5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:26.973391Z",
          "iopub.status.busy": "2021-09-29T05:50:26.972458Z",
          "iopub.status.idle": "2021-09-29T05:50:26.977791Z",
          "shell.execute_reply": "2021-09-29T05:50:26.978417Z",
          "shell.execute_reply.started": "2021-09-29T05:37:57.626501Z"
        },
        "papermill": {
          "duration": 0.127686,
          "end_time": "2021-09-29T05:50:26.978620",
          "exception": false,
          "start_time": "2021-09-29T05:50:26.850934",
          "status": "completed"
        },
        "tags": [],
        "id": "040eda71",
        "outputId": "1d254cfb-0ff8-41d6-e9e0-5f14f3a68520"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "040eda71",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:27.210475Z",
          "iopub.status.busy": "2021-09-29T05:50:27.209388Z",
          "iopub.status.idle": "2021-09-29T05:50:34.677062Z",
          "shell.execute_reply": "2021-09-29T05:50:34.676585Z",
          "shell.execute_reply.started": "2021-09-29T05:37:57.634680Z"
        },
        "papermill": {
          "duration": 7.604812,
          "end_time": "2021-09-29T05:50:34.677201",
          "exception": false,
          "start_time": "2021-09-29T05:50:27.072389",
          "status": "completed"
        },
        "tags": [],
        "id": "f9d8cde4"
      },
      "source": [
        "Train4 = process(Train4)\n",
        "Test4 = process(Test4)"
      ],
      "id": "f9d8cde4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:34.823730Z",
          "iopub.status.busy": "2021-09-29T05:50:34.823099Z",
          "iopub.status.idle": "2021-09-29T05:50:34.825744Z",
          "shell.execute_reply": "2021-09-29T05:50:34.826172Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.127535Z"
        },
        "papermill": {
          "duration": 0.077485,
          "end_time": "2021-09-29T05:50:34.826298",
          "exception": false,
          "start_time": "2021-09-29T05:50:34.748813",
          "status": "completed"
        },
        "tags": [],
        "id": "14b2df9d",
        "outputId": "c01b8d73-585b-4816-a8c1-bd7203a355c2"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "14b2df9d",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 324), (35295, 323))"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:35.173178Z",
          "iopub.status.busy": "2021-09-29T05:50:35.172142Z",
          "iopub.status.idle": "2021-09-29T05:50:35.490909Z",
          "shell.execute_reply": "2021-09-29T05:50:35.490428Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.135660Z"
        },
        "papermill": {
          "duration": 0.594109,
          "end_time": "2021-09-29T05:50:35.491030",
          "exception": false,
          "start_time": "2021-09-29T05:50:34.896921",
          "status": "completed"
        },
        "tags": [],
        "id": "79e018c1",
        "outputId": "6c3f34d3-4924-4372-ef2f-b3ee2a4b7e9f"
      },
      "source": [
        "Train = pd.concat([Train,Train2.drop(columns=['field_id','label']),Train3.drop(columns=['field_id','label']),Train4.drop(columns=['field_id','label'])],axis=1)\n",
        "Train.shape"
      ],
      "id": "79e018c1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(87114, 1290)"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T05:50:35.694139Z",
          "iopub.status.busy": "2021-09-29T05:50:35.693499Z",
          "iopub.status.idle": "2021-09-29T05:50:36.512342Z",
          "shell.execute_reply": "2021-09-29T05:50:36.511917Z",
          "shell.execute_reply.started": "2021-09-29T05:38:05.666689Z"
        },
        "id": "39df5209",
        "papermill": {
          "duration": 0.950552,
          "end_time": "2021-09-29T05:50:36.512462",
          "exception": false,
          "start_time": "2021-09-29T05:50:35.561910",
          "status": "completed"
        },
        "tags": [],
        "outputId": "14bef70b-b3d5-4ffb-d20e-983e85885217"
      },
      "source": [
        "Test = pd.concat([Test,Test2.drop(columns=['field_id']),Test3.drop(columns=['field_id'])],axis=1)\n",
        "Test = pd.merge(Test,Test4,on='field_id',how='left')\n",
        "Test.shape"
      ],
      "id": "39df5209",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35295, 1289)"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0733a397",
        "papermill": {
          "duration": 0.066838,
          "end_time": "2021-09-29T06:51:14.965933",
          "exception": false,
          "start_time": "2021-09-29T06:51:14.899095",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODELING"
      ],
      "id": "0733a397"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:51:15.323197Z",
          "iopub.status.busy": "2021-09-29T06:51:15.322460Z",
          "iopub.status.idle": "2021-09-29T06:51:16.657667Z",
          "shell.execute_reply": "2021-09-29T06:51:16.657060Z",
          "shell.execute_reply.started": "2021-09-29T06:33:17.549849Z"
        },
        "papermill": {
          "duration": 1.443828,
          "end_time": "2021-09-29T06:51:16.657812",
          "exception": false,
          "start_time": "2021-09-29T06:51:15.213984",
          "status": "completed"
        },
        "tags": [],
        "id": "2cf2651d"
      },
      "source": [
        "X    = Train.fillna(-999).replace(np.inf,9999).drop(['field_id','label'], axis=1)\n",
        "y    = Train.label\n",
        "TEST = Test.fillna(-999).replace(np.inf,9999).drop(['field_id'], axis=1)"
      ],
      "id": "2cf2651d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:51:16.872950Z",
          "iopub.status.busy": "2021-09-29T06:51:16.872427Z",
          "iopub.status.idle": "2021-09-29T06:51:16.874286Z",
          "shell.execute_reply": "2021-09-29T06:51:16.873846Z",
          "shell.execute_reply.started": "2021-09-29T06:33:54.291940Z"
        },
        "papermill": {
          "duration": 0.111147,
          "end_time": "2021-09-29T06:51:16.874392",
          "exception": false,
          "start_time": "2021-09-29T06:51:16.763245",
          "status": "completed"
        },
        "tags": [],
        "id": "c461c98f"
      },
      "source": [
        "TEST.columns = X.columns.tolist()"
      ],
      "id": "c461c98f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:51:17.014090Z",
          "iopub.status.busy": "2021-09-29T06:51:17.012354Z",
          "iopub.status.idle": "2021-09-29T06:52:06.490284Z",
          "shell.execute_reply": "2021-09-29T06:52:06.489725Z",
          "shell.execute_reply.started": "2021-09-29T06:33:54.972949Z"
        },
        "papermill": {
          "duration": 49.549514,
          "end_time": "2021-09-29T06:52:06.490432",
          "exception": false,
          "start_time": "2021-09-29T06:51:16.940918",
          "status": "completed"
        },
        "tags": [],
        "id": "ff821449"
      },
      "source": [
        "data = pd.concat([X,TEST])\n",
        "qt=QuantileTransformer(output_distribution=\"normal\",random_state=42)\n",
        "data= pd.DataFrame(qt.fit_transform(data),columns=X.columns)"
      ],
      "id": "ff821449",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:06.704795Z",
          "iopub.status.busy": "2021-09-29T06:52:06.704173Z",
          "iopub.status.idle": "2021-09-29T06:52:06.705383Z",
          "shell.execute_reply": "2021-09-29T06:52:06.705974Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.228274Z"
        },
        "papermill": {
          "duration": 0.111454,
          "end_time": "2021-09-29T06:52:06.706130",
          "exception": false,
          "start_time": "2021-09-29T06:52:06.594676",
          "status": "completed"
        },
        "tags": [],
        "id": "7487a635"
      },
      "source": [
        "X = data[:X.shape[0]].values\n",
        "TEST = data[X.shape[0]:].values"
      ],
      "id": "7487a635",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:06.917867Z",
          "iopub.status.busy": "2021-09-29T06:52:06.917346Z",
          "iopub.status.idle": "2021-09-29T06:52:06.921898Z",
          "shell.execute_reply": "2021-09-29T06:52:06.922368Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.235081Z"
        },
        "papermill": {
          "duration": 0.112589,
          "end_time": "2021-09-29T06:52:06.922522",
          "exception": false,
          "start_time": "2021-09-29T06:52:06.809933",
          "status": "completed"
        },
        "tags": [],
        "id": "b5714a1f",
        "outputId": "0d5b1738-2cb8-4873-c54a-9d5f7d852d5d"
      },
      "source": [
        "X.shape  , TEST.shape"
      ],
      "id": "b5714a1f",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 1288), (35295, 1288))"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:07.536704Z",
          "iopub.status.busy": "2021-09-29T06:52:07.536207Z",
          "iopub.status.idle": "2021-09-29T06:52:07.549240Z",
          "shell.execute_reply": "2021-09-29T06:52:07.549678Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.280301Z"
        },
        "papermill": {
          "duration": 0.084248,
          "end_time": "2021-09-29T06:52:07.549828",
          "exception": false,
          "start_time": "2021-09-29T06:52:07.465580",
          "status": "completed"
        },
        "tags": [],
        "id": "0fe50a1f"
      },
      "source": [
        "def CUSTOM_ANN_CNN(shape1) :\n",
        "  # define two sets of inputs\n",
        "  input_1 = tf.keras.layers.Input(shape = (shape1),name='20_Feats')\n",
        "  input_2 = tf.keras.layers.Input(shape = (shape1, 1),name='20_Feats_Conv')\n",
        "  \n",
        "  # create Convolution Layer \n",
        "  head_conv = Convolution1D(filters=128, kernel_size=1, input_shape=(shape1, 1),name='Conv_Layer')(input_2)\n",
        "  head_conv = Activation('relu',name='Conv_Activation')(head_conv)\n",
        "  head_conv = Flatten()(head_conv)\n",
        "  \n",
        "  # create a simple 2 layers for input_1\n",
        "  # Layer 1 -- input_1\n",
        "  head_1 = tf.keras.layers.BatchNormalization()(input_1)\n",
        "  head_1 = tf.keras.layers.Dense(512, activation = \"relu\")(head_1)\n",
        "  head_1 = tf.keras.layers.Dropout(0.15)(head_1)\n",
        "  # Layer 2 -- input_1\n",
        "  head_1 = tf.keras.layers.BatchNormalization()(head_1)\n",
        "  head_1 = tf.keras.layers.Dense(256, activation = \"relu\")(head_1)\n",
        "  head_1 = tf.keras.layers.Dropout(0.15)(head_1)\n",
        "    \n",
        "  head_1 = tf.keras.layers.BatchNormalization()(head_1)  \n",
        "  head_1 = tf.keras.layers.add([head_1, Dense(256, activation='relu')(head_1)])\n",
        "  head_1 = tf.keras.layers.Dropout(0.15)(head_1)\n",
        "\n",
        "  # Layer 3 -- input_1\n",
        "  head_1 = tf.keras.layers.BatchNormalization()(head_1)\n",
        "  head_1 = tf.keras.layers.Dense(128, activation = \"relu\")(head_1)\n",
        "  out_1 = tf.keras.layers.Dropout(0.2)(head_1)\n",
        "\n",
        "\n",
        "  output = tf.keras.layers.Dense(9,activation = \"softmax\",name='FC_Output')(out_1)\n",
        "  \n",
        "  model = tf.keras.models.Model(inputs = [input_1,input_2], outputs = output)\n",
        "  model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "  \n",
        "  return model "
      ],
      "id": "0fe50a1f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:07.712241Z",
          "iopub.status.busy": "2021-09-29T06:52:07.711592Z",
          "iopub.status.idle": "2021-09-29T06:52:07.716456Z",
          "shell.execute_reply": "2021-09-29T06:52:07.715998Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.295819Z"
        },
        "papermill": {
          "duration": 0.089076,
          "end_time": "2021-09-29T06:52:07.716598",
          "exception": false,
          "start_time": "2021-09-29T06:52:07.627522",
          "status": "completed"
        },
        "tags": [],
        "id": "51db2843"
      },
      "source": [
        "# Function to seed everything\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "id": "51db2843",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:07.860750Z",
          "iopub.status.busy": "2021-09-29T06:52:07.860195Z",
          "iopub.status.idle": "2021-09-29T06:52:08.082556Z",
          "shell.execute_reply": "2021-09-29T06:52:08.082945Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.312141Z"
        },
        "papermill": {
          "duration": 0.296853,
          "end_time": "2021-09-29T06:52:08.083084",
          "exception": false,
          "start_time": "2021-09-29T06:52:07.786231",
          "status": "completed"
        },
        "tags": [],
        "id": "4294e4fc"
      },
      "source": [
        "# reshape test data For Conv Layers Input\n",
        "X_test_r = np.zeros((len(TEST), X.shape[1], 1))\n",
        "X_test_r[:, :, 0] = TEST[:, :]\n",
        "\n",
        "y_train = y.copy()\n",
        "n_labels = y.unique().shape[0]\n",
        "\n",
        "# we need to binarize the labels for the neural net\n",
        "LE = LabelEncoder()\n",
        "ytrain_enc = pd.get_dummies(y_train).values\n",
        "TARGETS = pd.get_dummies(y_train).columns\n",
        "\n",
        "y_oof = np.zeros([X.shape[0], n_labels])\n",
        "y_test = np.zeros([TEST.shape[0], n_labels])\n",
        "\n",
        "i = 0\n",
        "metrics = list()\n",
        "apply_aug = False"
      ],
      "id": "4294e4fc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T06:52:08.230757Z",
          "iopub.status.busy": "2021-09-29T06:52:08.227689Z",
          "iopub.status.idle": "2021-09-29T09:13:04.553893Z",
          "shell.execute_reply": "2021-09-29T09:13:04.554418Z",
          "shell.execute_reply.started": "2021-09-29T06:34:55.756610Z"
        },
        "papermill": {
          "duration": 8456.402915,
          "end_time": "2021-09-29T09:13:04.554618",
          "exception": false,
          "start_time": "2021-09-29T06:52:08.151703",
          "status": "completed"
        },
        "tags": [],
        "id": "808349bb",
        "outputId": "15bda4f1-cc18-4d8e-dd78-cc7906563762"
      },
      "source": [
        "n_splits = 10\n",
        "kf = StratifiedKFold(n_splits=n_splits, random_state=47, shuffle=True)\n",
        "\n",
        "for idx , (tr_idx, val_idx) in enumerate(kf.split(X, y_train)):\n",
        "    # Verbosity\n",
        "    VERBOSE = 0\n",
        "\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
        "                                                      mode = 'min',\n",
        "                                                      patience = 25,\n",
        "                                                      restore_best_weights = True,\n",
        "                                                      verbose = 1)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Radiant_NN_{idx}.h5',\n",
        "                                                    monitor = 'val_loss',\n",
        "                                                    verbose = 1,\n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True)\n",
        "\n",
        "    X_tr, X_vl = X[tr_idx, :], X[val_idx, :]\n",
        "    y_tr, y_vl = ytrain_enc[tr_idx], ytrain_enc[val_idx]\n",
        "    y_train_, y_vld_ = y_train[tr_idx], y_train[val_idx]\n",
        "\n",
        "    # reshape train data For Conv Layers Input\n",
        "    X_train_r = np.zeros((len(X_tr), X.shape[1], 1))\n",
        "    X_train_r[:, :, 0] = X_tr[:, :]\n",
        "\n",
        "    # reshape validation data For Conv Layers Input\n",
        "    X_valid_r = np.zeros((len(X_vl), X.shape[1], 1))\n",
        "    X_valid_r[:, :, 0] = X_vl[:, :]\n",
        "    \n",
        "    # 1 CNN ANN\n",
        "    seed_everything(seed=1)\n",
        "    model_cnn_nn = CUSTOM_ANN_CNN(shape1=X.shape[1])\n",
        "    model_cnn_nn.fit([X_tr,X_train_r], y=y_tr, batch_size=64, epochs=200, verbose=1,\n",
        "                     validation_data=([X_vl,X_valid_r], y_vl),\n",
        "                     callbacks = [early_stopping,  checkpoint])\n",
        "    \n",
        "    model_cnn_nn = CUSTOM_ANN_CNN(shape1=X.shape[1])\n",
        "    model_cnn_nn.load_weights(f'Radiant_NN_{idx}.h5')\n",
        "    y_pred_cnn_nn = model_cnn_nn.predict([X_vl,X_valid_r])\n",
        "    test_pred_cnn_nn = model_cnn_nn.predict([TEST,X_test_r])\n",
        "    \n",
        "    \n",
        "    y_oof[val_idx, :] = y_pred_cnn_nn\n",
        "    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "    metric = log_loss(y_vl, y_pred_cnn_nn)\n",
        "    print(\"fold #{} Log Loss: {}\".format(i, metric))\n",
        "    \n",
        "    i += 1\n",
        "    test_pred = test_pred_cnn_nn \n",
        "    y_test += test_pred / n_splits\n",
        "    metrics.append(metric)\n",
        "    \n",
        "metrics = np.array(metrics).mean()\n",
        "print(f'Full Log loss {metrics}') "
      ],
      "id": "808349bb",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-29 06:52:08.906407: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-29 06:52:08.910296: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
            "2021-09-29 06:52:08.910336: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
            "2021-09-29 06:52:08.910357: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (36fc38758c18): /proc/driver/nvidia/version does not exist\n",
            "2021-09-29 06:52:08.910726: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-09-29 06:52:08.911079: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-29 06:52:09.618962: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-09-29 06:52:09.631858: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2250000000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 12ms/step - loss: 1.2315 - val_loss: 0.9259\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92587, saving model to Radiant_NN_0.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9437 - val_loss: 0.8609\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92587 to 0.86091, saving model to Radiant_NN_0.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8809 - val_loss: 0.8258\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86091 to 0.82576, saving model to Radiant_NN_0.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8470 - val_loss: 0.8156\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82576 to 0.81556, saving model to Radiant_NN_0.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8177 - val_loss: 0.7991\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81556 to 0.79915, saving model to Radiant_NN_0.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7884 - val_loss: 0.7740\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79915 to 0.77402, saving model to Radiant_NN_0.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7674 - val_loss: 0.7677\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77402 to 0.76770, saving model to Radiant_NN_0.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7383 - val_loss: 0.7496\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76770 to 0.74955, saving model to Radiant_NN_0.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7182 - val_loss: 0.7668\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.74955\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7033 - val_loss: 0.7305\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74955 to 0.73050, saving model to Radiant_NN_0.h5\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6875 - val_loss: 0.7396\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.73050\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6674 - val_loss: 0.7195\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73050 to 0.71948, saving model to Radiant_NN_0.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6637 - val_loss: 0.7147\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71948 to 0.71465, saving model to Radiant_NN_0.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6357 - val_loss: 0.7179\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.71465\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6315 - val_loss: 0.7079\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71465 to 0.70786, saving model to Radiant_NN_0.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6253 - val_loss: 0.7007\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70786 to 0.70073, saving model to Radiant_NN_0.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6094 - val_loss: 0.7030\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70073\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5900 - val_loss: 0.7092\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70073\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5828 - val_loss: 0.7063\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70073\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5673 - val_loss: 0.6966\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.70073 to 0.69663, saving model to Radiant_NN_0.h5\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5615 - val_loss: 0.6884\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69663 to 0.68838, saving model to Radiant_NN_0.h5\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5448 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.68838 to 0.68304, saving model to Radiant_NN_0.h5\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5327 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68304\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5372 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68304\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5194 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68304\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5115 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68304\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5103 - val_loss: 0.7105\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68304\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5010 - val_loss: 0.6841\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68304\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4920 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68304\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4859 - val_loss: 0.6913\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68304\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4730 - val_loss: 0.6818\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.68304 to 0.68184, saving model to Radiant_NN_0.h5\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4728 - val_loss: 0.6814\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.68184 to 0.68142, saving model to Radiant_NN_0.h5\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4617 - val_loss: 0.7021\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68142\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4579 - val_loss: 0.6899\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68142\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4576 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68142\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4385 - val_loss: 0.6844\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68142\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4400 - val_loss: 0.7213\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68142\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4391 - val_loss: 0.7042\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68142\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4309 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68142\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4269 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68142\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4199 - val_loss: 0.7044\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68142\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4139 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68142\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4104 - val_loss: 0.7082\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68142\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4058 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68142\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4057 - val_loss: 0.7096\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68142\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3914 - val_loss: 0.7073\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68142\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3946 - val_loss: 0.7161\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68142\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3918 - val_loss: 0.7013\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68142\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3871 - val_loss: 0.7023\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68142\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3858 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68142\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3776 - val_loss: 0.7056\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68142\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3794 - val_loss: 0.7356\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68142\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3651 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68142\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3623 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68142\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3620 - val_loss: 0.7016\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68142\n",
            "Epoch 56/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3639 - val_loss: 0.7179\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68142\n",
            "Epoch 57/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3529 - val_loss: 0.7163\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68142\n",
            "Epoch 00057: early stopping\n",
            "fold #0 Log Loss: 0.6814187947242473\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 13ms/step - loss: 1.2328 - val_loss: 0.8991\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89910, saving model to Radiant_NN_1.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9416 - val_loss: 0.8400\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89910 to 0.84001, saving model to Radiant_NN_1.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8870 - val_loss: 0.8244\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84001 to 0.82444, saving model to Radiant_NN_1.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8523 - val_loss: 0.8017\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82444 to 0.80167, saving model to Radiant_NN_1.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8181 - val_loss: 0.7807\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80167 to 0.78071, saving model to Radiant_NN_1.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7954 - val_loss: 0.7625\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78071 to 0.76247, saving model to Radiant_NN_1.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7741 - val_loss: 0.7448\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76247 to 0.74476, saving model to Radiant_NN_1.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7542 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74476 to 0.72334, saving model to Radiant_NN_1.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7240 - val_loss: 0.7131\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.72334 to 0.71308, saving model to Radiant_NN_1.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7059 - val_loss: 0.7148\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.71308\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6933 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.71308\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6823 - val_loss: 0.6909\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.71308 to 0.69086, saving model to Radiant_NN_1.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6640 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.69086\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6460 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.69086\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6327 - val_loss: 0.6703\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.69086 to 0.67031, saving model to Radiant_NN_1.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6310 - val_loss: 0.6772\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.67031\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6078 - val_loss: 0.6814\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67031\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5885 - val_loss: 0.6708\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67031\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5878 - val_loss: 0.6654\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.67031 to 0.66536, saving model to Radiant_NN_1.h5\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5808 - val_loss: 0.6718\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.66536\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5671 - val_loss: 0.6618\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.66536 to 0.66179, saving model to Radiant_NN_1.h5\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5567 - val_loss: 0.6587\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.66179 to 0.65874, saving model to Radiant_NN_1.h5\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5455 - val_loss: 0.6618\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.65874\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5333 - val_loss: 0.6483\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.65874 to 0.64834, saving model to Radiant_NN_1.h5\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5368 - val_loss: 0.6542\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.64834\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5166 - val_loss: 0.6618\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.64834\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5194 - val_loss: 0.6599\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.64834\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5016 - val_loss: 0.6589\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.64834\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4983 - val_loss: 0.6626\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.64834\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4878 - val_loss: 0.6622\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.64834\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4889 - val_loss: 0.6556\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.64834\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4718 - val_loss: 0.6612\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.64834\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4713 - val_loss: 0.6517\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.64834\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4619 - val_loss: 0.6625\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.64834\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4658 - val_loss: 0.6513\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.64834\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4449 - val_loss: 0.6767\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.64834\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4525 - val_loss: 0.6616\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.64834\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4434 - val_loss: 0.6611\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.64834\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4371 - val_loss: 0.6525\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.64834\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4375 - val_loss: 0.6612\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.64834\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4237 - val_loss: 0.6583\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64834\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4185 - val_loss: 0.6680\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64834\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4114 - val_loss: 0.6720\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64834\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4120 - val_loss: 0.6660\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64834\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4035 - val_loss: 0.6626\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.64834\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4073 - val_loss: 0.6576\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64834\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3989 - val_loss: 0.6802\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.64834\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3909 - val_loss: 0.6714\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.64834\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3864 - val_loss: 0.6686\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.64834\n",
            "Epoch 00049: early stopping\n",
            "fold #1 Log Loss: 0.6483397701616287\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 1.2309 - val_loss: 0.8980\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89804, saving model to Radiant_NN_2.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.9435 - val_loss: 0.8475\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89804 to 0.84754, saving model to Radiant_NN_2.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8859 - val_loss: 0.8130\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.84754 to 0.81295, saving model to Radiant_NN_2.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8506 - val_loss: 0.7924\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.81295 to 0.79239, saving model to Radiant_NN_2.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8278 - val_loss: 0.7693\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79239 to 0.76930, saving model to Radiant_NN_2.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7996 - val_loss: 0.7549\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.76930 to 0.75491, saving model to Radiant_NN_2.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7788 - val_loss: 0.7381\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.75491 to 0.73812, saving model to Radiant_NN_2.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7445 - val_loss: 0.7330\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.73812 to 0.73299, saving model to Radiant_NN_2.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7199 - val_loss: 0.7184\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.73299 to 0.71841, saving model to Radiant_NN_2.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7131 - val_loss: 0.7113\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.71841 to 0.71132, saving model to Radiant_NN_2.h5\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7005 - val_loss: 0.7001\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.71132 to 0.70006, saving model to Radiant_NN_2.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6706 - val_loss: 0.7104\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.70006\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6600 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.70006 to 0.69150, saving model to Radiant_NN_2.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6512 - val_loss: 0.6885\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69150 to 0.68853, saving model to Radiant_NN_2.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6299 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.68853 to 0.68506, saving model to Radiant_NN_2.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6198 - val_loss: 0.6731\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68506 to 0.67306, saving model to Radiant_NN_2.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6043 - val_loss: 0.6849\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67306\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5946 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67306\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5853 - val_loss: 0.6794\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67306\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5771 - val_loss: 0.6726\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.67306 to 0.67257, saving model to Radiant_NN_2.h5\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5601 - val_loss: 0.6648\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.67257 to 0.66477, saving model to Radiant_NN_2.h5\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5468 - val_loss: 0.6634\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.66477 to 0.66342, saving model to Radiant_NN_2.h5\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5380 - val_loss: 0.6685\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.66342\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5313 - val_loss: 0.6640\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.66342\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5228 - val_loss: 0.6735\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66342\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5212 - val_loss: 0.6592\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.66342 to 0.65917, saving model to Radiant_NN_2.h5\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5053 - val_loss: 0.6599\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.65917\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5057 - val_loss: 0.6652\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.65917\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5041 - val_loss: 0.6597\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.65917\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4794 - val_loss: 0.6627\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.65917\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4827 - val_loss: 0.6626\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.65917\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4662 - val_loss: 0.6692\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.65917\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4611 - val_loss: 0.6709\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65917\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4596 - val_loss: 0.6767\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65917\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4580 - val_loss: 0.6581\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.65917 to 0.65809, saving model to Radiant_NN_2.h5\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4454 - val_loss: 0.6586\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65809\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4534 - val_loss: 0.6586\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65809\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4403 - val_loss: 0.6637\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65809\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4326 - val_loss: 0.6471\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.65809 to 0.64707, saving model to Radiant_NN_2.h5\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4299 - val_loss: 0.6581\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.64707\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4252 - val_loss: 0.6575\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.64707\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4169 - val_loss: 0.6689\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.64707\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4152 - val_loss: 0.6620\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.64707\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4170 - val_loss: 0.6592\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.64707\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4021 - val_loss: 0.6734\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.64707\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4071 - val_loss: 0.6790\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.64707\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3939 - val_loss: 0.6757\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.64707\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3939 - val_loss: 0.6594\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.64707\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3844 - val_loss: 0.6748\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.64707\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3861 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.64707\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3850 - val_loss: 0.6800\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.64707\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3798 - val_loss: 0.6833\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.64707\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3737 - val_loss: 0.6890\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.64707\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3712 - val_loss: 0.6672\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.64707\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3651 - val_loss: 0.6808\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.64707\n",
            "Epoch 56/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3654 - val_loss: 0.6916\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.64707\n",
            "Epoch 57/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3583 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.64707\n",
            "Epoch 58/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3672 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.64707\n",
            "Epoch 59/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3589 - val_loss: 0.6743\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.64707\n",
            "Epoch 60/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3574 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.64707\n",
            "Epoch 61/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3493 - val_loss: 0.6811\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.64707\n",
            "Epoch 62/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3466 - val_loss: 0.6737\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.64707\n",
            "Epoch 63/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3418 - val_loss: 0.6833\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.64707\n",
            "Epoch 64/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3447 - val_loss: 0.6772\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.64707\n",
            "Epoch 00064: early stopping\n",
            "fold #2 Log Loss: 0.6470665127967771\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 18s 14ms/step - loss: 1.2354 - val_loss: 0.8955\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89546, saving model to Radiant_NN_3.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9476 - val_loss: 0.8519\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89546 to 0.85187, saving model to Radiant_NN_3.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.8874 - val_loss: 0.8238\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85187 to 0.82380, saving model to Radiant_NN_3.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8553 - val_loss: 0.8329\n",
            "\n",
            "Epoch 00004: val_loss did not improve from 0.82380\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8222 - val_loss: 0.7850\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.82380 to 0.78501, saving model to Radiant_NN_3.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8008 - val_loss: 0.7746\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78501 to 0.77455, saving model to Radiant_NN_3.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7633 - val_loss: 0.7681\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77455 to 0.76814, saving model to Radiant_NN_3.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7486 - val_loss: 0.7493\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76814 to 0.74931, saving model to Radiant_NN_3.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7321 - val_loss: 0.7494\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 0.74931\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7130 - val_loss: 0.7395\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74931 to 0.73948, saving model to Radiant_NN_3.h5\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6905 - val_loss: 0.7379\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73948 to 0.73787, saving model to Radiant_NN_3.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6807 - val_loss: 0.7195\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73787 to 0.71952, saving model to Radiant_NN_3.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6592 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71952 to 0.70974, saving model to Radiant_NN_3.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6490 - val_loss: 0.7118\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70974\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6372 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70974 to 0.70654, saving model to Radiant_NN_3.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6213 - val_loss: 0.6923\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70654 to 0.69226, saving model to Radiant_NN_3.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6141 - val_loss: 0.7036\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69226\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5996 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69226\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5852 - val_loss: 0.6865\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69226 to 0.68649, saving model to Radiant_NN_3.h5\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5746 - val_loss: 0.6966\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68649\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5621 - val_loss: 0.6944\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68649\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5571 - val_loss: 0.6909\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68649\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5518 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68649\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5433 - val_loss: 0.6944\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68649\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5268 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68649\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5165 - val_loss: 0.6786\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68649 to 0.67863, saving model to Radiant_NN_3.h5\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5120 - val_loss: 0.6790\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67863\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5008 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67863\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5006 - val_loss: 0.6765\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.67863 to 0.67654, saving model to Radiant_NN_3.h5\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4866 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67654\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4799 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67654\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4688 - val_loss: 0.6878\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67654\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4683 - val_loss: 0.7007\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67654\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4661 - val_loss: 0.6882\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67654\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4557 - val_loss: 0.7022\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67654\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4559 - val_loss: 0.6921\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67654\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4472 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67654\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4344 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67654\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4380 - val_loss: 0.7078\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67654\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4283 - val_loss: 0.6910\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67654\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4205 - val_loss: 0.6787\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67654\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4171 - val_loss: 0.6898\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67654\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4106 - val_loss: 0.7162\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67654\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4213 - val_loss: 0.7161\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67654\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4112 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67654\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4005 - val_loss: 0.6940\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67654\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3977 - val_loss: 0.7098\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67654\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3969 - val_loss: 0.7151\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67654\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3924 - val_loss: 0.7081\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67654\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3921 - val_loss: 0.7122\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67654\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3824 - val_loss: 0.7228\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67654\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3811 - val_loss: 0.7190\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67654\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3748 - val_loss: 0.7239\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67654\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3764 - val_loss: 0.6926\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67654\n",
            "Epoch 00054: early stopping\n",
            "fold #3 Log Loss: 0.6765419456220593\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 13ms/step - loss: 1.2404 - val_loss: 0.9032\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90325, saving model to Radiant_NN_4.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.9397 - val_loss: 0.8624\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90325 to 0.86239, saving model to Radiant_NN_4.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8928 - val_loss: 0.8472\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86239 to 0.84721, saving model to Radiant_NN_4.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8620 - val_loss: 0.8202\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84721 to 0.82019, saving model to Radiant_NN_4.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8287 - val_loss: 0.8070\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.82019 to 0.80698, saving model to Radiant_NN_4.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7891 - val_loss: 0.7724\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.80698 to 0.77237, saving model to Radiant_NN_4.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7655 - val_loss: 0.7589\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77237 to 0.75888, saving model to Radiant_NN_4.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7513 - val_loss: 0.7627\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.75888\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7316 - val_loss: 0.7482\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75888 to 0.74822, saving model to Radiant_NN_4.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7059 - val_loss: 0.7332\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74822 to 0.73319, saving model to Radiant_NN_4.h5\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6981 - val_loss: 0.7191\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73319 to 0.71914, saving model to Radiant_NN_4.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6678 - val_loss: 0.7207\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.71914\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6623 - val_loss: 0.7137\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71914 to 0.71369, saving model to Radiant_NN_4.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6471 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71369 to 0.70574, saving model to Radiant_NN_4.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6277 - val_loss: 0.7074\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70574\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6244 - val_loss: 0.6917\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70574 to 0.69167, saving model to Radiant_NN_4.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6040 - val_loss: 0.6770\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69167 to 0.67697, saving model to Radiant_NN_4.h5\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5982 - val_loss: 0.6824\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67697\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5857 - val_loss: 0.6893\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.67697\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5717 - val_loss: 0.6796\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67697\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5579 - val_loss: 0.6823\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67697\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5480 - val_loss: 0.6781\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67697\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5468 - val_loss: 0.6753\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.67697 to 0.67526, saving model to Radiant_NN_4.h5\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5388 - val_loss: 0.6783\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67526\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5294 - val_loss: 0.6689\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.67526 to 0.66891, saving model to Radiant_NN_4.h5\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5233 - val_loss: 0.6730\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66891\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5125 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66891\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4979 - val_loss: 0.6810\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.66891\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4914 - val_loss: 0.6658\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.66891 to 0.66583, saving model to Radiant_NN_4.h5\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4833 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66583\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4815 - val_loss: 0.6745\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66583\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4776 - val_loss: 0.6564\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.66583 to 0.65641, saving model to Radiant_NN_4.h5\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4737 - val_loss: 0.6687\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65641\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4670 - val_loss: 0.6627\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65641\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4573 - val_loss: 0.6733\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.65641\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4506 - val_loss: 0.6672\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65641\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4556 - val_loss: 0.6750\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65641\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4382 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65641\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4328 - val_loss: 0.6638\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65641\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4316 - val_loss: 0.6620\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65641\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4252 - val_loss: 0.6620\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65641\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4279 - val_loss: 0.6754\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65641\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4175 - val_loss: 0.6666\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65641\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4105 - val_loss: 0.6640\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65641\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4100 - val_loss: 0.6783\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65641\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4063 - val_loss: 0.6730\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65641\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3958 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65641\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3988 - val_loss: 0.6763\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65641\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3919 - val_loss: 0.6702\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65641\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3930 - val_loss: 0.6746\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65641\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3853 - val_loss: 0.6697\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.65641\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3863 - val_loss: 0.6688\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.65641\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3827 - val_loss: 0.6792\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.65641\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3726 - val_loss: 0.6892\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.65641\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3726 - val_loss: 0.6873\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.65641\n",
            "Epoch 56/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3694 - val_loss: 0.6907\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.65641\n",
            "Epoch 57/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3745 - val_loss: 0.6901\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.65641\n",
            "Epoch 00057: early stopping\n",
            "fold #4 Log Loss: 0.6564087342185794\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 12ms/step - loss: 1.2308 - val_loss: 0.8863\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.88628, saving model to Radiant_NN_5.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.9403 - val_loss: 0.8363\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.88628 to 0.83630, saving model to Radiant_NN_5.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8859 - val_loss: 0.8234\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.83630 to 0.82337, saving model to Radiant_NN_5.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8541 - val_loss: 0.8063\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82337 to 0.80631, saving model to Radiant_NN_5.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8267 - val_loss: 0.7815\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80631 to 0.78146, saving model to Radiant_NN_5.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7932 - val_loss: 0.7644\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78146 to 0.76440, saving model to Radiant_NN_5.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7795 - val_loss: 0.7492\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76440 to 0.74918, saving model to Radiant_NN_5.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7511 - val_loss: 0.7366\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.74918 to 0.73657, saving model to Radiant_NN_5.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7278 - val_loss: 0.7260\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.73657 to 0.72602, saving model to Radiant_NN_5.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7042 - val_loss: 0.7304\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72602\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6912 - val_loss: 0.7214\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72602 to 0.72140, saving model to Radiant_NN_5.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6732 - val_loss: 0.7148\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72140 to 0.71476, saving model to Radiant_NN_5.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6662 - val_loss: 0.7036\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71476 to 0.70356, saving model to Radiant_NN_5.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6462 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.70356 to 0.70079, saving model to Radiant_NN_5.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6341 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.70079 to 0.69775, saving model to Radiant_NN_5.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6177 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.69775\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6124 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.69775 to 0.68913, saving model to Radiant_NN_5.h5\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.5914 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68913\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5822 - val_loss: 0.6950\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.68913\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5783 - val_loss: 0.6808\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68913 to 0.68077, saving model to Radiant_NN_5.h5\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5670 - val_loss: 0.6850\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68077\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5571 - val_loss: 0.6866\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68077\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5532 - val_loss: 0.6748\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.68077 to 0.67479, saving model to Radiant_NN_5.h5\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5393 - val_loss: 0.6671\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.67479 to 0.66713, saving model to Radiant_NN_5.h5\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5162 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.66713\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5187 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.66713\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5137 - val_loss: 0.6820\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.66713\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5056 - val_loss: 0.6664\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.66713 to 0.66635, saving model to Radiant_NN_5.h5\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5005 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.66635\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4934 - val_loss: 0.6696\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.66635\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4860 - val_loss: 0.6723\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66635\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4781 - val_loss: 0.6817\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66635\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4642 - val_loss: 0.6795\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66635\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4600 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66635\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4545 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66635\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4512 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66635\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4584 - val_loss: 0.6750\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66635\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4384 - val_loss: 0.6737\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66635\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4432 - val_loss: 0.6738\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66635\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4359 - val_loss: 0.6770\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66635\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4247 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66635\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4270 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66635\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4227 - val_loss: 0.6791\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66635\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4145 - val_loss: 0.6836\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66635\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4165 - val_loss: 0.6814\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66635\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4034 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66635\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3992 - val_loss: 0.6726\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66635\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4058 - val_loss: 0.6992\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66635\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3949 - val_loss: 0.6927\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66635\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3936 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66635\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3901 - val_loss: 0.7054\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66635\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3834 - val_loss: 0.6829\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66635\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3869 - val_loss: 0.6940\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66635\n",
            "Epoch 00053: early stopping\n",
            "fold #5 Log Loss: 0.6663545096843201\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 1.2366 - val_loss: 0.9120\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91201, saving model to Radiant_NN_6.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.9489 - val_loss: 0.8778\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91201 to 0.87777, saving model to Radiant_NN_6.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.9074 - val_loss: 0.8492\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.87777 to 0.84917, saving model to Radiant_NN_6.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8724 - val_loss: 0.8079\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84917 to 0.80790, saving model to Radiant_NN_6.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8265 - val_loss: 0.7943\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80790 to 0.79431, saving model to Radiant_NN_6.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8133 - val_loss: 0.7885\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79431 to 0.78850, saving model to Radiant_NN_6.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7927 - val_loss: 0.7714\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78850 to 0.77144, saving model to Radiant_NN_6.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7702 - val_loss: 0.7558\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77144 to 0.75576, saving model to Radiant_NN_6.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7410 - val_loss: 0.7417\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75576 to 0.74169, saving model to Radiant_NN_6.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7336 - val_loss: 0.7398\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74169 to 0.73976, saving model to Radiant_NN_6.h5\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7171 - val_loss: 0.7282\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73976 to 0.72824, saving model to Radiant_NN_6.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6919 - val_loss: 0.7310\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.72824\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6888 - val_loss: 0.7196\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72824 to 0.71964, saving model to Radiant_NN_6.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6753 - val_loss: 0.7245\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.71964\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6569 - val_loss: 0.7220\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.71964\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6336 - val_loss: 0.6894\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71964 to 0.68944, saving model to Radiant_NN_6.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6261 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.68944\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6184 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68944\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6004 - val_loss: 0.6883\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68944 to 0.68834, saving model to Radiant_NN_6.h5\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5878 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68834\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5827 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.68834 to 0.68223, saving model to Radiant_NN_6.h5\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5766 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.68223\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5637 - val_loss: 0.6859\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68223\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5494 - val_loss: 0.6827\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68223\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5384 - val_loss: 0.6782\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68223 to 0.67824, saving model to Radiant_NN_6.h5\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5400 - val_loss: 0.6803\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67824\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5280 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67824\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5216 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67824\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5137 - val_loss: 0.6783\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67824\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5111 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67824\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4980 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67824\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4827 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67824\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4821 - val_loss: 0.6701\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.67824 to 0.67007, saving model to Radiant_NN_6.h5\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4800 - val_loss: 0.6736\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67007\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4764 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67007\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4665 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67007\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4493 - val_loss: 0.6811\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67007\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4529 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67007\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4485 - val_loss: 0.6819\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67007\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4444 - val_loss: 0.6787\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67007\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4358 - val_loss: 0.6663\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.67007 to 0.66625, saving model to Radiant_NN_6.h5\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4333 - val_loss: 0.7007\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66625\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4283 - val_loss: 0.6908\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66625\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4250 - val_loss: 0.6882\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66625\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4231 - val_loss: 0.6918\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66625\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4214 - val_loss: 0.6958\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66625\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4161 - val_loss: 0.6955\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66625\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4092 - val_loss: 0.6980\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66625\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4104 - val_loss: 0.7026\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66625\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3975 - val_loss: 0.6782\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66625\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3944 - val_loss: 0.6852\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66625\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3896 - val_loss: 0.6873\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66625\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3904 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66625\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3955 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66625\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3786 - val_loss: 0.7069\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66625\n",
            "Epoch 56/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3802 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66625\n",
            "Epoch 57/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3763 - val_loss: 0.6976\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.66625\n",
            "Epoch 58/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3707 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.66625\n",
            "Epoch 59/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3785 - val_loss: 0.6746\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.66625\n",
            "Epoch 60/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3640 - val_loss: 0.6963\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.66625\n",
            "Epoch 61/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3644 - val_loss: 0.6966\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.66625\n",
            "Epoch 62/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3533 - val_loss: 0.7200\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.66625\n",
            "Epoch 63/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3641 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.66625\n",
            "Epoch 64/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3516 - val_loss: 0.7177\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.66625\n",
            "Epoch 65/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3535 - val_loss: 0.7145\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.66625\n",
            "Epoch 66/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3494 - val_loss: 0.7044\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.66625\n",
            "Epoch 00066: early stopping\n",
            "fold #6 Log Loss: 0.6662502440049382\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 12ms/step - loss: 1.2353 - val_loss: 0.9380\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93802, saving model to Radiant_NN_7.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.9490 - val_loss: 0.8659\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93802 to 0.86587, saving model to Radiant_NN_7.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8926 - val_loss: 0.8226\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86587 to 0.82263, saving model to Radiant_NN_7.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8537 - val_loss: 0.8030\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82263 to 0.80302, saving model to Radiant_NN_7.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8251 - val_loss: 0.7789\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80302 to 0.77885, saving model to Radiant_NN_7.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7945 - val_loss: 0.7694\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77885 to 0.76941, saving model to Radiant_NN_7.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7719 - val_loss: 0.7572\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76941 to 0.75716, saving model to Radiant_NN_7.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7500 - val_loss: 0.7399\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75716 to 0.73990, saving model to Radiant_NN_7.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7303 - val_loss: 0.7280\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.73990 to 0.72805, saving model to Radiant_NN_7.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.7070 - val_loss: 0.7283\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72805\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6960 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72805 to 0.72328, saving model to Radiant_NN_7.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6696 - val_loss: 0.7114\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72328 to 0.71145, saving model to Radiant_NN_7.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6547 - val_loss: 0.7114\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.71145 to 0.71143, saving model to Radiant_NN_7.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6409 - val_loss: 0.6955\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71143 to 0.69552, saving model to Radiant_NN_7.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6338 - val_loss: 0.6893\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.69552 to 0.68927, saving model to Radiant_NN_7.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6202 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68927 to 0.68807, saving model to Radiant_NN_7.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6091 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.68807 to 0.68754, saving model to Radiant_NN_7.h5\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5875 - val_loss: 0.6878\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.68754\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5850 - val_loss: 0.6844\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.68754 to 0.68443, saving model to Radiant_NN_7.h5\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5774 - val_loss: 0.6767\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.68443 to 0.67674, saving model to Radiant_NN_7.h5\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5671 - val_loss: 0.6914\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.67674\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5572 - val_loss: 0.6828\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.67674\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5417 - val_loss: 0.6721\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.67674 to 0.67211, saving model to Radiant_NN_7.h5\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5368 - val_loss: 0.6735\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.67211\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5288 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.67211\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5238 - val_loss: 0.6870\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.67211\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5096 - val_loss: 0.6886\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67211\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5046 - val_loss: 0.6747\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67211\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5038 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67211\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4920 - val_loss: 0.6695\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.67211 to 0.66954, saving model to Radiant_NN_7.h5\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4854 - val_loss: 0.6698\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.66954\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4786 - val_loss: 0.6776\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.66954\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4705 - val_loss: 0.6802\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.66954\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4677 - val_loss: 0.6709\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.66954\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4624 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.66954\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4641 - val_loss: 0.6936\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.66954\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4602 - val_loss: 0.6904\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.66954\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4496 - val_loss: 0.6707\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.66954\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4395 - val_loss: 0.6862\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.66954\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4345 - val_loss: 0.6975\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66954\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4324 - val_loss: 0.6899\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66954\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4221 - val_loss: 0.6825\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66954\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4254 - val_loss: 0.6788\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66954\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4168 - val_loss: 0.6848\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66954\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4167 - val_loss: 0.7007\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66954\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4049 - val_loss: 0.6945\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66954\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4071 - val_loss: 0.6828\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66954\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4067 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66954\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3949 - val_loss: 0.6774\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66954\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3941 - val_loss: 0.6762\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66954\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3902 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66954\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3837 - val_loss: 0.7156\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66954\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3824 - val_loss: 0.6966\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66954\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.3741 - val_loss: 0.7108\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66954\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3763 - val_loss: 0.7129\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66954\n",
            "Epoch 00055: early stopping\n",
            "fold #7 Log Loss: 0.6695359834229454\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 12ms/step - loss: 1.2393 - val_loss: 0.8917\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.89168, saving model to Radiant_NN_8.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9483 - val_loss: 0.8507\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.89168 to 0.85066, saving model to Radiant_NN_8.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8856 - val_loss: 0.8319\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85066 to 0.83186, saving model to Radiant_NN_8.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8568 - val_loss: 0.7988\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.83186 to 0.79882, saving model to Radiant_NN_8.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8273 - val_loss: 0.7751\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.79882 to 0.77506, saving model to Radiant_NN_8.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7930 - val_loss: 0.7693\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77506 to 0.76925, saving model to Radiant_NN_8.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7784 - val_loss: 0.7502\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76925 to 0.75022, saving model to Radiant_NN_8.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7523 - val_loss: 0.7424\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75022 to 0.74237, saving model to Radiant_NN_8.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7307 - val_loss: 0.7246\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74237 to 0.72464, saving model to Radiant_NN_8.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 15s 13ms/step - loss: 0.7088 - val_loss: 0.7372\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.72464\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6927 - val_loss: 0.7094\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.72464 to 0.70941, saving model to Radiant_NN_8.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6767 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.70941 to 0.70084, saving model to Radiant_NN_8.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6646 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.70084 to 0.69781, saving model to Radiant_NN_8.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6475 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.69781 to 0.69027, saving model to Radiant_NN_8.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6303 - val_loss: 0.6805\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.69027 to 0.68054, saving model to Radiant_NN_8.h5\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6064 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.68054 to 0.67770, saving model to Radiant_NN_8.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6013 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.67770\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5949 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.67770\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5831 - val_loss: 0.6760\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.67770 to 0.67596, saving model to Radiant_NN_8.h5\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5744 - val_loss: 0.6783\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.67596\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5505 - val_loss: 0.6549\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.67596 to 0.65493, saving model to Radiant_NN_8.h5\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5563 - val_loss: 0.6738\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.65493\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5421 - val_loss: 0.6576\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.65493\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5290 - val_loss: 0.6623\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.65493\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5238 - val_loss: 0.6512\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.65493 to 0.65124, saving model to Radiant_NN_8.h5\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5152 - val_loss: 0.6672\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.65124\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5106 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.65124\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5025 - val_loss: 0.6570\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.65124\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4950 - val_loss: 0.6525\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.65124\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4901 - val_loss: 0.6602\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.65124\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4825 - val_loss: 0.6558\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.65124\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4718 - val_loss: 0.6655\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.65124\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4677 - val_loss: 0.6662\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.65124\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4631 - val_loss: 0.6732\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.65124\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4674 - val_loss: 0.6630\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.65124\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4566 - val_loss: 0.6647\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.65124\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4541 - val_loss: 0.6571\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.65124\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4457 - val_loss: 0.6665\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.65124\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4375 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.65124\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4275 - val_loss: 0.6615\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.65124\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4337 - val_loss: 0.6601\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.65124\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 18s 14ms/step - loss: 0.4248 - val_loss: 0.6714\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.65124\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4141 - val_loss: 0.6598\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.65124\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4137 - val_loss: 0.6624\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.65124\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4061 - val_loss: 0.6717\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.65124\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4189 - val_loss: 0.6617\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.65124\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3949 - val_loss: 0.6722\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.65124\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4002 - val_loss: 0.6761\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.65124\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3981 - val_loss: 0.6714\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.65124\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3936 - val_loss: 0.6608\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.65124\n",
            "Epoch 00050: early stopping\n",
            "fold #8 Log Loss: 0.6512422418390004\n",
            "Epoch 1/200\n",
            "1226/1226 [==============================] - 16s 12ms/step - loss: 1.2356 - val_loss: 0.9018\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.90177, saving model to Radiant_NN_9.h5\n",
            "Epoch 2/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.9531 - val_loss: 0.8523\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.90177 to 0.85231, saving model to Radiant_NN_9.h5\n",
            "Epoch 3/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8948 - val_loss: 0.8451\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85231 to 0.84505, saving model to Radiant_NN_9.h5\n",
            "Epoch 4/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8495 - val_loss: 0.8048\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84505 to 0.80481, saving model to Radiant_NN_9.h5\n",
            "Epoch 5/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8271 - val_loss: 0.7751\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80481 to 0.77511, saving model to Radiant_NN_9.h5\n",
            "Epoch 6/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8061 - val_loss: 0.7682\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77511 to 0.76818, saving model to Radiant_NN_9.h5\n",
            "Epoch 7/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7794 - val_loss: 0.7545\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76818 to 0.75451, saving model to Radiant_NN_9.h5\n",
            "Epoch 8/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7452 - val_loss: 0.7502\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75451 to 0.75021, saving model to Radiant_NN_9.h5\n",
            "Epoch 9/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7297 - val_loss: 0.7301\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75021 to 0.73013, saving model to Radiant_NN_9.h5\n",
            "Epoch 10/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7200 - val_loss: 0.7359\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.73013\n",
            "Epoch 11/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6892 - val_loss: 0.7270\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73013 to 0.72701, saving model to Radiant_NN_9.h5\n",
            "Epoch 12/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6854 - val_loss: 0.7246\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72701 to 0.72458, saving model to Radiant_NN_9.h5\n",
            "Epoch 13/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6635 - val_loss: 0.7171\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72458 to 0.71709, saving model to Radiant_NN_9.h5\n",
            "Epoch 14/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6455 - val_loss: 0.7040\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.71709 to 0.70402, saving model to Radiant_NN_9.h5\n",
            "Epoch 15/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6346 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70402\n",
            "Epoch 16/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6206 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70402 to 0.70123, saving model to Radiant_NN_9.h5\n",
            "Epoch 17/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6059 - val_loss: 0.6962\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.70123 to 0.69616, saving model to Radiant_NN_9.h5\n",
            "Epoch 18/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5997 - val_loss: 0.7002\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69616\n",
            "Epoch 19/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5828 - val_loss: 0.7063\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.69616\n",
            "Epoch 20/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5732 - val_loss: 0.7026\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69616\n",
            "Epoch 21/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5642 - val_loss: 0.7040\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69616\n",
            "Epoch 22/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5577 - val_loss: 0.6958\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.69616 to 0.69583, saving model to Radiant_NN_9.h5\n",
            "Epoch 23/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5414 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69583 to 0.68747, saving model to Radiant_NN_9.h5\n",
            "Epoch 24/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5362 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68747\n",
            "Epoch 25/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5299 - val_loss: 0.6808\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68747 to 0.68082, saving model to Radiant_NN_9.h5\n",
            "Epoch 26/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5163 - val_loss: 0.6927\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68082\n",
            "Epoch 27/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5215 - val_loss: 0.6838\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68082\n",
            "Epoch 28/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5058 - val_loss: 0.6825\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68082\n",
            "Epoch 29/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4911 - val_loss: 0.6897\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68082\n",
            "Epoch 30/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4957 - val_loss: 0.6814\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68082\n",
            "Epoch 31/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4809 - val_loss: 0.6872\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68082\n",
            "Epoch 32/200\n",
            "1226/1226 [==============================] - 16s 13ms/step - loss: 0.4731 - val_loss: 0.6849\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68082\n",
            "Epoch 33/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4666 - val_loss: 0.6853\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68082\n",
            "Epoch 34/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4686 - val_loss: 0.6811\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68082\n",
            "Epoch 35/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4572 - val_loss: 0.6811\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68082\n",
            "Epoch 36/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4623 - val_loss: 0.6983\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68082\n",
            "Epoch 37/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4395 - val_loss: 0.6913\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68082\n",
            "Epoch 38/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4405 - val_loss: 0.6843\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68082\n",
            "Epoch 39/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4360 - val_loss: 0.6830\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68082\n",
            "Epoch 40/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4344 - val_loss: 0.6791\n",
            "\n",
            "Epoch 00040: val_loss improved from 0.68082 to 0.67907, saving model to Radiant_NN_9.h5\n",
            "Epoch 41/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4286 - val_loss: 0.6895\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67907\n",
            "Epoch 42/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4159 - val_loss: 0.6906\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67907\n",
            "Epoch 43/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4119 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67907\n",
            "Epoch 44/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4179 - val_loss: 0.6935\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67907\n",
            "Epoch 45/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4131 - val_loss: 0.6848\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67907\n",
            "Epoch 46/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4000 - val_loss: 0.6768\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.67907 to 0.67680, saving model to Radiant_NN_9.h5\n",
            "Epoch 47/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4046 - val_loss: 0.7250\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67680\n",
            "Epoch 48/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3981 - val_loss: 0.6941\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67680\n",
            "Epoch 49/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3906 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67680\n",
            "Epoch 50/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3907 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67680\n",
            "Epoch 51/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3831 - val_loss: 0.7157\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67680\n",
            "Epoch 52/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3847 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67680\n",
            "Epoch 53/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3788 - val_loss: 0.7194\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67680\n",
            "Epoch 54/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3725 - val_loss: 0.7025\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67680\n",
            "Epoch 55/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3775 - val_loss: 0.7039\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67680\n",
            "Epoch 56/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3670 - val_loss: 0.6928\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67680\n",
            "Epoch 57/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3673 - val_loss: 0.7255\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67680\n",
            "Epoch 58/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3672 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67680\n",
            "Epoch 59/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3627 - val_loss: 0.7188\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67680\n",
            "Epoch 60/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3577 - val_loss: 0.7199\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67680\n",
            "Epoch 61/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3539 - val_loss: 0.7166\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67680\n",
            "Epoch 62/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3520 - val_loss: 0.7146\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67680\n",
            "Epoch 63/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3497 - val_loss: 0.7146\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67680\n",
            "Epoch 64/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3429 - val_loss: 0.7110\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67680\n",
            "Epoch 65/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3438 - val_loss: 0.7140\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67680\n",
            "Epoch 66/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3429 - val_loss: 0.7314\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67680\n",
            "Epoch 67/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3437 - val_loss: 0.7253\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67680\n",
            "Epoch 68/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3350 - val_loss: 0.7410\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67680\n",
            "Epoch 69/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3247 - val_loss: 0.7202\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67680\n",
            "Epoch 70/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3201 - val_loss: 0.7406\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67680\n",
            "Epoch 71/200\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.3239 - val_loss: 0.7165\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67680\n",
            "Epoch 00071: early stopping\n",
            "fold #9 Log Loss: 0.6768020085595189\n",
            "Full Log loss 0.6639960745034015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T09:15:49.958038Z",
          "iopub.status.busy": "2021-09-29T09:15:49.957280Z",
          "iopub.status.idle": "2021-09-29T09:15:50.001839Z",
          "shell.execute_reply": "2021-09-29T09:15:50.001232Z",
          "shell.execute_reply.started": "2021-09-26T08:55:00.281909Z"
        },
        "id": "d9b004d8",
        "papermill": {
          "duration": 40.095501,
          "end_time": "2021-09-29T09:15:50.001972",
          "exception": false,
          "start_time": "2021-09-29T09:15:09.906471",
          "status": "completed"
        },
        "tags": [],
        "outputId": "a550112e-2325-4679-d7f5-168385e5acc9"
      },
      "source": [
        "print('NN LOG LOSS :',log_loss(y_train,y_oof)) "
      ],
      "id": "d9b004d8",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN LOG LOSS : 0.6639960505955135\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T09:18:47.900981Z",
          "iopub.status.busy": "2021-09-29T09:18:47.900308Z",
          "iopub.status.idle": "2021-09-29T09:18:47.924013Z",
          "shell.execute_reply": "2021-09-29T09:18:47.924558Z",
          "shell.execute_reply.started": "2021-09-19T09:36:32.581317Z"
        },
        "id": "fff79466",
        "papermill": {
          "duration": 39.365533,
          "end_time": "2021-09-29T09:18:47.924720",
          "exception": false,
          "start_time": "2021-09-29T09:18:08.559187",
          "status": "completed"
        },
        "tags": [],
        "outputId": "40599105-d725-4abe-d8da-5304a35489f3"
      },
      "source": [
        "# In this part we format the DataFrame to have column names and order similar to the sample submission file. \n",
        "pred_df = pd.DataFrame(y_test)\n",
        "pred_df = pred_df.rename(columns={\n",
        "    0:'Crop_ID_1',\n",
        "    1:'Crop_ID_2', \n",
        "    2:'Crop_ID_3',\n",
        "    3:'Crop_ID_4',\n",
        "    4:'Crop_ID_5',\n",
        "    5:'Crop_ID_6',\n",
        "    6:'Crop_ID_7',\n",
        "    7:'Crop_ID_8',\n",
        "    8:'Crop_ID_9'\n",
        "})\n",
        "pred_df['field_id'] = Test['field_id'].astype('int').values\n",
        "pred_df = pred_df[['field_id', 'Crop_ID_1', 'Crop_ID_2', 'Crop_ID_3', 'Crop_ID_4', 'Crop_ID_5', 'Crop_ID_6', 'Crop_ID_7', 'Crop_ID_8', 'Crop_ID_9']]\n",
        "pred_df.head()"
      ],
      "id": "fff79466",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>field_id</th>\n",
              "      <th>Crop_ID_1</th>\n",
              "      <th>Crop_ID_2</th>\n",
              "      <th>Crop_ID_3</th>\n",
              "      <th>Crop_ID_4</th>\n",
              "      <th>Crop_ID_5</th>\n",
              "      <th>Crop_ID_6</th>\n",
              "      <th>Crop_ID_7</th>\n",
              "      <th>Crop_ID_8</th>\n",
              "      <th>Crop_ID_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.046520</td>\n",
              "      <td>0.328960</td>\n",
              "      <td>0.006543</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.212916</td>\n",
              "      <td>0.362892</td>\n",
              "      <td>3.914557e-02</td>\n",
              "      <td>1.072579e-03</td>\n",
              "      <td>0.001784</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39</td>\n",
              "      <td>0.686957</td>\n",
              "      <td>0.241258</td>\n",
              "      <td>0.019341</td>\n",
              "      <td>0.018311</td>\n",
              "      <td>0.028357</td>\n",
              "      <td>0.005650</td>\n",
              "      <td>7.911746e-05</td>\n",
              "      <td>1.481858e-05</td>\n",
              "      <td>0.000031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49</td>\n",
              "      <td>0.023400</td>\n",
              "      <td>0.414496</td>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.002732</td>\n",
              "      <td>0.454757</td>\n",
              "      <td>9.625371e-02</td>\n",
              "      <td>7.462219e-03</td>\n",
              "      <td>0.000587</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000054</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.999428</td>\n",
              "      <td>0.000405</td>\n",
              "      <td>0.000002</td>\n",
              "      <td>8.097651e-07</td>\n",
              "      <td>7.920195e-09</td>\n",
              "      <td>0.000003</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>0.934724</td>\n",
              "      <td>0.010780</td>\n",
              "      <td>0.000454</td>\n",
              "      <td>0.000020</td>\n",
              "      <td>0.000393</td>\n",
              "      <td>0.022358</td>\n",
              "      <td>2.919895e-02</td>\n",
              "      <td>2.066566e-03</td>\n",
              "      <td>0.000005</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   field_id  Crop_ID_1  Crop_ID_2  Crop_ID_3  Crop_ID_4  Crop_ID_5  Crop_ID_6  \\\n",
              "0        30   0.046520   0.328960   0.006543   0.000167   0.212916   0.362892   \n",
              "1        39   0.686957   0.241258   0.019341   0.018311   0.028357   0.005650   \n",
              "2        49   0.023400   0.414496   0.000069   0.000243   0.002732   0.454757   \n",
              "3        54   0.000009   0.000054   0.000099   0.999428   0.000405   0.000002   \n",
              "4        56   0.934724   0.010780   0.000454   0.000020   0.000393   0.022358   \n",
              "\n",
              "      Crop_ID_7     Crop_ID_8  Crop_ID_9  \n",
              "0  3.914557e-02  1.072579e-03   0.001784  \n",
              "1  7.911746e-05  1.481858e-05   0.000031  \n",
              "2  9.625371e-02  7.462219e-03   0.000587  \n",
              "3  8.097651e-07  7.920195e-09   0.000003  \n",
              "4  2.919895e-02  2.066566e-03   0.000005  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-29T09:20:04.994573Z",
          "iopub.status.busy": "2021-09-29T09:20:04.993935Z",
          "iopub.status.idle": "2021-09-29T09:20:05.428404Z",
          "shell.execute_reply": "2021-09-29T09:20:05.427838Z",
          "shell.execute_reply.started": "2021-09-19T09:36:38.613679Z"
        },
        "id": "43dbab4d",
        "papermill": {
          "duration": 36.669937,
          "end_time": "2021-09-29T09:20:05.428570",
          "exception": false,
          "start_time": "2021-09-29T09:19:28.758633",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Write the predicted probabilites to a csv for submission\n",
        "pred_df.to_csv('S1_NN.csv', index=False)"
      ],
      "id": "43dbab4d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMOPuaFC0hFx"
      },
      "source": [
        "np.save('S1_oof_NN.npy',y_oof)"
      ],
      "id": "MMOPuaFC0hFx",
      "execution_count": null,
      "outputs": []
    }
  ]
}