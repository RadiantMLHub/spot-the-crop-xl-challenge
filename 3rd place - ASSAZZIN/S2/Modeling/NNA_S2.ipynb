{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "papermill": {
      "default_parameters": {},
      "duration": 10497.174924,
      "end_time": "2021-09-30T09:54:34.092266",
      "environment_variables": {},
      "exception": null,
      "input_path": "__notebook__.ipynb",
      "output_path": "__notebook__.ipynb",
      "parameters": {},
      "start_time": "2021-09-30T06:59:36.917342",
      "version": "2.3.3"
    },
    "colab": {
      "name": "NNA_S2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76a5c903",
        "papermill": {
          "duration": 0.055792,
          "end_time": "2021-09-30T06:46:36.950143",
          "exception": false,
          "start_time": "2021-09-30T06:46:36.894351",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# SETUP"
      ],
      "id": "76a5c903"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxysVeaJRcJj"
      },
      "source": [
        "# NOTE: Please Run this on  Tesla P100-PCIE [ 16280 Mib ]"
      ],
      "id": "TxysVeaJRcJj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:59:43.435526Z",
          "iopub.status.busy": "2021-09-30T06:59:43.434084Z",
          "iopub.status.idle": "2021-09-30T06:59:44.185275Z",
          "shell.execute_reply": "2021-09-30T06:59:44.184663Z",
          "shell.execute_reply.started": "2021-09-30T06:50:05.530296Z"
        },
        "papermill": {
          "duration": 0.79393,
          "end_time": "2021-09-30T06:59:44.185427",
          "exception": false,
          "start_time": "2021-09-30T06:59:43.391497",
          "status": "completed"
        },
        "tags": [],
        "id": "d4699373",
        "outputId": "b4ead7a1-d40a-442d-964b-08b5b8eeb98b"
      },
      "source": [
        "!nvidia-smi"
      ],
      "id": "d4699373",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Thu Sep 30 06:59:44 2021       \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| NVIDIA-SMI 450.119.04   Driver Version: 450.119.04   CUDA Version: 11.0     |\r\n",
            "|-------------------------------+----------------------+----------------------+\r\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
            "|                               |                      |               MIG M. |\r\n",
            "|===============================+======================+======================|\r\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\r\n",
            "| N/A   33C    P0    27W / 250W |      0MiB / 16280MiB |      0%      Default |\r\n",
            "|                               |                      |                  N/A |\r\n",
            "+-------------------------------+----------------------+----------------------+\r\n",
            "                                                                               \r\n",
            "+-----------------------------------------------------------------------------+\r\n",
            "| Processes:                                                                  |\r\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\r\n",
            "|        ID   ID                                                   Usage      |\r\n",
            "|=============================================================================|\r\n",
            "|  No running processes found                                                 |\r\n",
            "+-----------------------------------------------------------------------------+\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyI0fQ2ESzKV"
      },
      "source": [
        "!pip install -r requirements_kaggle.txt -q"
      ],
      "id": "PyI0fQ2ESzKV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bwx59hhN_Xk4"
      },
      "source": [
        "> To speed up the review process , i provided the ***drive id*** of the data i've created from the Train creation folder noteboooks .\n",
        "---\n",
        "> I  also add each data drive link in the Readme Pdf file attached with this solution"
      ],
      "id": "bwx59hhN_Xk4"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:46:37.057399Z",
          "iopub.status.busy": "2021-09-30T06:46:37.056725Z",
          "iopub.status.idle": "2021-09-30T06:46:58.480731Z",
          "shell.execute_reply": "2021-09-30T06:46:58.480033Z",
          "shell.execute_reply.started": "2021-09-30T06:40:20.168742Z"
        },
        "papermill": {
          "duration": 21.482379,
          "end_time": "2021-09-30T06:46:58.480907",
          "exception": false,
          "start_time": "2021-09-30T06:46:36.998528",
          "status": "completed"
        },
        "tags": [],
        "id": "57fa6259",
        "outputId": "9a037ac9-b938-40bb-c9d6-e47e05f02cb6"
      },
      "source": [
        "!pip install -q gdown"
      ],
      "id": "57fa6259",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:46:58.686826Z",
          "iopub.status.busy": "2021-09-30T06:46:58.683338Z",
          "iopub.status.idle": "2021-09-30T06:47:29.355912Z",
          "shell.execute_reply": "2021-09-30T06:47:29.356423Z",
          "shell.execute_reply.started": "2021-09-30T06:40:42.145842Z"
        },
        "papermill": {
          "duration": 30.736026,
          "end_time": "2021-09-30T06:47:29.356665",
          "exception": false,
          "start_time": "2021-09-30T06:46:58.620639",
          "status": "completed"
        },
        "tags": [],
        "id": "65fa5192"
      },
      "source": [
        "!gdown --id 1hNRbtcqd9F6stMOK1xAZApDITwAjiSDJ\n",
        "!gdown --id 1-QCmWsNGREXuWArifN0nD_Sp4hJxf0tu"
      ],
      "id": "65fa5192",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:47:29.591060Z",
          "iopub.status.busy": "2021-09-30T06:47:29.587631Z",
          "iopub.status.idle": "2021-09-30T06:47:59.497978Z",
          "shell.execute_reply": "2021-09-30T06:47:59.497344Z",
          "shell.execute_reply.started": "2021-09-30T06:41:15.674837Z"
        },
        "papermill": {
          "duration": 29.981016,
          "end_time": "2021-09-30T06:47:59.498133",
          "exception": false,
          "start_time": "2021-09-30T06:47:29.517117",
          "status": "completed"
        },
        "tags": [],
        "id": "2b3ea2cc"
      },
      "source": [
        "!gdown --id 1-47L_1NKLeVgW1vWmqXXXCuWZ3gwZWsS\n",
        "!gdown --id 1-aO4FEtv5CF-ZOcxDSO3jGEzPcIFdxgP"
      ],
      "id": "2b3ea2cc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:47:59.748679Z",
          "iopub.status.busy": "2021-09-30T06:47:59.747858Z",
          "iopub.status.idle": "2021-09-30T06:48:28.292944Z",
          "shell.execute_reply": "2021-09-30T06:48:28.292324Z",
          "shell.execute_reply.started": "2021-09-30T06:41:48.558626Z"
        },
        "papermill": {
          "duration": 28.624889,
          "end_time": "2021-09-30T06:48:28.293090",
          "exception": false,
          "start_time": "2021-09-30T06:47:59.668201",
          "status": "completed"
        },
        "tags": [],
        "id": "4a0f0ebd"
      },
      "source": [
        "!gdown --id 1-8J_xFgI0WKT5UXFnfH4q1KUw_KgNY37\n",
        "!gdown --id 1-a55a7N6a4SoqolPF_wI4C6Q70u_d7Hj"
      ],
      "id": "4a0f0ebd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:48:28.589656Z",
          "iopub.status.busy": "2021-09-30T06:48:28.585655Z",
          "iopub.status.idle": "2021-09-30T06:48:59.555103Z",
          "shell.execute_reply": "2021-09-30T06:48:59.554402Z",
          "shell.execute_reply.started": "2021-09-30T06:42:21.582947Z"
        },
        "papermill": {
          "duration": 31.063816,
          "end_time": "2021-09-30T06:48:59.555269",
          "exception": false,
          "start_time": "2021-09-30T06:48:28.491453",
          "status": "completed"
        },
        "tags": [],
        "id": "f31228f0"
      },
      "source": [
        "!gdown --id 1-BgXQwmXqBuk_P8VtvLfdLqy83dv56Kz\n",
        "!gdown --id 1-hQGF2TNBbsy3jsGNtndmK55egbdFDjs"
      ],
      "id": "f31228f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84022bc0",
        "papermill": {
          "duration": 0.067511,
          "end_time": "2021-09-30T06:48:59.690681",
          "exception": false,
          "start_time": "2021-09-30T06:48:59.623170",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## LIBRARIES"
      ],
      "id": "84022bc0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:48:59.841117Z",
          "iopub.status.busy": "2021-09-30T06:48:59.840370Z",
          "iopub.status.idle": "2021-09-30T06:49:06.419754Z",
          "shell.execute_reply": "2021-09-30T06:49:06.420757Z",
          "shell.execute_reply.started": "2021-09-30T06:42:56.082578Z"
        },
        "papermill": {
          "duration": 6.65955,
          "end_time": "2021-09-30T06:49:06.421052",
          "exception": false,
          "start_time": "2021-09-30T06:48:59.761502",
          "status": "completed"
        },
        "tags": [],
        "id": "45574b83",
        "outputId": "09acac3e-aa8f-41e3-f066-476d6c7d2a85"
      },
      "source": [
        "#import necessary dependecies\n",
        "import os\n",
        "import numpy as np  \n",
        "import pandas as pd\n",
        "\n",
        "import random\n",
        "from tqdm import tqdm \n",
        "import copy\n",
        "\n",
        "from sklearn.preprocessing import  LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.metrics import log_loss\n",
        "from sklearn.preprocessing import QuantileTransformer\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore') \n",
        "\n",
        "# tf \n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Concatenate , Input ,concatenate ,add\n",
        "from tensorflow.keras.models import Sequential , Model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dense, Activation, Flatten, Convolution1D, Dropout\n",
        "from tensorflow.keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
        "from tensorflow.keras.preprocessing import sequence, text\n",
        "from tensorflow.keras.callbacks import EarlyStopping , LearningRateScheduler\n",
        "from tensorflow.keras.metrics import Accuracy\n",
        "from tensorflow.keras.initializers import glorot_normal , l2\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "# fix seed\n",
        "tf.random.set_seed(111)\n",
        "np.random.seed(111)\n",
        "random.seed(111)"
      ],
      "id": "45574b83",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-30 06:49:01.807573: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /opt/conda/lib\n",
            "2021-09-30 06:49:01.807709: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uftWiLyuj9Na"
      },
      "source": [
        "## Train Creation"
      ],
      "id": "uftWiLyuj9Na"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qtbt64vOkTOy"
      },
      "source": [
        "def create_train():\n",
        "  train =pd.read_csv(\"S2TrainObs1.csv\" )\n",
        "\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "Qtbt64vOkTOy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PvJMVAkO-fFs"
      },
      "source": [
        "def create_test():\n",
        "  test =pd.read_csv(\"S2TestObs1.csv\" )\n",
        "  return  test"
      ],
      "id": "PvJMVAkO-fFs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_3g9cwRp8_SF"
      },
      "source": [
        "def createObs2_train():\n",
        "  train =pd.read_csv(\"S2TrainObs2.csv\" )\n",
        "\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "_3g9cwRp8_SF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Re8kWJQWC8jK"
      },
      "source": [
        "def createObs2_test():\n",
        "  test =pd.read_csv(\"S2TestObs2.csv\" )\n",
        "  return test"
      ],
      "id": "Re8kWJQWC8jK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OsFgaYj7Z85k"
      },
      "source": [
        "def createObs3_train():\n",
        "  train =pd.read_csv(\"S2TrainObs3.csv\" )\n",
        "\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "OsFgaYj7Z85k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6qjVNLtZ83D"
      },
      "source": [
        "def createObs3_test():\n",
        "  test =pd.read_csv(\"S2TestObs3.csv\" )\n",
        "  return test"
      ],
      "id": "D6qjVNLtZ83D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XGwIJQOjJnZh"
      },
      "source": [
        "def createObs4_train():\n",
        "  train =pd.read_csv(\"S2TrainObs4.csv\" )\n",
        "\n",
        "  train = train.groupby('field_id').median().reset_index().sort_values('field_id')\n",
        "  train.label = train.label.astype('int')\n",
        "  return train"
      ],
      "id": "XGwIJQOjJnZh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LKqH411JnWz"
      },
      "source": [
        "def createObs4_test():\n",
        "  test =pd.read_csv(\"S2TestObs4.csv\" )\n",
        "  return test"
      ],
      "id": "5LKqH411JnWz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6926bf88",
        "papermill": {
          "duration": 0.066942,
          "end_time": "2021-09-30T06:49:08.119019",
          "exception": false,
          "start_time": "2021-09-30T06:49:08.052077",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "## Feature Engineering"
      ],
      "id": "6926bf88"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:08.258128Z",
          "iopub.status.busy": "2021-09-30T06:49:08.257358Z",
          "iopub.status.idle": "2021-09-30T06:49:08.474133Z",
          "shell.execute_reply": "2021-09-30T06:49:08.473564Z",
          "shell.execute_reply.started": "2021-09-30T06:43:03.474868Z"
        },
        "id": "175229b3",
        "papermill": {
          "duration": 0.28741,
          "end_time": "2021-09-30T06:49:08.474282",
          "exception": false,
          "start_time": "2021-09-30T06:49:08.186872",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "def process(T) :\n",
        "\n",
        "  # process bands\n",
        "  Bcols = T.filter(like='B').columns.tolist()\n",
        "  Vcols = T.filter(like='V').columns.tolist()\n",
        "  Obs1 = T.filter(like='Month4').columns.tolist()\n",
        "  Obs2 = T.filter(like='Month5').columns.tolist()\n",
        "  Obs3 = T.filter(like='Month6').columns.tolist()\n",
        "  Obs4 = T.filter(like='Month7').columns.tolist()\n",
        "  Obs5 = T.filter(like='Month8').columns.tolist()\n",
        "  Obs6 = T.filter(like='Month9').columns.tolist()\n",
        "  Obs7 = T.filter(like='Month10').columns.tolist()\n",
        "  Obs8 = T.filter(like='Month11').columns.tolist()\n",
        "\n",
        "\n",
        "  # vegetation indexes \n",
        "  B8cols = T.filter(like='B8_').columns.tolist()\n",
        "  B8cols = [x for x in B8cols if 'std' not in x]\n",
        "  \n",
        "  B4cols = T.filter(like='B4_').columns.tolist()\n",
        "  B4cols = [x for x in B4cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B5cols = T.filter(like='B5_').columns.tolist()\n",
        "  B5cols = [x for x in B5cols if 'std' not in x]\n",
        "\n",
        "  B3cols = T.filter(like='B3_').columns.tolist()\n",
        "  B3cols = [x for x in B3cols if 'std' not in x]\n",
        "\n",
        "  B2cols = T.filter(like='B2_').columns.tolist()\n",
        "  B2cols = [x for x in B2cols if 'std' not in x]\n",
        "\n",
        "  B7cols = T.filter(like='B7_').columns.tolist()\n",
        "  B7cols = [x for x in B7cols if 'std' not in x]\n",
        "\n",
        "  B8Acols = T.filter(like='B8A_').columns.tolist()\n",
        "  B8Acols = [x for x in B8Acols if 'std' not in x]\n",
        "\n",
        "  B6cols = T.filter(like='B6_').columns.tolist()\n",
        "  B6cols = [x for x in B6cols if 'std' not in x]\n",
        "  \n",
        "  B12cols = T.filter(like='B12_').columns.tolist()\n",
        "  B12cols = [x for x in B12cols if 'std' not in x]\n",
        "\n",
        "  B11cols = T.filter(like='B11_').columns.tolist()\n",
        "  B11cols = [x for x in B11cols if 'std' not in x]\n",
        "\n",
        "  B1cols = T.filter(like='B1_').columns.tolist()\n",
        "  B1cols = [x for x in B1cols if 'std' not in x]\n",
        "\n",
        "  B9cols = T.filter(like='B9_').columns.tolist()\n",
        "  B9cols = [x for x in B9cols if 'std' not in x]\n",
        "\n",
        "  L = 0.725\n",
        "  for b1,b2 ,b3 ,b4, b5 , b6, b7, b8 ,b8a ,b9,b11,b12 in zip(B1cols,B2cols,B3cols,B4cols,B5cols,B6cols,B7cols,B8cols,B8Acols,B9cols,B11cols,B12cols) :\n",
        "    T[f'NDVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4])).values.clip(-5,5)\n",
        "    T[f'SAVI_{b8.split(\"_\")[1]}']   = ((T[b8] - T[b4]) /  (T[b8] + T[b4]+L) * (1.0 + L)).values.clip(-5,5)\n",
        "    # T[f'CCCI_{b8.split(\"_\")[1]}']   =  ((T[b8] - T[b5]) /  (T[b8] + T[b5])) / ((T[b8] - T[b4]) /  (T[b8] + T[b4]))\n",
        "    T[f'GRNDVI_{b8.split(\"_\")[1]}'] = ((T[b8] - (T[b3]+T[b4])) /  (T[b8] + (T[b3]+T[b4]))).values.clip(-5,5)\n",
        "    T[f'GNDVI_{b8.split(\"_\")[1]}']  = ((T[b8] - T[b3] ) /  (T[b8] + T[b3])).values.clip(-5,5)\n",
        "    # T[f'WDRVI_{b8.split(\"_\")[1]}']  = (0.1 * T[b5] - T[b4])/ (0.1 * T[b5] + T[b4])\n",
        "    T[f'NDRE_{b8.split(\"_\")[1]}']   = ((T[b5] - T[b4])/ (T[b5] + T[b4])).values.clip(-5,5)\n",
        "    T[f'EVI_{b8.split(\"_\")[1]}']    = (2.5 * (T[b8]  - T[b4] ) / ((T[b8]  + 6.0 * T[b4]  - 7.5 * T[b2]) + 1.0)).values.clip(min=-5,max=5)\n",
        "    T[f'WDRVI_{b8.split(\"_\")[1]}']  = (((8 * T[b8]) - T[b4])/ ((8* T[b8]) + T[b4])).values.clip(-5,5)\n",
        "    T[f'ExBlue_{b8.split(\"_\")[1]}']  = ((2 * T[b2]) - (T[b3]+T[b4]))\n",
        "    T[f'ExGreen_{b8.split(\"_\")[1]}']  = ((2 * T[b3]) - (T[b2]+T[b4]) )\n",
        "    T[f'NDRE7_{b8.split(\"_\")[1]}']   = ((T[b7] - T[b4])/ (T[b7] + T[b4])).values.clip(-5,5)\n",
        "    T[f'MTCI_{b8.split(\"_\")[1]}']   = ((T[b8a] - T[b6])/ (T[b7] + T[b6])).values.clip(-5,5)\n",
        "    T[f'VARI_{b8.split(\"_\")[1]}']   = ((T[b3] - T[b4])/ (T[b3] + T[b4] - T[b2])).values.clip(-5,5)\n",
        "    T[f'b3b1_{b8.split(\"_\")[1]}']  = (T[b3] - T[b1])/ (T[b3] + T[b1])    # B7  / B3\n",
        "    T[f'b11b8_{b8.split(\"_\")[1]}']  = (T[b11] - T[b8])/ (T[b11] + T[b8])    # B7  / B3\n",
        "    T[f'b12b11_{b8.split(\"_\")[1]}']  = (T[b12] - T[b11])/ (T[b12] + T[b11])    # B7  / B3\n",
        "    T[f'b3b4_{b8.split(\"_\")[1]}']  = (T[b3] - T[b4])/ (T[b3] + T[b4])    # B7  / B3\n",
        "    T[f'b9b4_{b8.split(\"_\")[1]}']  = (T[b9] - T[b4])/ (T[b9] + T[b4])    # B7  / B3\n",
        "    T[f'b5b3_{b8.split(\"_\")[1]}']  = (T[b5] - T[b3])/ (T[b5] + T[b3])    # B7  / B3\n",
        "    T[f'b12b3_{b8.split(\"_\")[1]}']  = (T[b12] - T[b3])/ (T[b12] + T[b3])    # B7  / B3\n",
        "    \n",
        "    T[f'b2b1_{b8.split(\"_\")[1]}']  = (T[b2] - T[b1])/ (T[b2] + T[b1])    # B7  / B3\n",
        "    T[f'b4b1_{b8.split(\"_\")[1]}']  = (T[b4] - T[b1])/ (T[b4] + T[b1])    # B7  / B3\n",
        "    T[f'b11b3_{b8.split(\"_\")[1]}']  = (T[b11] - T[b3])/ (T[b11] + T[b3]) \n",
        "    \n",
        "  for col in Bcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "  \n",
        "  for col in Vcols :\n",
        "    T[col] = np.sqrt(T[col])\n",
        "    \n",
        "  for col1,col2,col3,col4,col5,col6,col7,col8 in zip(Obs1,Obs2,Obs3,Obs4,Obs5,Obs6,Obs7,Obs8) :\n",
        "    # T[f'{col1}_{col2}'] = T[[col1,col2]].mean(axis=1)\n",
        "    T[f'{col1.split(\"_\")[0]}_std'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].std(axis=1)\n",
        "    T[f'{col1.split(\"_\")[0]}_mean'] = T[[col1,col2,col3,col4,col5,col6,col7,col8]].mean(axis=1)\n",
        "\n",
        "    \n",
        "  # NDVI \n",
        "  T['water']       = T['NDVI_Month4'].apply(lambda x :1 if x<0 else 0)\n",
        "  T['dense_green'] = T['NDVI_Month4'].apply(lambda x :1 if x>=0.5 else 0)\n",
        "  T['not_green']   = T['NDVI_Month4'].apply(lambda x :1 if( (x>0) & (x<0.5)) else 0)\n",
        "\n",
        "  T['high_green'] = T['SAVI_Month4'].apply(lambda x :1 if x<0.1 else 0)\n",
        "  T['low_green']  = T['SAVI_Month4'].apply(lambda x :1 if x>=0.8 else 0)\n",
        "  T['chlorophyll_EVI'] = T['EVI_Month4'].apply(lambda x :1 if( (x>0.2) & (x<0.8)) else 0)\n",
        "\n",
        "  # process Vegetation indexes\n",
        "\n",
        "  ObsN   = T.filter(like='NDVI_').columns.tolist()\n",
        "  ObsSA  = T.filter(like='SAVI_').columns.tolist()\n",
        "  ObsCC  = T.filter(like='CCCI_').columns.tolist()\n",
        "  ObsWDR = T.filter(like='WDRVI_').columns.tolist()\n",
        "  ObsNDRE7 = T.filter(like='NDRE7_').columns.tolist()\n",
        "\n",
        "  return T"
      ],
      "id": "175229b3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:08.614494Z",
          "iopub.status.busy": "2021-09-30T06:49:08.613825Z",
          "iopub.status.idle": "2021-09-30T06:49:11.119834Z",
          "shell.execute_reply": "2021-09-30T06:49:11.119069Z",
          "shell.execute_reply.started": "2021-09-30T06:43:03.694802Z"
        },
        "id": "6e6eedaf",
        "papermill": {
          "duration": 2.57821,
          "end_time": "2021-09-30T06:49:11.120001",
          "exception": false,
          "start_time": "2021-09-30T06:49:08.541791",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train = create_train()\n",
        "Test = create_test()"
      ],
      "id": "6e6eedaf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:11.264264Z",
          "iopub.status.busy": "2021-09-30T06:49:11.263435Z",
          "iopub.status.idle": "2021-09-30T06:49:11.267122Z",
          "shell.execute_reply": "2021-09-30T06:49:11.267626Z",
          "shell.execute_reply.started": "2021-09-30T06:43:06.279535Z"
        },
        "id": "c85b6b51",
        "papermill": {
          "duration": 0.079049,
          "end_time": "2021-09-30T06:49:11.267791",
          "exception": false,
          "start_time": "2021-09-30T06:49:11.188742",
          "status": "completed"
        },
        "tags": [],
        "outputId": "c58ca82a-d949-4f73-c29a-b3f225fe5974"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "c85b6b51",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 98), (35295, 97))"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:11.408901Z",
          "iopub.status.busy": "2021-09-30T06:49:11.407943Z",
          "iopub.status.idle": "2021-09-30T06:49:13.841565Z",
          "shell.execute_reply": "2021-09-30T06:49:13.840510Z",
          "shell.execute_reply.started": "2021-09-30T06:43:06.290956Z"
        },
        "papermill": {
          "duration": 2.50639,
          "end_time": "2021-09-30T06:49:13.841747",
          "exception": false,
          "start_time": "2021-09-30T06:49:11.335357",
          "status": "completed"
        },
        "tags": [],
        "id": "8bbfa494"
      },
      "source": [
        "Train2 = createObs2_train()\n",
        "Test2 = createObs2_test()"
      ],
      "id": "8bbfa494",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:13.989046Z",
          "iopub.status.busy": "2021-09-30T06:49:13.988147Z",
          "iopub.status.idle": "2021-09-30T06:49:13.992128Z",
          "shell.execute_reply": "2021-09-30T06:49:13.991513Z",
          "shell.execute_reply.started": "2021-09-30T06:43:08.665512Z"
        },
        "papermill": {
          "duration": 0.082611,
          "end_time": "2021-09-30T06:49:13.992262",
          "exception": false,
          "start_time": "2021-09-30T06:49:13.909651",
          "status": "completed"
        },
        "tags": [],
        "id": "aa4c2832",
        "outputId": "bb3fe3db-0f27-47c9-9f05-f5a8eb8eedb6"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "aa4c2832",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 98), (35295, 97))"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:14.137706Z",
          "iopub.status.busy": "2021-09-30T06:49:14.136302Z",
          "iopub.status.idle": "2021-09-30T06:49:16.491321Z",
          "shell.execute_reply": "2021-09-30T06:49:16.490656Z",
          "shell.execute_reply.started": "2021-09-30T06:43:08.674638Z"
        },
        "papermill": {
          "duration": 2.429268,
          "end_time": "2021-09-30T06:49:16.491481",
          "exception": false,
          "start_time": "2021-09-30T06:49:14.062213",
          "status": "completed"
        },
        "tags": [],
        "id": "8b1c8927"
      },
      "source": [
        "Train3 = createObs3_train()\n",
        "Test3 = createObs3_test()"
      ],
      "id": "8b1c8927",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:16.633905Z",
          "iopub.status.busy": "2021-09-30T06:49:16.633020Z",
          "iopub.status.idle": "2021-09-30T06:49:16.636062Z",
          "shell.execute_reply": "2021-09-30T06:49:16.636571Z",
          "shell.execute_reply.started": "2021-09-30T06:43:11.036012Z"
        },
        "papermill": {
          "duration": 0.077121,
          "end_time": "2021-09-30T06:49:16.636750",
          "exception": false,
          "start_time": "2021-09-30T06:49:16.559629",
          "status": "completed"
        },
        "tags": [],
        "id": "c544ba12",
        "outputId": "0520b255-e666-4265-cc8d-a3de16649833"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "c544ba12",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 98), (35295, 97))"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:16.779308Z",
          "iopub.status.busy": "2021-09-30T06:49:16.778637Z",
          "iopub.status.idle": "2021-09-30T06:49:19.051935Z",
          "shell.execute_reply": "2021-09-30T06:49:19.051239Z",
          "shell.execute_reply.started": "2021-09-30T06:43:11.044229Z"
        },
        "papermill": {
          "duration": 2.34657,
          "end_time": "2021-09-30T06:49:19.052091",
          "exception": false,
          "start_time": "2021-09-30T06:49:16.705521",
          "status": "completed"
        },
        "tags": [],
        "id": "2ff7aecd"
      },
      "source": [
        "Train4 = createObs4_train()\n",
        "Test4 = createObs4_test()"
      ],
      "id": "2ff7aecd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:19.217116Z",
          "iopub.status.busy": "2021-09-30T06:49:19.216319Z",
          "iopub.status.idle": "2021-09-30T06:49:19.220702Z",
          "shell.execute_reply": "2021-09-30T06:49:19.221488Z",
          "shell.execute_reply.started": "2021-09-30T06:43:13.287488Z"
        },
        "papermill": {
          "duration": 0.091338,
          "end_time": "2021-09-30T06:49:19.221742",
          "exception": false,
          "start_time": "2021-09-30T06:49:19.130404",
          "status": "completed"
        },
        "tags": [],
        "id": "058f6430",
        "outputId": "fd7aad85-8a56-4286-fdbf-2423f9354d4c"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "058f6430",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 98), (35295, 97))"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:19.380812Z",
          "iopub.status.busy": "2021-09-30T06:49:19.379308Z",
          "iopub.status.idle": "2021-09-30T06:49:25.813393Z",
          "shell.execute_reply": "2021-09-30T06:49:25.812734Z",
          "shell.execute_reply.started": "2021-09-30T06:43:13.296429Z"
        },
        "id": "2fead4ca",
        "papermill": {
          "duration": 6.512531,
          "end_time": "2021-09-30T06:49:25.813551",
          "exception": false,
          "start_time": "2021-09-30T06:49:19.301020",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "Train = process(Train)\n",
        "Test = process(Test)"
      ],
      "id": "2fead4ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:25.958611Z",
          "iopub.status.busy": "2021-09-30T06:49:25.957937Z",
          "iopub.status.idle": "2021-09-30T06:49:25.960333Z",
          "shell.execute_reply": "2021-09-30T06:49:25.960907Z",
          "shell.execute_reply.started": "2021-09-30T06:43:19.686023Z"
        },
        "papermill": {
          "duration": 0.079039,
          "end_time": "2021-09-30T06:49:25.961071",
          "exception": false,
          "start_time": "2021-09-30T06:49:25.882032",
          "status": "completed"
        },
        "tags": [],
        "id": "5d691bb9",
        "outputId": "25a24128-2aaf-4807-ac61-2a3c2cfa1ae8"
      },
      "source": [
        "Train.shape , Test.shape"
      ],
      "id": "5d691bb9",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 304), (35295, 303))"
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:26.101292Z",
          "iopub.status.busy": "2021-09-30T06:49:26.100684Z",
          "iopub.status.idle": "2021-09-30T06:49:32.436176Z",
          "shell.execute_reply": "2021-09-30T06:49:32.436744Z",
          "shell.execute_reply.started": "2021-09-30T06:43:19.699989Z"
        },
        "papermill": {
          "duration": 6.407202,
          "end_time": "2021-09-30T06:49:32.436919",
          "exception": false,
          "start_time": "2021-09-30T06:49:26.029717",
          "status": "completed"
        },
        "tags": [],
        "id": "69253841"
      },
      "source": [
        "Train2 = process(Train2)\n",
        "Test2 = process(Test2)"
      ],
      "id": "69253841",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:32.581894Z",
          "iopub.status.busy": "2021-09-30T06:49:32.581072Z",
          "iopub.status.idle": "2021-09-30T06:49:32.583700Z",
          "shell.execute_reply": "2021-09-30T06:49:32.584193Z",
          "shell.execute_reply.started": "2021-09-30T06:43:26.107706Z"
        },
        "papermill": {
          "duration": 0.077177,
          "end_time": "2021-09-30T06:49:32.584356",
          "exception": false,
          "start_time": "2021-09-30T06:49:32.507179",
          "status": "completed"
        },
        "tags": [],
        "id": "0ecc611d",
        "outputId": "9e82d983-1fc6-4bfd-bdbd-2ebb7676e6bd"
      },
      "source": [
        "Train2.shape , Test2.shape"
      ],
      "id": "0ecc611d",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 304), (35295, 303))"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:32.727572Z",
          "iopub.status.busy": "2021-09-30T06:49:32.726923Z",
          "iopub.status.idle": "2021-09-30T06:49:39.112039Z",
          "shell.execute_reply": "2021-09-30T06:49:39.111395Z",
          "shell.execute_reply.started": "2021-09-30T06:43:26.116123Z"
        },
        "papermill": {
          "duration": 6.45904,
          "end_time": "2021-09-30T06:49:39.112194",
          "exception": false,
          "start_time": "2021-09-30T06:49:32.653154",
          "status": "completed"
        },
        "tags": [],
        "id": "706dc857"
      },
      "source": [
        "Train3 = process(Train3)\n",
        "Test3 = process(Test3)"
      ],
      "id": "706dc857",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:39.256615Z",
          "iopub.status.busy": "2021-09-30T06:49:39.255819Z",
          "iopub.status.idle": "2021-09-30T06:49:39.259454Z",
          "shell.execute_reply": "2021-09-30T06:49:39.258943Z",
          "shell.execute_reply.started": "2021-09-30T06:43:32.542400Z"
        },
        "papermill": {
          "duration": 0.077987,
          "end_time": "2021-09-30T06:49:39.259607",
          "exception": false,
          "start_time": "2021-09-30T06:49:39.181620",
          "status": "completed"
        },
        "tags": [],
        "id": "63d037a5",
        "outputId": "b335007a-0047-4337-b9a2-6aeb733c9e31"
      },
      "source": [
        "Train3.shape , Test3.shape"
      ],
      "id": "63d037a5",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 304), (35295, 303))"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:39.402007Z",
          "iopub.status.busy": "2021-09-30T06:49:39.401207Z",
          "iopub.status.idle": "2021-09-30T06:49:45.818753Z",
          "shell.execute_reply": "2021-09-30T06:49:45.819282Z",
          "shell.execute_reply.started": "2021-09-30T06:43:32.549328Z"
        },
        "papermill": {
          "duration": 6.490539,
          "end_time": "2021-09-30T06:49:45.819455",
          "exception": false,
          "start_time": "2021-09-30T06:49:39.328916",
          "status": "completed"
        },
        "tags": [],
        "id": "1ecde2e6"
      },
      "source": [
        "Train4 = process(Train4)\n",
        "Test4 = process(Test4)"
      ],
      "id": "1ecde2e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:45.962696Z",
          "iopub.status.busy": "2021-09-30T06:49:45.961986Z",
          "iopub.status.idle": "2021-09-30T06:49:45.968148Z",
          "shell.execute_reply": "2021-09-30T06:49:45.967648Z",
          "shell.execute_reply.started": "2021-09-30T06:43:38.969605Z"
        },
        "papermill": {
          "duration": 0.078835,
          "end_time": "2021-09-30T06:49:45.968289",
          "exception": false,
          "start_time": "2021-09-30T06:49:45.889454",
          "status": "completed"
        },
        "tags": [],
        "id": "4c562a1c",
        "outputId": "67f43f6f-f849-4e85-c108-06b152898028"
      },
      "source": [
        "Train4.shape , Test4.shape"
      ],
      "id": "4c562a1c",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 304), (35295, 303))"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:46.113400Z",
          "iopub.status.busy": "2021-09-30T06:49:46.112761Z",
          "iopub.status.idle": "2021-09-30T06:49:46.713603Z",
          "shell.execute_reply": "2021-09-30T06:49:46.714154Z",
          "shell.execute_reply.started": "2021-09-30T06:43:38.976433Z"
        },
        "papermill": {
          "duration": 0.675105,
          "end_time": "2021-09-30T06:49:46.714326",
          "exception": false,
          "start_time": "2021-09-30T06:49:46.039221",
          "status": "completed"
        },
        "tags": [],
        "id": "fbf6ebc1",
        "outputId": "1a4c0071-2f98-4c7c-9966-3a1088eef83a"
      },
      "source": [
        "Train = pd.concat([Train,Train2.drop(columns=['field_id','label']),Train3.drop(columns=['field_id','label']),Train4.drop(columns=['field_id','label'])],axis=1)\n",
        "Train.shape"
      ],
      "id": "fbf6ebc1",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(87114, 1210)"
            ]
          },
          "execution_count": 32,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T06:49:46.861676Z",
          "iopub.status.busy": "2021-09-30T06:49:46.860976Z",
          "iopub.status.idle": "2021-09-30T06:49:47.684820Z",
          "shell.execute_reply": "2021-09-30T06:49:47.684282Z",
          "shell.execute_reply.started": "2021-09-30T06:43:39.655710Z"
        },
        "papermill": {
          "duration": 0.89734,
          "end_time": "2021-09-30T06:49:47.684967",
          "exception": false,
          "start_time": "2021-09-30T06:49:46.787627",
          "status": "completed"
        },
        "tags": [],
        "id": "fbb6d7e6",
        "outputId": "775a993d-c0ee-4733-9791-5a63b24773ed"
      },
      "source": [
        "Test = pd.concat([Test,Test2.drop(columns=['field_id']),Test3.drop(columns=['field_id'])],axis=1)\n",
        "Test = pd.merge(Test,Test4,on='field_id',how='left')\n",
        "Test.shape"
      ],
      "id": "fbb6d7e6",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(35295, 1209)"
            ]
          },
          "execution_count": 33,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "816ad2cc",
        "papermill": {
          "duration": 0.060202,
          "end_time": "2021-09-30T07:02:52.821567",
          "exception": false,
          "start_time": "2021-09-30T07:02:52.761365",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# MODELING"
      ],
      "id": "816ad2cc"
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:02:53.078262Z",
          "iopub.status.busy": "2021-09-30T07:02:53.077157Z",
          "iopub.status.idle": "2021-09-30T07:02:55.679513Z",
          "shell.execute_reply": "2021-09-30T07:02:55.679034Z",
          "shell.execute_reply.started": "2021-09-30T06:52:42.016165Z"
        },
        "id": "ecc02c82",
        "papermill": {
          "duration": 2.669336,
          "end_time": "2021-09-30T07:02:55.679664",
          "exception": false,
          "start_time": "2021-09-30T07:02:53.010328",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "X    = Train.fillna(-999).replace(np.inf,9999).drop(['field_id','label'], axis=1)\n",
        "y    = Train.label\n",
        "TEST = Test.fillna(-999).replace(np.inf,9999).drop(['field_id'], axis=1)"
      ],
      "id": "ecc02c82",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:02:55.805773Z",
          "iopub.status.busy": "2021-09-30T07:02:55.805160Z",
          "iopub.status.idle": "2021-09-30T07:02:55.808960Z",
          "shell.execute_reply": "2021-09-30T07:02:55.808551Z",
          "shell.execute_reply.started": "2021-09-30T06:52:44.541307Z"
        },
        "papermill": {
          "duration": 0.068232,
          "end_time": "2021-09-30T07:02:55.809077",
          "exception": false,
          "start_time": "2021-09-30T07:02:55.740845",
          "status": "completed"
        },
        "tags": [],
        "id": "8efa7604"
      },
      "source": [
        "TEST.columns = X.columns.tolist()"
      ],
      "id": "8efa7604",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:02:55.934676Z",
          "iopub.status.busy": "2021-09-30T07:02:55.933662Z",
          "iopub.status.idle": "2021-09-30T07:03:41.166328Z",
          "shell.execute_reply": "2021-09-30T07:03:41.165423Z",
          "shell.execute_reply.started": "2021-09-30T06:52:44.548106Z"
        },
        "papermill": {
          "duration": 45.297735,
          "end_time": "2021-09-30T07:03:41.166470",
          "exception": false,
          "start_time": "2021-09-30T07:02:55.868735",
          "status": "completed"
        },
        "tags": [],
        "id": "2d9d204f"
      },
      "source": [
        "data = pd.concat([X,TEST])\n",
        "qt=QuantileTransformer(output_distribution=\"normal\",random_state=42)\n",
        "data= pd.DataFrame(qt.fit_transform(data),columns=X.columns)"
      ],
      "id": "2d9d204f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:41.291641Z",
          "iopub.status.busy": "2021-09-30T07:03:41.289019Z",
          "iopub.status.idle": "2021-09-30T07:03:41.294978Z",
          "shell.execute_reply": "2021-09-30T07:03:41.294519Z",
          "shell.execute_reply.started": "2021-09-30T06:53:29.942970Z"
        },
        "papermill": {
          "duration": 0.068674,
          "end_time": "2021-09-30T07:03:41.295101",
          "exception": false,
          "start_time": "2021-09-30T07:03:41.226427",
          "status": "completed"
        },
        "tags": [],
        "id": "fe6247ef",
        "outputId": "f37e3af6-df1a-4ee7-f07c-906d502e0d6c"
      },
      "source": [
        "X.shape , TEST.shape"
      ],
      "id": "fe6247ef",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 1208), (35295, 1208))"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:41.667385Z",
          "iopub.status.busy": "2021-09-30T07:03:41.666632Z",
          "iopub.status.idle": "2021-09-30T07:03:41.668963Z",
          "shell.execute_reply": "2021-09-30T07:03:41.668555Z",
          "shell.execute_reply.started": "2021-09-30T06:53:29.964215Z"
        },
        "papermill": {
          "duration": 0.068399,
          "end_time": "2021-09-30T07:03:41.669076",
          "exception": false,
          "start_time": "2021-09-30T07:03:41.600677",
          "status": "completed"
        },
        "tags": [],
        "id": "627c9cb9"
      },
      "source": [
        "def attention_3d_block(inputs, name):\n",
        "  # inputs.shape = (batch_size, time_steps, input_dim)\n",
        "  TIME_STEPS = inputs.shape[1]\n",
        "  SINGLE_ATTENTION_VECTOR = False\n",
        "  input_dim = inputs.shape[2]\n",
        "  a = Permute((2, 1))(inputs)\n",
        "  a = Dense(TIME_STEPS, activation='softmax')(a)\n",
        "  if SINGLE_ATTENTION_VECTOR:  \n",
        "    a = Lambda(lambda x: K.mean(x, axis=1))(a)\n",
        "    a = RepeatVector(input_dim)(a)\n",
        "  a_probs = Permute((2, 1), name=name)(a)\n",
        "  output_attention_mul = Multiply()([inputs, a_probs])\n",
        "  return output_attention_mul"
      ],
      "id": "627c9cb9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:41.838081Z",
          "iopub.status.busy": "2021-09-30T07:03:41.834632Z",
          "iopub.status.idle": "2021-09-30T07:03:41.840059Z",
          "shell.execute_reply": "2021-09-30T07:03:41.840413Z",
          "shell.execute_reply.started": "2021-09-30T06:53:29.974192Z"
        },
        "papermill": {
          "duration": 0.072926,
          "end_time": "2021-09-30T07:03:41.840550",
          "exception": false,
          "start_time": "2021-09-30T07:03:41.767624",
          "status": "completed"
        },
        "tags": [],
        "id": "adc4af96"
      },
      "source": [
        "def get_model():\n",
        "  tf.random.set_seed(1)\n",
        "  np.random.seed(1)\n",
        "  random.seed(1)\n",
        "    \n",
        "  input_tensor = Input(shape=(1,X.shape[2]))\n",
        "  \n",
        "  x = BatchNormalization()(input_tensor)\n",
        "  x = Conv1D(256, 5,strides=5,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_1')\n",
        "  x = Dropout(0.1)(x)\n",
        "  \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv1D(128, 7,strides=3,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_2')\n",
        "  x = Dropout(0.15)(x)\n",
        "  \n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv1D(512, 9,strides=3,padding='same',activation='relu')(x)\n",
        "  x = attention_3d_block(x, 'attention_vec_3')\n",
        "  x = Dropout(0.15)(x)\n",
        "\n",
        "  x = GlobalMaxPooling1D()(x)\n",
        "\n",
        "  x = Dropout(0.3)(x)\n",
        "\n",
        "  out = Dense(9,kernel_initializer=glorot_normal(seed=1),\n",
        "                bias_initializer=glorot_normal(seed=1),\n",
        "                activation=\"softmax\")(x)\n",
        "\n",
        "  model = Model(inputs=input_tensor,outputs =out)\n",
        "\n",
        "  model.compile(optimizer='adam', loss='categorical_crossentropy')\n",
        "  return model"
      ],
      "id": "adc4af96",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:41.967280Z",
          "iopub.status.busy": "2021-09-30T07:03:41.966531Z",
          "iopub.status.idle": "2021-09-30T07:03:41.968924Z",
          "shell.execute_reply": "2021-09-30T07:03:41.968448Z",
          "shell.execute_reply.started": "2021-09-30T06:53:29.988320Z"
        },
        "papermill": {
          "duration": 0.068609,
          "end_time": "2021-09-30T07:03:41.969034",
          "exception": false,
          "start_time": "2021-09-30T07:03:41.900425",
          "status": "completed"
        },
        "tags": [],
        "id": "d6d6e485"
      },
      "source": [
        "X    = data[:X.shape[0]].values\n",
        "TEST = data[X.shape[0]:].values"
      ],
      "id": "d6d6e485",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.096349Z",
          "iopub.status.busy": "2021-09-30T07:03:42.095560Z",
          "iopub.status.idle": "2021-09-30T07:03:42.097498Z",
          "shell.execute_reply": "2021-09-30T07:03:42.097882Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.002497Z"
        },
        "papermill": {
          "duration": 0.069824,
          "end_time": "2021-09-30T07:03:42.098010",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.028186",
          "status": "completed"
        },
        "tags": [],
        "id": "6a8076f6"
      },
      "source": [
        "X    = X.reshape(X.shape[0], 1,X.shape[1])\n",
        "TEST = TEST.reshape(TEST.shape[0],1,TEST.shape[1])"
      ],
      "id": "6a8076f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.239422Z",
          "iopub.status.busy": "2021-09-30T07:03:42.238310Z",
          "iopub.status.idle": "2021-09-30T07:03:42.241493Z",
          "shell.execute_reply": "2021-09-30T07:03:42.241924Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.012225Z"
        },
        "papermill": {
          "duration": 0.076226,
          "end_time": "2021-09-30T07:03:42.242072",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.165846",
          "status": "completed"
        },
        "tags": [],
        "id": "0d63d7ff",
        "outputId": "7b5cc173-7c53-40ac-b2af-79379f75e829"
      },
      "source": [
        "X.shape , TEST.shape"
      ],
      "id": "0d63d7ff",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((87114, 1, 1208), (35295, 1, 1208))"
            ]
          },
          "execution_count": 45,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.371462Z",
          "iopub.status.busy": "2021-09-30T07:03:42.370727Z",
          "iopub.status.idle": "2021-09-30T07:03:42.373314Z",
          "shell.execute_reply": "2021-09-30T07:03:42.373698Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.023799Z"
        },
        "papermill": {
          "duration": 0.068622,
          "end_time": "2021-09-30T07:03:42.373855",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.305233",
          "status": "completed"
        },
        "tags": [],
        "id": "139c1d38"
      },
      "source": [
        "# Function to seed everything\n",
        "\n",
        "def seed_everything(seed):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    tf.random.set_seed(seed)"
      ],
      "id": "139c1d38",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.499267Z",
          "iopub.status.busy": "2021-09-30T07:03:42.498420Z",
          "iopub.status.idle": "2021-09-30T07:03:42.500269Z",
          "shell.execute_reply": "2021-09-30T07:03:42.500683Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.031757Z"
        },
        "papermill": {
          "duration": 0.066459,
          "end_time": "2021-09-30T07:03:42.500815",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.434356",
          "status": "completed"
        },
        "tags": [],
        "id": "a433dbc3"
      },
      "source": [
        "seed_everything(42)"
      ],
      "id": "a433dbc3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.755129Z",
          "iopub.status.busy": "2021-09-30T07:03:42.753404Z",
          "iopub.status.idle": "2021-09-30T07:03:42.763182Z",
          "shell.execute_reply": "2021-09-30T07:03:42.763540Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.050262Z"
        },
        "papermill": {
          "duration": 0.07475,
          "end_time": "2021-09-30T07:03:42.763684",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.688934",
          "status": "completed"
        },
        "tags": [],
        "id": "fe302d70"
      },
      "source": [
        "y_train = y.copy()\n",
        "n_labels = y.unique().shape[0]\n",
        "\n",
        "# we need to binarize the labels for the neural net\n",
        "LE = LabelEncoder()\n",
        "ytrain_enc = pd.get_dummies(y_train).values\n",
        "TARGETS = pd.get_dummies(y_train).columns\n",
        "\n",
        "y_oof = np.zeros([X.shape[0], n_labels])\n",
        "y_test = np.zeros([TEST.shape[0], n_labels])\n",
        "\n",
        "i = 0\n",
        "metrics = list()\n",
        "apply_aug = False"
      ],
      "id": "fe302d70",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T07:03:42.967900Z",
          "iopub.status.busy": "2021-09-30T07:03:42.967038Z",
          "iopub.status.idle": "2021-09-30T09:44:54.534319Z",
          "shell.execute_reply": "2021-09-30T09:44:54.533023Z",
          "shell.execute_reply.started": "2021-09-30T06:53:30.067849Z"
        },
        "papermill": {
          "duration": 9671.674415,
          "end_time": "2021-09-30T09:44:54.534480",
          "exception": false,
          "start_time": "2021-09-30T07:03:42.860065",
          "status": "completed"
        },
        "tags": [],
        "id": "7d757968",
        "outputId": "8ac5a901-421f-4a60-e2f4-8b0ff77f7f61"
      },
      "source": [
        "seed_everything(42)\n",
        "n_splits = 10\n",
        "kf = StratifiedKFold(n_splits=n_splits, random_state=47, shuffle=True)\n",
        "\n",
        "for idx , (tr_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "    # Verbosity\n",
        "    VERBOSE = 0\n",
        "    seed_everything(42)\n",
        "    early_stopping = tf.keras.callbacks.EarlyStopping(monitor = 'val_loss',\n",
        "                                                      mode = 'min',\n",
        "                                                      patience = 35,\n",
        "                                                      restore_best_weights = True,\n",
        "                                                      verbose = 1)\n",
        "    checkpoint = tf.keras.callbacks.ModelCheckpoint(f'Radiant_NN_{idx}.h5',\n",
        "                                                    monitor = 'val_loss',\n",
        "                                                    verbose = 1,\n",
        "                                                    save_best_only = True,\n",
        "                                                    save_weights_only = True)\n",
        "\n",
        "    X_tr, X_vl = X[tr_idx, :], X[val_idx, :]\n",
        "    y_tr, y_vl = ytrain_enc[tr_idx], ytrain_enc[val_idx]\n",
        "    y_train_, y_vld_ = y[tr_idx], y[val_idx]\n",
        "\n",
        "    # 1 CNN ANN\n",
        "    seed_everything(seed=1)\n",
        "    model = get_model()\n",
        "    model.fit( \n",
        "        X_tr,\n",
        "        y_tr,\n",
        "        validation_data =(X_vl, y_vl),\n",
        "        callbacks = [early_stopping,  checkpoint],\n",
        "        epochs = 300,\n",
        "        batch_size = 64)\n",
        "    \n",
        "    \n",
        "    \n",
        "    \n",
        "    model = get_model()\n",
        "    model.load_weights(f'Radiant_NN_{idx}.h5')\n",
        "    y_pred_cnn_nn = model.predict(X_vl)\n",
        "    test_pred_cnn_nn = model.predict(TEST, batch_size= 1024)\n",
        "    \n",
        "    \n",
        "    y_oof[val_idx, :] = y_pred_cnn_nn\n",
        "    y_tr, y_vl = y_train.iloc[tr_idx], y_train.iloc[val_idx]\n",
        "    metric = log_loss(y_vl, y_pred_cnn_nn)\n",
        "    print(\"fold #{} Log Loss: {}\".format(i, metric))\n",
        "    \n",
        "    i += 1\n",
        "    test_pred = test_pred_cnn_nn \n",
        "    y_test += test_pred / n_splits\n",
        "    metrics.append(metric)\n",
        "    \n",
        "metrics = np.array(metrics).mean()\n",
        "print(f'Full Log loss {metrics}') "
      ],
      "id": "7d757968",
      "execution_count": null,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-30 07:03:43.702675: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-30 07:03:43.706123: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcuda.so.1\n",
            "2021-09-30 07:03:43.746692: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.747323: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-30 07:03:43.747387: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-30 07:03:43.773792: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-30 07:03:43.773897: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-30 07:03:43.792000: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-30 07:03:43.800286: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-30 07:03:43.824447: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-30 07:03:43.832221: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-30 07:03:43.834998: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-30 07:03:43.835189: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.835890: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.837381: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-30 07:03:43.837836: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2021-09-30 07:03:43.838045: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
            "2021-09-30 07:03:43.838215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.838797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla P100-PCIE-16GB computeCapability: 6.0\n",
            "coreClock: 1.3285GHz coreCount: 56 deviceMemorySize: 15.90GiB deviceMemoryBandwidth: 681.88GiB/s\n",
            "2021-09-30 07:03:43.838879: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-30 07:03:43.838915: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-30 07:03:43.838939: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-30 07:03:43.838962: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcufft.so.10\n",
            "2021-09-30 07:03:43.838985: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcurand.so.10\n",
            "2021-09-30 07:03:43.839007: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-09-30 07:03:43.839031: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-09-30 07:03:43.839055: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-09-30 07:03:43.839133: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.839766: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:43.840318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0\n",
            "2021-09-30 07:03:43.840942: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-09-30 07:03:45.200186: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-09-30 07:03:45.200232: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 \n",
            "2021-09-30 07:03:45.200243: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N \n",
            "2021-09-30 07:03:45.202638: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:45.203343: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:45.204014: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:941] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-09-30 07:03:45.204570: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14957 MB memory) -> physical GPU (device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0)\n",
            "2021-09-30 07:03:46.162094: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)\n",
            "2021-09-30 07:03:46.172353: I tensorflow/core/platform/profile_utils/cpu_utils.cc:112] CPU Frequency: 2000185000 Hz\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/300\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2021-09-30 07:03:47.554761: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublas.so.11\n",
            "2021-09-30 07:03:48.303978: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-09-30 07:03:48.338724: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudnn.so.8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1226/1226 [==============================] - 18s 9ms/step - loss: 1.1695 - val_loss: 0.9351\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93512, saving model to Radiant_NN_0.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9467 - val_loss: 0.8845\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93512 to 0.88454, saving model to Radiant_NN_0.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8853 - val_loss: 0.8466\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.88454 to 0.84662, saving model to Radiant_NN_0.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.8542 - val_loss: 0.8198\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84662 to 0.81984, saving model to Radiant_NN_0.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8251 - val_loss: 0.8121\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81984 to 0.81215, saving model to Radiant_NN_0.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8010 - val_loss: 0.7872\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.81215 to 0.78720, saving model to Radiant_NN_0.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7802 - val_loss: 0.7834\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78720 to 0.78339, saving model to Radiant_NN_0.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7608 - val_loss: 0.7700\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.78339 to 0.77001, saving model to Radiant_NN_0.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7396 - val_loss: 0.7667\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77001 to 0.76666, saving model to Radiant_NN_0.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7333 - val_loss: 0.7546\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.76666 to 0.75461, saving model to Radiant_NN_0.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7211 - val_loss: 0.7559\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.75461\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7039 - val_loss: 0.7518\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.75461 to 0.75180, saving model to Radiant_NN_0.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6987 - val_loss: 0.7493\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.75180 to 0.74932, saving model to Radiant_NN_0.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6792 - val_loss: 0.7378\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.74932 to 0.73781, saving model to Radiant_NN_0.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6716 - val_loss: 0.7323\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.73781 to 0.73225, saving model to Radiant_NN_0.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6662 - val_loss: 0.7304\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.73225 to 0.73042, saving model to Radiant_NN_0.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6528 - val_loss: 0.7307\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.73042\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6404 - val_loss: 0.7196\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.73042 to 0.71959, saving model to Radiant_NN_0.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6345 - val_loss: 0.7226\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.71959\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6298 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.71959\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6193 - val_loss: 0.7082\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.71959 to 0.70818, saving model to Radiant_NN_0.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6140 - val_loss: 0.7122\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70818\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5996 - val_loss: 0.7059\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.70818 to 0.70585, saving model to Radiant_NN_0.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5992 - val_loss: 0.7113\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.70585\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5883 - val_loss: 0.7050\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.70585 to 0.70504, saving model to Radiant_NN_0.h5\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5845 - val_loss: 0.7123\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.70504\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5829 - val_loss: 0.7050\n",
            "\n",
            "Epoch 00027: val_loss improved from 0.70504 to 0.70501, saving model to Radiant_NN_0.h5\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5781 - val_loss: 0.7066\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.70501\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5717 - val_loss: 0.7015\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.70501 to 0.70146, saving model to Radiant_NN_0.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5667 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.70146 to 0.69740, saving model to Radiant_NN_0.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5589 - val_loss: 0.6910\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.69740 to 0.69096, saving model to Radiant_NN_0.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5574 - val_loss: 0.7020\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69096\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5463 - val_loss: 0.7133\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69096\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5431 - val_loss: 0.6916\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69096\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5406 - val_loss: 0.6907\n",
            "\n",
            "Epoch 00035: val_loss improved from 0.69096 to 0.69069, saving model to Radiant_NN_0.h5\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5229 - val_loss: 0.7013\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69069\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5265 - val_loss: 0.7044\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69069\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5266 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69069\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5218 - val_loss: 0.6959\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69069\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5140 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69069\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5125 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69069\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5153 - val_loss: 0.6852\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.69069 to 0.68519, saving model to Radiant_NN_0.h5\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5059 - val_loss: 0.6994\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68519\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5014 - val_loss: 0.6899\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68519\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5067 - val_loss: 0.7044\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68519\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4925 - val_loss: 0.6906\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68519\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4975 - val_loss: 0.7135\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68519\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4941 - val_loss: 0.6895\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68519\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4868 - val_loss: 0.7004\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68519\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4851 - val_loss: 0.6892\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68519\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4815 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68519\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4846 - val_loss: 0.7155\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68519\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4738 - val_loss: 0.6872\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68519\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4807 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68519\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4691 - val_loss: 0.6796\n",
            "\n",
            "Epoch 00055: val_loss improved from 0.68519 to 0.67956, saving model to Radiant_NN_0.h5\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4699 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67956\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4621 - val_loss: 0.6944\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67956\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4607 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67956\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4611 - val_loss: 0.7013\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67956\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4629 - val_loss: 0.7027\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67956\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4552 - val_loss: 0.6894\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67956\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4599 - val_loss: 0.6841\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67956\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4548 - val_loss: 0.6997\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67956\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4510 - val_loss: 0.6861\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67956\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4496 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67956\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4503 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67956\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4526 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67956\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4435 - val_loss: 0.7232\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67956\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4414 - val_loss: 0.7058\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67956\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4439 - val_loss: 0.7143\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67956\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4395 - val_loss: 0.6889\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67956\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4386 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67956\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4386 - val_loss: 0.7259\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67956\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4323 - val_loss: 0.7177\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67956\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4273 - val_loss: 0.7059\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67956\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4276 - val_loss: 0.7142\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67956\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4307 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67956\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4257 - val_loss: 0.7095\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67956\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4114 - val_loss: 0.7115\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67956\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4205 - val_loss: 0.6861\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67956\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4264 - val_loss: 0.7163\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67956\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4214 - val_loss: 0.6998\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67956\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4122 - val_loss: 0.7137\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67956\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4133 - val_loss: 0.7172\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67956\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4149 - val_loss: 0.6991\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67956\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4188 - val_loss: 0.7226\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67956\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4106 - val_loss: 0.7083\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67956\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4159 - val_loss: 0.7162\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67956\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4073 - val_loss: 0.7061\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67956\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4094 - val_loss: 0.7160\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67956\n",
            "Epoch 00090: early stopping\n",
            "fold #0 Log Loss: 0.679555199094739\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 14s 10ms/step - loss: 1.1720 - val_loss: 0.9232\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92323, saving model to Radiant_NN_1.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9460 - val_loss: 0.8553\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92323 to 0.85528, saving model to Radiant_NN_1.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8943 - val_loss: 0.8278\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85528 to 0.82784, saving model to Radiant_NN_1.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8529 - val_loss: 0.8022\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82784 to 0.80221, saving model to Radiant_NN_1.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8221 - val_loss: 0.7941\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80221 to 0.79405, saving model to Radiant_NN_1.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8052 - val_loss: 0.7731\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79405 to 0.77306, saving model to Radiant_NN_1.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7841 - val_loss: 0.7682\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77306 to 0.76817, saving model to Radiant_NN_1.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.7681 - val_loss: 0.7500\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.76817 to 0.75001, saving model to Radiant_NN_1.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7420 - val_loss: 0.7404\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75001 to 0.74041, saving model to Radiant_NN_1.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7282 - val_loss: 0.7362\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74041 to 0.73621, saving model to Radiant_NN_1.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7202 - val_loss: 0.7338\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73621 to 0.73377, saving model to Radiant_NN_1.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7087 - val_loss: 0.7283\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73377 to 0.72830, saving model to Radiant_NN_1.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6922 - val_loss: 0.7131\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72830 to 0.71312, saving model to Radiant_NN_1.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.6779 - val_loss: 0.7180\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.71312\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6706 - val_loss: 0.7052\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71312 to 0.70519, saving model to Radiant_NN_1.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6689 - val_loss: 0.7126\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.70519\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6491 - val_loss: 0.7174\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70519\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6394 - val_loss: 0.6998\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.70519 to 0.69982, saving model to Radiant_NN_1.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6357 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69982 to 0.69464, saving model to Radiant_NN_1.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6261 - val_loss: 0.7056\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69464\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6208 - val_loss: 0.7035\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69464\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6100 - val_loss: 0.6894\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.69464 to 0.68941, saving model to Radiant_NN_1.h5\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6030 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68941\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5941 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.68941 to 0.68751, saving model to Radiant_NN_1.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5908 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68751\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5776 - val_loss: 0.6880\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68751\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5834 - val_loss: 0.6951\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68751\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5742 - val_loss: 0.6876\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68751\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5660 - val_loss: 0.6864\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.68751 to 0.68641, saving model to Radiant_NN_1.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5635 - val_loss: 0.6825\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.68641 to 0.68253, saving model to Radiant_NN_1.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5620 - val_loss: 0.6780\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.68253 to 0.67797, saving model to Radiant_NN_1.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5468 - val_loss: 0.6849\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67797\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5446 - val_loss: 0.6803\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67797\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5455 - val_loss: 0.6805\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67797\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5500 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67797\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5289 - val_loss: 0.6827\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67797\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5355 - val_loss: 0.6926\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67797\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5269 - val_loss: 0.6823\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67797\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5196 - val_loss: 0.6787\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67797\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5248 - val_loss: 0.6797\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67797\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5102 - val_loss: 0.6880\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67797\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5118 - val_loss: 0.6828\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67797\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5072 - val_loss: 0.6840\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67797\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5028 - val_loss: 0.6753\n",
            "\n",
            "Epoch 00044: val_loss improved from 0.67797 to 0.67532, saving model to Radiant_NN_1.h5\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4993 - val_loss: 0.6819\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67532\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4990 - val_loss: 0.6879\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67532\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4971 - val_loss: 0.6731\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.67532 to 0.67307, saving model to Radiant_NN_1.h5\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4886 - val_loss: 0.6749\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67307\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4842 - val_loss: 0.6721\n",
            "\n",
            "Epoch 00049: val_loss improved from 0.67307 to 0.67212, saving model to Radiant_NN_1.h5\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4859 - val_loss: 0.6855\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67212\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4843 - val_loss: 0.6704\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.67212 to 0.67036, saving model to Radiant_NN_1.h5\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4845 - val_loss: 0.6829\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67036\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4781 - val_loss: 0.6889\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67036\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4738 - val_loss: 0.7060\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67036\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4661 - val_loss: 0.6755\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67036\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4700 - val_loss: 0.6736\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67036\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4732 - val_loss: 0.6818\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67036\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4658 - val_loss: 0.6871\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67036\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4645 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67036\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4581 - val_loss: 0.6828\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67036\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4579 - val_loss: 0.6789\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67036\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4564 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67036\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4568 - val_loss: 0.6987\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67036\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4482 - val_loss: 0.6819\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67036\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4493 - val_loss: 0.6858\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67036\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4454 - val_loss: 0.6961\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67036\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4447 - val_loss: 0.6780\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67036\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4375 - val_loss: 0.6843\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67036\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4457 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67036\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4395 - val_loss: 0.6680\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.67036 to 0.66803, saving model to Radiant_NN_1.h5\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4409 - val_loss: 0.6829\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66803\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4405 - val_loss: 0.6921\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66803\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4264 - val_loss: 0.6860\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.66803\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4301 - val_loss: 0.6790\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.66803\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4262 - val_loss: 0.6766\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.66803\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4312 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.66803\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4258 - val_loss: 0.7053\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.66803\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4190 - val_loss: 0.6954\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.66803\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4203 - val_loss: 0.7186\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.66803\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4233 - val_loss: 0.7181\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.66803\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4152 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.66803\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4160 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.66803\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4169 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.66803\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4224 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.66803\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4191 - val_loss: 0.7165\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.66803\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4213 - val_loss: 0.7240\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.66803\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4058 - val_loss: 0.6864\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.66803\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4076 - val_loss: 0.7210\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.66803\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4079 - val_loss: 0.7078\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.66803\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4100 - val_loss: 0.6838\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.66803\n",
            "Epoch 91/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4065 - val_loss: 0.6773\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.66803\n",
            "Epoch 92/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4042 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.66803\n",
            "Epoch 93/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4016 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.66803\n",
            "Epoch 94/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3990 - val_loss: 0.7138\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.66803\n",
            "Epoch 95/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3949 - val_loss: 0.7234\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.66803\n",
            "Epoch 96/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4087 - val_loss: 0.6936\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.66803\n",
            "Epoch 97/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3986 - val_loss: 0.7157\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.66803\n",
            "Epoch 98/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3967 - val_loss: 0.7127\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.66803\n",
            "Epoch 99/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3985 - val_loss: 0.6836\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.66803\n",
            "Epoch 100/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3957 - val_loss: 0.7052\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.66803\n",
            "Epoch 101/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3904 - val_loss: 0.7227\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.66803\n",
            "Epoch 102/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3974 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.66803\n",
            "Epoch 103/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3919 - val_loss: 0.7129\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.66803\n",
            "Epoch 104/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3869 - val_loss: 0.6935\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.66803\n",
            "Epoch 105/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.3928 - val_loss: 0.7040\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.66803\n",
            "Epoch 00105: early stopping\n",
            "fold #1 Log Loss: 0.6680302569517913\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 13s 9ms/step - loss: 1.1711 - val_loss: 0.9157\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91572, saving model to Radiant_NN_2.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9568 - val_loss: 0.8530\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91572 to 0.85304, saving model to Radiant_NN_2.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8897 - val_loss: 0.8232\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85304 to 0.82317, saving model to Radiant_NN_2.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8547 - val_loss: 0.8058\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82317 to 0.80584, saving model to Radiant_NN_2.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8329 - val_loss: 0.7776\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80584 to 0.77759, saving model to Radiant_NN_2.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8075 - val_loss: 0.7666\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.77759 to 0.76661, saving model to Radiant_NN_2.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7846 - val_loss: 0.7590\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.76661 to 0.75902, saving model to Radiant_NN_2.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7571 - val_loss: 0.7463\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.75902 to 0.74625, saving model to Radiant_NN_2.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7371 - val_loss: 0.7451\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.74625 to 0.74506, saving model to Radiant_NN_2.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7339 - val_loss: 0.7344\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74506 to 0.73444, saving model to Radiant_NN_2.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7270 - val_loss: 0.7250\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73444 to 0.72500, saving model to Radiant_NN_2.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7018 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.72500 to 0.72325, saving model to Radiant_NN_2.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6895 - val_loss: 0.7013\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.72325 to 0.70134, saving model to Radiant_NN_2.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6792 - val_loss: 0.7089\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 0.70134\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6737 - val_loss: 0.7061\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.70134\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6594 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.70134 to 0.69772, saving model to Radiant_NN_2.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6476 - val_loss: 0.7044\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.69772\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6396 - val_loss: 0.7023\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.69772\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6423 - val_loss: 0.6855\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.69772 to 0.68552, saving model to Radiant_NN_2.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6316 - val_loss: 0.6960\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.68552\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6203 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.68552\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6067 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.68552 to 0.68306, saving model to Radiant_NN_2.h5\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6024 - val_loss: 0.6866\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.68306\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5972 - val_loss: 0.6853\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.68306\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5878 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68306\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5921 - val_loss: 0.6746\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.68306 to 0.67458, saving model to Radiant_NN_2.h5\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5773 - val_loss: 0.6904\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.67458\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5787 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.67458\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5663 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67458\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5568 - val_loss: 0.6774\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67458\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5522 - val_loss: 0.6762\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.67458\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5461 - val_loss: 0.6725\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.67458 to 0.67252, saving model to Radiant_NN_2.h5\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5490 - val_loss: 0.6819\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67252\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5422 - val_loss: 0.6806\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67252\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5426 - val_loss: 0.6791\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67252\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5326 - val_loss: 0.6739\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67252\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5387 - val_loss: 0.6847\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67252\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5232 - val_loss: 0.6776\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67252\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5181 - val_loss: 0.6662\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.67252 to 0.66621, saving model to Radiant_NN_2.h5\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5240 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.66621\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5166 - val_loss: 0.6735\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.66621\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5035 - val_loss: 0.6717\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.66621\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5092 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.66621\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5135 - val_loss: 0.6703\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.66621\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5021 - val_loss: 0.6714\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.66621\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4997 - val_loss: 0.6729\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.66621\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4913 - val_loss: 0.6823\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.66621\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4958 - val_loss: 0.6698\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.66621\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4888 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.66621\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4876 - val_loss: 0.6775\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.66621\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4856 - val_loss: 0.6663\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.66621\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4797 - val_loss: 0.6775\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.66621\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4739 - val_loss: 0.6781\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.66621\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4755 - val_loss: 0.6710\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.66621\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4702 - val_loss: 0.6745\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.66621\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4682 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.66621\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4638 - val_loss: 0.6771\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.66621\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4738 - val_loss: 0.6765\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.66621\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4542 - val_loss: 0.6779\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.66621\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4611 - val_loss: 0.6793\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.66621\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4626 - val_loss: 0.6730\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.66621\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4572 - val_loss: 0.6844\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.66621\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4524 - val_loss: 0.6879\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.66621\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.4486 - val_loss: 0.6683\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.66621\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4533 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.66621\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4502 - val_loss: 0.6822\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.66621\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4434 - val_loss: 0.6923\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.66621\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4439 - val_loss: 0.6695\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.66621\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4415 - val_loss: 0.6882\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.66621\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4359 - val_loss: 0.6785\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.66621\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4388 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.66621\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4382 - val_loss: 0.6749\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.66621\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4332 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.66621\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4378 - val_loss: 0.6925\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.66621\n",
            "Epoch 00074: early stopping\n",
            "fold #2 Log Loss: 0.666209671814525\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1763 - val_loss: 0.9219\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92188, saving model to Radiant_NN_3.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.9496 - val_loss: 0.8666\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92188 to 0.86659, saving model to Radiant_NN_3.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8922 - val_loss: 0.8414\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86659 to 0.84140, saving model to Radiant_NN_3.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8597 - val_loss: 0.8309\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84140 to 0.83087, saving model to Radiant_NN_3.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.8279 - val_loss: 0.7990\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.83087 to 0.79898, saving model to Radiant_NN_3.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8103 - val_loss: 0.7861\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79898 to 0.78613, saving model to Radiant_NN_3.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7744 - val_loss: 0.7885\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 0.78613\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.7644 - val_loss: 0.7707\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.78613 to 0.77066, saving model to Radiant_NN_3.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7550 - val_loss: 0.7547\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77066 to 0.75473, saving model to Radiant_NN_3.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7336 - val_loss: 0.7631\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.75473\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7134 - val_loss: 0.7532\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.75473 to 0.75319, saving model to Radiant_NN_3.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.7116 - val_loss: 0.7466\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.75319 to 0.74658, saving model to Radiant_NN_3.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6941 - val_loss: 0.7427\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.74658 to 0.74269, saving model to Radiant_NN_3.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6827 - val_loss: 0.7267\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.74269 to 0.72675, saving model to Radiant_NN_3.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6774 - val_loss: 0.7306\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.72675\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6580 - val_loss: 0.7225\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.72675 to 0.72249, saving model to Radiant_NN_3.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6599 - val_loss: 0.7188\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.72249 to 0.71878, saving model to Radiant_NN_3.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6516 - val_loss: 0.7308\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.71878\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6376 - val_loss: 0.7182\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.71878 to 0.71824, saving model to Radiant_NN_3.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6263 - val_loss: 0.7214\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.71824\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6216 - val_loss: 0.7191\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.71824\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6096 - val_loss: 0.7170\n",
            "\n",
            "Epoch 00022: val_loss improved from 0.71824 to 0.71701, saving model to Radiant_NN_3.h5\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6083 - val_loss: 0.7080\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.71701 to 0.70797, saving model to Radiant_NN_3.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6040 - val_loss: 0.7166\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.70797\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5878 - val_loss: 0.7162\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.70797\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5848 - val_loss: 0.7053\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.70797 to 0.70531, saving model to Radiant_NN_3.h5\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5806 - val_loss: 0.7142\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.70531\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5687 - val_loss: 0.7123\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.70531\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.5707 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.70531 to 0.70311, saving model to Radiant_NN_3.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5619 - val_loss: 0.6981\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.70311 to 0.69809, saving model to Radiant_NN_3.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5536 - val_loss: 0.7159\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.69809\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5580 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69809\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5537 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69809\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5401 - val_loss: 0.7067\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69809\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5373 - val_loss: 0.7016\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69809\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5361 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69809\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5355 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.69809 to 0.69565, saving model to Radiant_NN_3.h5\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5253 - val_loss: 0.6963\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69565\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5305 - val_loss: 0.7002\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69565\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5224 - val_loss: 0.6983\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69565\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5173 - val_loss: 0.6970\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.69565\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5110 - val_loss: 0.6951\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.69565 to 0.69506, saving model to Radiant_NN_3.h5\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4999 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.69506\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5080 - val_loss: 0.6981\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.69506\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5024 - val_loss: 0.7181\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.69506\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4931 - val_loss: 0.6968\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.69506\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4990 - val_loss: 0.7042\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.69506\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4971 - val_loss: 0.7175\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.69506\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4938 - val_loss: 0.7041\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.69506\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4880 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.69506\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4811 - val_loss: 0.7072\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.69506\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4844 - val_loss: 0.6991\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.69506\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4828 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.69506\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4734 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.69506\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4749 - val_loss: 0.7099\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.69506\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4702 - val_loss: 0.7092\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.69506\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4713 - val_loss: 0.7316\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.69506\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4651 - val_loss: 0.7144\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.69506\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4659 - val_loss: 0.7154\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.69506\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4563 - val_loss: 0.7079\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.69506\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4622 - val_loss: 0.7104\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.69506\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4577 - val_loss: 0.7127\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.69506\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4560 - val_loss: 0.7121\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.69506\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4498 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00064: val_loss improved from 0.69506 to 0.69009, saving model to Radiant_NN_3.h5\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4469 - val_loss: 0.6998\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.69009\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4363 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.69009\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4484 - val_loss: 0.7127\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.69009\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4462 - val_loss: 0.7162\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.69009\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4480 - val_loss: 0.7203\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.69009\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4368 - val_loss: 0.6889\n",
            "\n",
            "Epoch 00070: val_loss improved from 0.69009 to 0.68889, saving model to Radiant_NN_3.h5\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4429 - val_loss: 0.7183\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.68889\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4403 - val_loss: 0.7194\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.68889\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4324 - val_loss: 0.7026\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.68889\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4332 - val_loss: 0.7417\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.68889\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4294 - val_loss: 0.7026\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.68889\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4271 - val_loss: 0.6949\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.68889\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4227 - val_loss: 0.7124\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.68889\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4183 - val_loss: 0.7303\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.68889\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4271 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.68889\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4216 - val_loss: 0.7035\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.68889\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4171 - val_loss: 0.7176\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.68889\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4265 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.68889\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4138 - val_loss: 0.7158\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.68889\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4220 - val_loss: 0.7265\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.68889\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4147 - val_loss: 0.7161\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.68889\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4113 - val_loss: 0.7145\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.68889\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4141 - val_loss: 0.7259\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.68889\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4025 - val_loss: 0.7163\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.68889\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4074 - val_loss: 0.7176\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.68889\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4132 - val_loss: 0.7170\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.68889\n",
            "Epoch 91/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4082 - val_loss: 0.7401\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.68889\n",
            "Epoch 92/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.4054 - val_loss: 0.7220\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.68889\n",
            "Epoch 93/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4084 - val_loss: 0.7178\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.68889\n",
            "Epoch 94/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4009 - val_loss: 0.7264\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.68889\n",
            "Epoch 95/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4033 - val_loss: 0.7361\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.68889\n",
            "Epoch 96/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3980 - val_loss: 0.7415\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.68889\n",
            "Epoch 97/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4071 - val_loss: 0.7313\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.68889\n",
            "Epoch 98/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3969 - val_loss: 0.7401\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.68889\n",
            "Epoch 99/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3909 - val_loss: 0.7150\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.68889\n",
            "Epoch 100/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3977 - val_loss: 0.7192\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.68889\n",
            "Epoch 101/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3896 - val_loss: 0.7338\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.68889\n",
            "Epoch 102/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3997 - val_loss: 0.7221\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.68889\n",
            "Epoch 103/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3922 - val_loss: 0.7171\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.68889\n",
            "Epoch 104/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3863 - val_loss: 0.7198\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.68889\n",
            "Epoch 105/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3903 - val_loss: 0.7239\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00105: val_loss did not improve from 0.68889\n",
            "Epoch 00105: early stopping\n",
            "fold #3 Log Loss: 0.6888934618258941\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1755 - val_loss: 0.9361\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93607, saving model to Radiant_NN_4.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.9436 - val_loss: 0.8679\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93607 to 0.86786, saving model to Radiant_NN_4.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8973 - val_loss: 0.8476\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86786 to 0.84762, saving model to Radiant_NN_4.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8651 - val_loss: 0.8270\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84762 to 0.82700, saving model to Radiant_NN_4.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8371 - val_loss: 0.8144\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.82700 to 0.81445, saving model to Radiant_NN_4.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.7978 - val_loss: 0.7875\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.81445 to 0.78748, saving model to Radiant_NN_4.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7811 - val_loss: 0.7792\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78748 to 0.77915, saving model to Radiant_NN_4.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7697 - val_loss: 0.7810\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.77915\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7518 - val_loss: 0.7682\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77915 to 0.76822, saving model to Radiant_NN_4.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7311 - val_loss: 0.7620\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.76822 to 0.76197, saving model to Radiant_NN_4.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7254 - val_loss: 0.7474\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.76197 to 0.74735, saving model to Radiant_NN_4.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6990 - val_loss: 0.7380\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.74735 to 0.73802, saving model to Radiant_NN_4.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6948 - val_loss: 0.7403\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.73802\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6907 - val_loss: 0.7367\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.73802 to 0.73666, saving model to Radiant_NN_4.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6702 - val_loss: 0.7165\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.73666 to 0.71652, saving model to Radiant_NN_4.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6616 - val_loss: 0.7269\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.71652\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6502 - val_loss: 0.7146\n",
            "\n",
            "Epoch 00017: val_loss improved from 0.71652 to 0.71455, saving model to Radiant_NN_4.h5\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6516 - val_loss: 0.7264\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.71455\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6413 - val_loss: 0.7178\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.71455\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6309 - val_loss: 0.7094\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.71455 to 0.70941, saving model to Radiant_NN_4.h5\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6213 - val_loss: 0.7090\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70941 to 0.70904, saving model to Radiant_NN_4.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6109 - val_loss: 0.7105\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70904\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6102 - val_loss: 0.7053\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.70904 to 0.70534, saving model to Radiant_NN_4.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6054 - val_loss: 0.7141\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 0.70534\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5980 - val_loss: 0.7139\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.70534\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5901 - val_loss: 0.7057\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.70534\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5918 - val_loss: 0.7058\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.70534\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5769 - val_loss: 0.7120\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.70534\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5633 - val_loss: 0.7003\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.70534 to 0.70028, saving model to Radiant_NN_4.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5613 - val_loss: 0.7072\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.70028\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5609 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.70028\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5546 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.70028 to 0.69728, saving model to Radiant_NN_4.h5\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5485 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69728\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5505 - val_loss: 0.6961\n",
            "\n",
            "Epoch 00034: val_loss improved from 0.69728 to 0.69615, saving model to Radiant_NN_4.h5\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5442 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69615\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5422 - val_loss: 0.7033\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.69615\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5393 - val_loss: 0.6997\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.69615\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.5228 - val_loss: 0.7022\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.69615\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5245 - val_loss: 0.7029\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.69615\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5183 - val_loss: 0.6996\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.69615\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5112 - val_loss: 0.6797\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.69615 to 0.67975, saving model to Radiant_NN_4.h5\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5322 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67975\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5118 - val_loss: 0.6817\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67975\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4977 - val_loss: 0.6950\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67975\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5061 - val_loss: 0.7092\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67975\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4997 - val_loss: 0.6992\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67975\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4950 - val_loss: 0.6924\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67975\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4961 - val_loss: 0.6924\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67975\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4906 - val_loss: 0.6928\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67975\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4951 - val_loss: 0.7024\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67975\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4902 - val_loss: 0.6927\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67975\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4853 - val_loss: 0.7046\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67975\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4911 - val_loss: 0.6956\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67975\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4713 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67975\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4809 - val_loss: 0.6942\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67975\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4756 - val_loss: 0.7148\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67975\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4721 - val_loss: 0.7151\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67975\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4669 - val_loss: 0.7089\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67975\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4602 - val_loss: 0.7124\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67975\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4577 - val_loss: 0.7190\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67975\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4595 - val_loss: 0.6974\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67975\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4644 - val_loss: 0.7176\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67975\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4632 - val_loss: 0.6994\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67975\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4541 - val_loss: 0.6892\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67975\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4551 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67975\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4521 - val_loss: 0.6952\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67975\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.4487 - val_loss: 0.7083\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67975\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4447 - val_loss: 0.7020\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67975\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4486 - val_loss: 0.7166\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67975\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4528 - val_loss: 0.7002\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67975\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4401 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67975\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4492 - val_loss: 0.7077\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67975\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4413 - val_loss: 0.7007\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67975\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4387 - val_loss: 0.7303\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67975\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4287 - val_loss: 0.7163\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67975\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4344 - val_loss: 0.6981\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67975\n",
            "Epoch 00076: early stopping\n",
            "fold #4 Log Loss: 0.6797473387222485\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1651 - val_loss: 0.9170\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91699, saving model to Radiant_NN_5.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.9489 - val_loss: 0.8641\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91699 to 0.86409, saving model to Radiant_NN_5.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8856 - val_loss: 0.8485\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86409 to 0.84854, saving model to Radiant_NN_5.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8596 - val_loss: 0.8095\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.84854 to 0.80953, saving model to Radiant_NN_5.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8281 - val_loss: 0.8026\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80953 to 0.80264, saving model to Radiant_NN_5.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8013 - val_loss: 0.7791\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.80264 to 0.77910, saving model to Radiant_NN_5.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7915 - val_loss: 0.7615\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.77910 to 0.76148, saving model to Radiant_NN_5.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7634 - val_loss: 0.7637\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.76148\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7413 - val_loss: 0.7384\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.76148 to 0.73844, saving model to Radiant_NN_5.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7235 - val_loss: 0.7427\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.73844\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7197 - val_loss: 0.7328\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73844 to 0.73282, saving model to Radiant_NN_5.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7025 - val_loss: 0.7342\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.73282\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6942 - val_loss: 0.7259\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.73282 to 0.72591, saving model to Radiant_NN_5.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6857 - val_loss: 0.7226\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72591 to 0.72260, saving model to Radiant_NN_5.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6713 - val_loss: 0.7189\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.72260 to 0.71887, saving model to Radiant_NN_5.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6616 - val_loss: 0.7079\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.71887 to 0.70788, saving model to Radiant_NN_5.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6535 - val_loss: 0.7179\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70788\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6366 - val_loss: 0.7071\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.70788 to 0.70713, saving model to Radiant_NN_5.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6305 - val_loss: 0.7083\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70713\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6217 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00020: val_loss improved from 0.70713 to 0.69782, saving model to Radiant_NN_5.h5\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6152 - val_loss: 0.7016\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.69782\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6016 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69782\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6037 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.69782 to 0.69007, saving model to Radiant_NN_5.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5967 - val_loss: 0.6885\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69007 to 0.68854, saving model to Radiant_NN_5.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5806 - val_loss: 0.6884\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.68854 to 0.68839, saving model to Radiant_NN_5.h5\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5772 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68839\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5704 - val_loss: 0.6889\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68839\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5658 - val_loss: 0.6790\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.68839 to 0.67900, saving model to Radiant_NN_5.h5\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5683 - val_loss: 0.6912\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.67900\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5603 - val_loss: 0.6847\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.67900\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5549 - val_loss: 0.6728\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.67900 to 0.67283, saving model to Radiant_NN_5.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5465 - val_loss: 0.6798\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67283\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5428 - val_loss: 0.6762\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67283\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5346 - val_loss: 0.6842\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67283\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5339 - val_loss: 0.6886\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67283\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5324 - val_loss: 0.6819\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67283\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5356 - val_loss: 0.6777\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.67283\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5203 - val_loss: 0.6848\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67283\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5173 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67283\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5159 - val_loss: 0.6870\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67283\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5113 - val_loss: 0.6870\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67283\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5061 - val_loss: 0.6878\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67283\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 13s 10ms/step - loss: 0.5106 - val_loss: 0.6862\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67283\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5018 - val_loss: 0.6829\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67283\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5065 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67283\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4981 - val_loss: 0.6841\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67283\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4885 - val_loss: 0.6945\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67283\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4982 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67283\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4884 - val_loss: 0.6837\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67283\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4757 - val_loss: 0.6712\n",
            "\n",
            "Epoch 00050: val_loss improved from 0.67283 to 0.67125, saving model to Radiant_NN_5.h5\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4839 - val_loss: 0.6906\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67125\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4782 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67125\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4758 - val_loss: 0.6904\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67125\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4712 - val_loss: 0.6857\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67125\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4858 - val_loss: 0.6945\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67125\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4685 - val_loss: 0.6836\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67125\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4660 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67125\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4638 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67125\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4576 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67125\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4602 - val_loss: 0.6854\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67125\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4628 - val_loss: 0.6911\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67125\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4554 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67125\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4535 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67125\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4507 - val_loss: 0.7082\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67125\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4429 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67125\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4477 - val_loss: 0.6976\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67125\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4440 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67125\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4501 - val_loss: 0.7095\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67125\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4370 - val_loss: 0.6774\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67125\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.4303 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67125\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4324 - val_loss: 0.6846\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67125\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4351 - val_loss: 0.7086\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67125\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4293 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67125\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4382 - val_loss: 0.7149\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67125\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4342 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67125\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4283 - val_loss: 0.6795\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67125\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4247 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67125\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4302 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67125\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4251 - val_loss: 0.7013\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67125\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4247 - val_loss: 0.6897\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67125\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4166 - val_loss: 0.6916\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67125\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4264 - val_loss: 0.7227\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67125\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4225 - val_loss: 0.7072\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67125\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4216 - val_loss: 0.6922\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67125\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4189 - val_loss: 0.6906\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67125\n",
            "Epoch 00085: early stopping\n",
            "fold #5 Log Loss: 0.6712457885096151\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1718 - val_loss: 0.9295\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.92952, saving model to Radiant_NN_6.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.9464 - val_loss: 0.8853\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.92952 to 0.88530, saving model to Radiant_NN_6.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9005 - val_loss: 0.8517\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.88530 to 0.85170, saving model to Radiant_NN_6.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8679 - val_loss: 0.8122\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.85170 to 0.81220, saving model to Radiant_NN_6.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8252 - val_loss: 0.7911\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81220 to 0.79108, saving model to Radiant_NN_6.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7989 - val_loss: 0.7828\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79108 to 0.78284, saving model to Radiant_NN_6.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7873 - val_loss: 0.7799\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78284 to 0.77989, saving model to Radiant_NN_6.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7662 - val_loss: 0.7712\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77989 to 0.77121, saving model to Radiant_NN_6.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.7434 - val_loss: 0.7592\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77121 to 0.75915, saving model to Radiant_NN_6.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7363 - val_loss: 0.7522\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.75915 to 0.75222, saving model to Radiant_NN_6.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.7240 - val_loss: 0.7359\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.75222 to 0.73594, saving model to Radiant_NN_6.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7033 - val_loss: 0.7355\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73594 to 0.73548, saving model to Radiant_NN_6.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7015 - val_loss: 0.7389\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.73548\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6921 - val_loss: 0.7312\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.73548 to 0.73118, saving model to Radiant_NN_6.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6746 - val_loss: 0.7293\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.73118 to 0.72925, saving model to Radiant_NN_6.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6582 - val_loss: 0.7127\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.72925 to 0.71272, saving model to Radiant_NN_6.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6465 - val_loss: 0.7138\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.71272\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6467 - val_loss: 0.7264\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.71272\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6366 - val_loss: 0.7083\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.71272 to 0.70828, saving model to Radiant_NN_6.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6232 - val_loss: 0.7100\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70828\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6231 - val_loss: 0.7058\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70828 to 0.70580, saving model to Radiant_NN_6.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6222 - val_loss: 0.7060\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70580\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5997 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.70580 to 0.69650, saving model to Radiant_NN_6.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5980 - val_loss: 0.6957\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69650 to 0.69573, saving model to Radiant_NN_6.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5923 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69573\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5959 - val_loss: 0.6988\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69573\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5860 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69573\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5843 - val_loss: 0.6873\n",
            "\n",
            "Epoch 00028: val_loss improved from 0.69573 to 0.68726, saving model to Radiant_NN_6.h5\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5719 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68726\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5659 - val_loss: 0.7063\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.68726\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5544 - val_loss: 0.6959\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68726\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5536 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68726\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5467 - val_loss: 0.6835\n",
            "\n",
            "Epoch 00033: val_loss improved from 0.68726 to 0.68352, saving model to Radiant_NN_6.h5\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 13s 11ms/step - loss: 0.5459 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68352\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.5494 - val_loss: 0.6952\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68352\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5306 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68352\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.5308 - val_loss: 0.6878\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68352\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5230 - val_loss: 0.6886\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68352\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5224 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68352\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5163 - val_loss: 0.6928\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68352\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5145 - val_loss: 0.6844\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68352\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5140 - val_loss: 0.7071\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68352\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5178 - val_loss: 0.7008\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68352\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5061 - val_loss: 0.6990\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68352\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5035 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68352\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4970 - val_loss: 0.6859\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.68352\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4990 - val_loss: 0.7049\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.68352\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4922 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.68352\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4896 - val_loss: 0.7115\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.68352\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4871 - val_loss: 0.6932\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.68352\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4854 - val_loss: 0.6905\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.68352\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4834 - val_loss: 0.6921\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.68352\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4759 - val_loss: 0.6861\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.68352\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4857 - val_loss: 0.6919\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.68352\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4778 - val_loss: 0.7039\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.68352\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4740 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.68352\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4676 - val_loss: 0.7043\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.68352\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4717 - val_loss: 0.7066\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.68352\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4813 - val_loss: 0.6941\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.68352\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4655 - val_loss: 0.6917\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.68352\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4624 - val_loss: 0.6940\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.68352\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4640 - val_loss: 0.6872\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.68352\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4626 - val_loss: 0.7165\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.68352\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4566 - val_loss: 0.7214\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.68352\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4555 - val_loss: 0.7062\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.68352\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4443 - val_loss: 0.7105\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.68352\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4400 - val_loss: 0.7120\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.68352\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4439 - val_loss: 0.7143\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.68352\n",
            "Epoch 00068: early stopping\n",
            "fold #6 Log Loss: 0.6835178368980162\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1725 - val_loss: 0.9360\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.93598, saving model to Radiant_NN_7.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.9546 - val_loss: 0.8726\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.93598 to 0.87256, saving model to Radiant_NN_7.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8935 - val_loss: 0.8316\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.87256 to 0.83161, saving model to Radiant_NN_7.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8644 - val_loss: 0.8239\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.83161 to 0.82385, saving model to Radiant_NN_7.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.8355 - val_loss: 0.7809\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.82385 to 0.78090, saving model to Radiant_NN_7.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8013 - val_loss: 0.7854\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.78090\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7890 - val_loss: 0.7727\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78090 to 0.77274, saving model to Radiant_NN_7.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7677 - val_loss: 0.7602\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77274 to 0.76020, saving model to Radiant_NN_7.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7553 - val_loss: 0.7506\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.76020 to 0.75055, saving model to Radiant_NN_7.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7340 - val_loss: 0.7486\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.75055 to 0.74856, saving model to Radiant_NN_7.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7247 - val_loss: 0.7400\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.74856 to 0.74001, saving model to Radiant_NN_7.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7083 - val_loss: 0.7336\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.74001 to 0.73357, saving model to Radiant_NN_7.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6995 - val_loss: 0.7342\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.73357\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6860 - val_loss: 0.7168\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.73357 to 0.71682, saving model to Radiant_NN_7.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6815 - val_loss: 0.7108\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71682 to 0.71079, saving model to Radiant_NN_7.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6689 - val_loss: 0.7204\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.71079\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6604 - val_loss: 0.7148\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.71079\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6494 - val_loss: 0.7128\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.71079\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 12s 10ms/step - loss: 0.6408 - val_loss: 0.7048\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.71079 to 0.70478, saving model to Radiant_NN_7.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.6303 - val_loss: 0.7054\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70478\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.6267 - val_loss: 0.7027\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.70478 to 0.70273, saving model to Radiant_NN_7.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6195 - val_loss: 0.7037\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70273\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.6127 - val_loss: 0.6972\n",
            "\n",
            "Epoch 00023: val_loss improved from 0.70273 to 0.69717, saving model to Radiant_NN_7.h5\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6011 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69717 to 0.69394, saving model to Radiant_NN_7.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5937 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.69394\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5870 - val_loss: 0.7077\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.69394\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5856 - val_loss: 0.7005\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69394\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5792 - val_loss: 0.7023\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69394\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5761 - val_loss: 0.6875\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.69394 to 0.68747, saving model to Radiant_NN_7.h5\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5628 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.68747 to 0.68308, saving model to Radiant_NN_7.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5617 - val_loss: 0.6797\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.68308 to 0.67971, saving model to Radiant_NN_7.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5622 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.67971\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5646 - val_loss: 0.6915\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.67971\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5476 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.67971\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5480 - val_loss: 0.6837\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.67971\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5374 - val_loss: 0.6865\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.67971\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5435 - val_loss: 0.6773\n",
            "\n",
            "Epoch 00037: val_loss improved from 0.67971 to 0.67733, saving model to Radiant_NN_7.h5\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5397 - val_loss: 0.6883\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.67733\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5319 - val_loss: 0.6813\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.67733\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5310 - val_loss: 0.7054\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.67733\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5280 - val_loss: 0.7010\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.67733\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5158 - val_loss: 0.6812\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.67733\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5158 - val_loss: 0.6811\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.67733\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5140 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67733\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5014 - val_loss: 0.6885\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67733\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5069 - val_loss: 0.7048\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67733\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4999 - val_loss: 0.6765\n",
            "\n",
            "Epoch 00047: val_loss improved from 0.67733 to 0.67650, saving model to Radiant_NN_7.h5\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4986 - val_loss: 0.6926\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67650\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4970 - val_loss: 0.6950\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67650\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.5002 - val_loss: 0.6831\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67650\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 0.5005 - val_loss: 0.6759\n",
            "\n",
            "Epoch 00051: val_loss improved from 0.67650 to 0.67593, saving model to Radiant_NN_7.h5\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4823 - val_loss: 0.6938\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67593\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4811 - val_loss: 0.6958\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67593\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4788 - val_loss: 0.7009\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67593\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4839 - val_loss: 0.6936\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67593\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4799 - val_loss: 0.6945\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67593\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4722 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67593\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4779 - val_loss: 0.6809\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67593\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4747 - val_loss: 0.6732\n",
            "\n",
            "Epoch 00059: val_loss improved from 0.67593 to 0.67318, saving model to Radiant_NN_7.h5\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4706 - val_loss: 0.7014\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67318\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4571 - val_loss: 0.6884\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67318\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4640 - val_loss: 0.6967\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67318\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4597 - val_loss: 0.7004\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67318\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4572 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67318\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4591 - val_loss: 0.6905\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67318\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4613 - val_loss: 0.7068\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67318\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4504 - val_loss: 0.7054\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67318\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4503 - val_loss: 0.7179\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67318\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4484 - val_loss: 0.6992\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67318\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4478 - val_loss: 0.7149\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67318\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4426 - val_loss: 0.7060\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67318\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4480 - val_loss: 0.6878\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67318\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4358 - val_loss: 0.6896\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67318\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4393 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67318\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4395 - val_loss: 0.7051\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67318\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4297 - val_loss: 0.6999\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67318\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4356 - val_loss: 0.7093\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67318\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4253 - val_loss: 0.7001\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67318\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4318 - val_loss: 0.7085\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67318\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4281 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67318\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4237 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67318\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4231 - val_loss: 0.6885\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67318\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4316 - val_loss: 0.6960\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67318\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4204 - val_loss: 0.7218\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67318\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4254 - val_loss: 0.7086\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67318\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4193 - val_loss: 0.7128\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67318\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4179 - val_loss: 0.7224\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67318\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4194 - val_loss: 0.7264\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67318\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4107 - val_loss: 0.7071\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67318\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4146 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67318\n",
            "Epoch 91/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4077 - val_loss: 0.7312\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67318\n",
            "Epoch 92/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4089 - val_loss: 0.6839\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67318\n",
            "Epoch 93/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4119 - val_loss: 0.7099\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67318\n",
            "Epoch 94/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4172 - val_loss: 0.7083\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67318\n",
            "Epoch 00094: early stopping\n",
            "fold #7 Log Loss: 0.6731839316986237\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 12s 9ms/step - loss: 1.1773 - val_loss: 0.9156\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91563, saving model to Radiant_NN_8.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.9465 - val_loss: 0.8576\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91563 to 0.85756, saving model to Radiant_NN_8.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8900 - val_loss: 0.8287\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.85756 to 0.82873, saving model to Radiant_NN_8.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8586 - val_loss: 0.8074\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.82873 to 0.80740, saving model to Radiant_NN_8.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8268 - val_loss: 0.7887\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.80740 to 0.78867, saving model to Radiant_NN_8.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8012 - val_loss: 0.7822\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.78867 to 0.78218, saving model to Radiant_NN_8.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.7821 - val_loss: 0.7746\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78218 to 0.77465, saving model to Radiant_NN_8.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7681 - val_loss: 0.7592\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77465 to 0.75922, saving model to Radiant_NN_8.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7490 - val_loss: 0.7474\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.75922 to 0.74740, saving model to Radiant_NN_8.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7258 - val_loss: 0.7377\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.74740 to 0.73768, saving model to Radiant_NN_8.h5\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7118 - val_loss: 0.7318\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.73768 to 0.73184, saving model to Radiant_NN_8.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7061 - val_loss: 0.7222\n",
            "\n",
            "Epoch 00012: val_loss improved from 0.73184 to 0.72217, saving model to Radiant_NN_8.h5\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6888 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.72217\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6837 - val_loss: 0.7140\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72217 to 0.71404, saving model to Radiant_NN_8.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6722 - val_loss: 0.7047\n",
            "\n",
            "Epoch 00015: val_loss improved from 0.71404 to 0.70474, saving model to Radiant_NN_8.h5\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6520 - val_loss: 0.7077\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.70474\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6488 - val_loss: 0.7061\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70474\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6345 - val_loss: 0.7197\n",
            "\n",
            "Epoch 00018: val_loss did not improve from 0.70474\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6393 - val_loss: 0.6983\n",
            "\n",
            "Epoch 00019: val_loss improved from 0.70474 to 0.69828, saving model to Radiant_NN_8.h5\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6279 - val_loss: 0.7046\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.69828\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6100 - val_loss: 0.6948\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.69828 to 0.69479, saving model to Radiant_NN_8.h5\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6107 - val_loss: 0.6979\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.69479\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6116 - val_loss: 0.6980\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.69479\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5964 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.69479 to 0.68694, saving model to Radiant_NN_8.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5905 - val_loss: 0.6890\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.68694\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5842 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.68694\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5774 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.68694\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5775 - val_loss: 0.6912\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.68694\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5705 - val_loss: 0.6873\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.68694\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5623 - val_loss: 0.6825\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.68694 to 0.68246, saving model to Radiant_NN_8.h5\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5632 - val_loss: 0.6874\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 0.68246\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5521 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.68246\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5492 - val_loss: 0.6857\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.68246\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5511 - val_loss: 0.6856\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.68246\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5440 - val_loss: 0.6859\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.68246\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5396 - val_loss: 0.6872\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.68246\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5402 - val_loss: 0.6847\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68246\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5287 - val_loss: 0.6896\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68246\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5175 - val_loss: 0.6927\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68246\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5155 - val_loss: 0.6902\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68246\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5228 - val_loss: 0.6840\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68246\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5258 - val_loss: 0.6968\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68246\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5105 - val_loss: 0.6784\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.68246 to 0.67842, saving model to Radiant_NN_8.h5\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5028 - val_loss: 0.6861\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.67842\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5031 - val_loss: 0.6816\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.67842\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5045 - val_loss: 0.6943\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.67842\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4899 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67842\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4941 - val_loss: 0.6834\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67842\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5049 - val_loss: 0.6905\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67842\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4889 - val_loss: 0.6832\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67842\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4913 - val_loss: 0.6981\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67842\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4833 - val_loss: 0.6851\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67842\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4829 - val_loss: 0.6744\n",
            "\n",
            "Epoch 00053: val_loss improved from 0.67842 to 0.67445, saving model to Radiant_NN_8.h5\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4751 - val_loss: 0.6942\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67445\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4828 - val_loss: 0.7011\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67445\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4716 - val_loss: 0.6929\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67445\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4717 - val_loss: 0.6960\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67445\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4676 - val_loss: 0.6869\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67445\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4662 - val_loss: 0.6791\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67445\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4590 - val_loss: 0.6970\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67445\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4622 - val_loss: 0.6748\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67445\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4560 - val_loss: 0.6778\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67445\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4633 - val_loss: 0.6894\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67445\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4551 - val_loss: 0.6877\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67445\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4541 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67445\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4501 - val_loss: 0.6941\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67445\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4457 - val_loss: 0.6947\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67445\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4500 - val_loss: 0.6888\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67445\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4421 - val_loss: 0.6744\n",
            "\n",
            "Epoch 00069: val_loss improved from 0.67445 to 0.67437, saving model to Radiant_NN_8.h5\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4494 - val_loss: 0.6774\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67437\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4444 - val_loss: 0.6965\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67437\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4369 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67437\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4465 - val_loss: 0.6924\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67437\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4343 - val_loss: 0.6901\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67437\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4369 - val_loss: 0.6870\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67437\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4335 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67437\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4383 - val_loss: 0.7017\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67437\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4307 - val_loss: 0.6914\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67437\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4330 - val_loss: 0.7034\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67437\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4275 - val_loss: 0.6891\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67437\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4261 - val_loss: 0.6916\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67437\n",
            "Epoch 82/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4222 - val_loss: 0.7193\n",
            "\n",
            "Epoch 00082: val_loss did not improve from 0.67437\n",
            "Epoch 83/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4228 - val_loss: 0.6985\n",
            "\n",
            "Epoch 00083: val_loss did not improve from 0.67437\n",
            "Epoch 84/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4226 - val_loss: 0.7075\n",
            "\n",
            "Epoch 00084: val_loss did not improve from 0.67437\n",
            "Epoch 85/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4145 - val_loss: 0.7049\n",
            "\n",
            "Epoch 00085: val_loss did not improve from 0.67437\n",
            "Epoch 86/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4195 - val_loss: 0.6925\n",
            "\n",
            "Epoch 00086: val_loss did not improve from 0.67437\n",
            "Epoch 87/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4179 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00087: val_loss did not improve from 0.67437\n",
            "Epoch 88/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4086 - val_loss: 0.6930\n",
            "\n",
            "Epoch 00088: val_loss did not improve from 0.67437\n",
            "Epoch 89/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4235 - val_loss: 0.7016\n",
            "\n",
            "Epoch 00089: val_loss did not improve from 0.67437\n",
            "Epoch 90/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4076 - val_loss: 0.7163\n",
            "\n",
            "Epoch 00090: val_loss did not improve from 0.67437\n",
            "Epoch 91/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4126 - val_loss: 0.6939\n",
            "\n",
            "Epoch 00091: val_loss did not improve from 0.67437\n",
            "Epoch 92/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4114 - val_loss: 0.7015\n",
            "\n",
            "Epoch 00092: val_loss did not improve from 0.67437\n",
            "Epoch 93/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4134 - val_loss: 0.6963\n",
            "\n",
            "Epoch 00093: val_loss did not improve from 0.67437\n",
            "Epoch 94/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4107 - val_loss: 0.6920\n",
            "\n",
            "Epoch 00094: val_loss did not improve from 0.67437\n",
            "Epoch 95/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4009 - val_loss: 0.7114\n",
            "\n",
            "Epoch 00095: val_loss did not improve from 0.67437\n",
            "Epoch 96/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3971 - val_loss: 0.6868\n",
            "\n",
            "Epoch 00096: val_loss did not improve from 0.67437\n",
            "Epoch 97/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3999 - val_loss: 0.7171\n",
            "\n",
            "Epoch 00097: val_loss did not improve from 0.67437\n",
            "Epoch 98/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3950 - val_loss: 0.7337\n",
            "\n",
            "Epoch 00098: val_loss did not improve from 0.67437\n",
            "Epoch 99/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3990 - val_loss: 0.7012\n",
            "\n",
            "Epoch 00099: val_loss did not improve from 0.67437\n",
            "Epoch 100/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3995 - val_loss: 0.7064\n",
            "\n",
            "Epoch 00100: val_loss did not improve from 0.67437\n",
            "Epoch 101/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4019 - val_loss: 0.7078\n",
            "\n",
            "Epoch 00101: val_loss did not improve from 0.67437\n",
            "Epoch 102/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3981 - val_loss: 0.6963\n",
            "\n",
            "Epoch 00102: val_loss did not improve from 0.67437\n",
            "Epoch 103/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.3914 - val_loss: 0.7212\n",
            "\n",
            "Epoch 00103: val_loss did not improve from 0.67437\n",
            "Epoch 104/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.3968 - val_loss: 0.6916\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00104: val_loss did not improve from 0.67437\n",
            "Epoch 00104: early stopping\n",
            "fold #8 Log Loss: 0.6743749209174766\n",
            "Epoch 1/300\n",
            "1226/1226 [==============================] - 17s 12ms/step - loss: 1.1755 - val_loss: 0.9140\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.91405, saving model to Radiant_NN_9.h5\n",
            "Epoch 2/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.9549 - val_loss: 0.8633\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.91405 to 0.86334, saving model to Radiant_NN_9.h5\n",
            "Epoch 3/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.8932 - val_loss: 0.8507\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.86334 to 0.85066, saving model to Radiant_NN_9.h5\n",
            "Epoch 4/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.8505 - val_loss: 0.8135\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.85066 to 0.81351, saving model to Radiant_NN_9.h5\n",
            "Epoch 5/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.8322 - val_loss: 0.7942\n",
            "\n",
            "Epoch 00005: val_loss improved from 0.81351 to 0.79423, saving model to Radiant_NN_9.h5\n",
            "Epoch 6/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.8045 - val_loss: 0.7805\n",
            "\n",
            "Epoch 00006: val_loss improved from 0.79423 to 0.78054, saving model to Radiant_NN_9.h5\n",
            "Epoch 7/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7828 - val_loss: 0.7719\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.78054 to 0.77191, saving model to Radiant_NN_9.h5\n",
            "Epoch 8/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7618 - val_loss: 0.7717\n",
            "\n",
            "Epoch 00008: val_loss improved from 0.77191 to 0.77172, saving model to Radiant_NN_9.h5\n",
            "Epoch 9/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7459 - val_loss: 0.7480\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.77172 to 0.74803, saving model to Radiant_NN_9.h5\n",
            "Epoch 10/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7315 - val_loss: 0.7545\n",
            "\n",
            "Epoch 00010: val_loss did not improve from 0.74803\n",
            "Epoch 11/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.7127 - val_loss: 0.7381\n",
            "\n",
            "Epoch 00011: val_loss improved from 0.74803 to 0.73814, saving model to Radiant_NN_9.h5\n",
            "Epoch 12/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.7007 - val_loss: 0.7437\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.73814\n",
            "Epoch 13/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6913 - val_loss: 0.7270\n",
            "\n",
            "Epoch 00013: val_loss improved from 0.73814 to 0.72698, saving model to Radiant_NN_9.h5\n",
            "Epoch 14/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6827 - val_loss: 0.7233\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.72698 to 0.72326, saving model to Radiant_NN_9.h5\n",
            "Epoch 15/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.6712 - val_loss: 0.7323\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.72326\n",
            "Epoch 16/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6558 - val_loss: 0.7078\n",
            "\n",
            "Epoch 00016: val_loss improved from 0.72326 to 0.70785, saving model to Radiant_NN_9.h5\n",
            "Epoch 17/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6484 - val_loss: 0.7116\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.70785\n",
            "Epoch 18/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6462 - val_loss: 0.7019\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.70785 to 0.70188, saving model to Radiant_NN_9.h5\n",
            "Epoch 19/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6308 - val_loss: 0.7174\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.70188\n",
            "Epoch 20/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6202 - val_loss: 0.7039\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.70188\n",
            "Epoch 21/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.6175 - val_loss: 0.7103\n",
            "\n",
            "Epoch 00021: val_loss did not improve from 0.70188\n",
            "Epoch 22/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6166 - val_loss: 0.7090\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.70188\n",
            "Epoch 23/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.6026 - val_loss: 0.7022\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.70188\n",
            "Epoch 24/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5935 - val_loss: 0.7003\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.70188 to 0.70034, saving model to Radiant_NN_9.h5\n",
            "Epoch 25/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5880 - val_loss: 0.6933\n",
            "\n",
            "Epoch 00025: val_loss improved from 0.70034 to 0.69327, saving model to Radiant_NN_9.h5\n",
            "Epoch 26/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5766 - val_loss: 0.6923\n",
            "\n",
            "Epoch 00026: val_loss improved from 0.69327 to 0.69227, saving model to Radiant_NN_9.h5\n",
            "Epoch 27/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5742 - val_loss: 0.7101\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.69227\n",
            "Epoch 28/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5723 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.69227\n",
            "Epoch 29/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5673 - val_loss: 0.7025\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 0.69227\n",
            "Epoch 30/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5644 - val_loss: 0.6971\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 0.69227\n",
            "Epoch 31/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5586 - val_loss: 0.6900\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.69227 to 0.69003, saving model to Radiant_NN_9.h5\n",
            "Epoch 32/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5478 - val_loss: 0.6984\n",
            "\n",
            "Epoch 00032: val_loss did not improve from 0.69003\n",
            "Epoch 33/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5455 - val_loss: 0.6937\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.69003\n",
            "Epoch 34/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5456 - val_loss: 0.7030\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.69003\n",
            "Epoch 35/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5406 - val_loss: 0.6943\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.69003\n",
            "Epoch 36/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5381 - val_loss: 0.6867\n",
            "\n",
            "Epoch 00036: val_loss improved from 0.69003 to 0.68668, saving model to Radiant_NN_9.h5\n",
            "Epoch 37/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5209 - val_loss: 0.6969\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.68668\n",
            "Epoch 38/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5205 - val_loss: 0.6916\n",
            "\n",
            "Epoch 00038: val_loss did not improve from 0.68668\n",
            "Epoch 39/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5170 - val_loss: 0.6881\n",
            "\n",
            "Epoch 00039: val_loss did not improve from 0.68668\n",
            "Epoch 40/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5205 - val_loss: 0.7106\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.68668\n",
            "Epoch 41/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5128 - val_loss: 0.7055\n",
            "\n",
            "Epoch 00041: val_loss did not improve from 0.68668\n",
            "Epoch 42/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5083 - val_loss: 0.6903\n",
            "\n",
            "Epoch 00042: val_loss did not improve from 0.68668\n",
            "Epoch 43/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.5075 - val_loss: 0.6921\n",
            "\n",
            "Epoch 00043: val_loss did not improve from 0.68668\n",
            "Epoch 44/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.5094 - val_loss: 0.6946\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.68668\n",
            "Epoch 45/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.5046 - val_loss: 0.6983\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.68668\n",
            "Epoch 46/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4937 - val_loss: 0.6797\n",
            "\n",
            "Epoch 00046: val_loss improved from 0.68668 to 0.67970, saving model to Radiant_NN_9.h5\n",
            "Epoch 47/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4938 - val_loss: 0.6982\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.67970\n",
            "Epoch 48/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4976 - val_loss: 0.6913\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.67970\n",
            "Epoch 49/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4868 - val_loss: 0.7018\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.67970\n",
            "Epoch 50/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4844 - val_loss: 0.6909\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.67970\n",
            "Epoch 51/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4756 - val_loss: 0.6977\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.67970\n",
            "Epoch 52/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4773 - val_loss: 0.6837\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.67970\n",
            "Epoch 53/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4804 - val_loss: 0.6962\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.67970\n",
            "Epoch 54/300\n",
            "1226/1226 [==============================] - 15s 12ms/step - loss: 0.4739 - val_loss: 0.6871\n",
            "\n",
            "Epoch 00054: val_loss did not improve from 0.67970\n",
            "Epoch 55/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4677 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00055: val_loss did not improve from 0.67970\n",
            "Epoch 56/300\n",
            "1226/1226 [==============================] - 14s 12ms/step - loss: 0.4687 - val_loss: 0.6860\n",
            "\n",
            "Epoch 00056: val_loss did not improve from 0.67970\n",
            "Epoch 57/300\n",
            "1226/1226 [==============================] - 14s 11ms/step - loss: 0.4611 - val_loss: 0.7097\n",
            "\n",
            "Epoch 00057: val_loss did not improve from 0.67970\n",
            "Epoch 58/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4679 - val_loss: 0.7092\n",
            "\n",
            "Epoch 00058: val_loss did not improve from 0.67970\n",
            "Epoch 59/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4641 - val_loss: 0.6906\n",
            "\n",
            "Epoch 00059: val_loss did not improve from 0.67970\n",
            "Epoch 60/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4583 - val_loss: 0.7122\n",
            "\n",
            "Epoch 00060: val_loss did not improve from 0.67970\n",
            "Epoch 61/300\n",
            "1226/1226 [==============================] - 10s 9ms/step - loss: 0.4572 - val_loss: 0.6893\n",
            "\n",
            "Epoch 00061: val_loss did not improve from 0.67970\n",
            "Epoch 62/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4650 - val_loss: 0.6978\n",
            "\n",
            "Epoch 00062: val_loss did not improve from 0.67970\n",
            "Epoch 63/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4503 - val_loss: 0.7153\n",
            "\n",
            "Epoch 00063: val_loss did not improve from 0.67970\n",
            "Epoch 64/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4551 - val_loss: 0.6968\n",
            "\n",
            "Epoch 00064: val_loss did not improve from 0.67970\n",
            "Epoch 65/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4414 - val_loss: 0.6837\n",
            "\n",
            "Epoch 00065: val_loss did not improve from 0.67970\n",
            "Epoch 66/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4462 - val_loss: 0.7052\n",
            "\n",
            "Epoch 00066: val_loss did not improve from 0.67970\n",
            "Epoch 67/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4473 - val_loss: 0.6973\n",
            "\n",
            "Epoch 00067: val_loss did not improve from 0.67970\n",
            "Epoch 68/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4403 - val_loss: 0.7121\n",
            "\n",
            "Epoch 00068: val_loss did not improve from 0.67970\n",
            "Epoch 69/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4429 - val_loss: 0.7000\n",
            "\n",
            "Epoch 00069: val_loss did not improve from 0.67970\n",
            "Epoch 70/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4307 - val_loss: 0.7027\n",
            "\n",
            "Epoch 00070: val_loss did not improve from 0.67970\n",
            "Epoch 71/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4346 - val_loss: 0.7084\n",
            "\n",
            "Epoch 00071: val_loss did not improve from 0.67970\n",
            "Epoch 72/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4424 - val_loss: 0.6993\n",
            "\n",
            "Epoch 00072: val_loss did not improve from 0.67970\n",
            "Epoch 73/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4442 - val_loss: 0.7055\n",
            "\n",
            "Epoch 00073: val_loss did not improve from 0.67970\n",
            "Epoch 74/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4300 - val_loss: 0.7065\n",
            "\n",
            "Epoch 00074: val_loss did not improve from 0.67970\n",
            "Epoch 75/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4255 - val_loss: 0.7049\n",
            "\n",
            "Epoch 00075: val_loss did not improve from 0.67970\n",
            "Epoch 76/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4298 - val_loss: 0.7125\n",
            "\n",
            "Epoch 00076: val_loss did not improve from 0.67970\n",
            "Epoch 77/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4265 - val_loss: 0.7075\n",
            "\n",
            "Epoch 00077: val_loss did not improve from 0.67970\n",
            "Epoch 78/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4261 - val_loss: 0.7262\n",
            "\n",
            "Epoch 00078: val_loss did not improve from 0.67970\n",
            "Epoch 79/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4232 - val_loss: 0.7151\n",
            "\n",
            "Epoch 00079: val_loss did not improve from 0.67970\n",
            "Epoch 80/300\n",
            "1226/1226 [==============================] - 10s 8ms/step - loss: 0.4218 - val_loss: 0.7031\n",
            "\n",
            "Epoch 00080: val_loss did not improve from 0.67970\n",
            "Epoch 81/300\n",
            "1226/1226 [==============================] - 11s 9ms/step - loss: 0.4207 - val_loss: 0.7067\n",
            "Restoring model weights from the end of the best epoch.\n",
            "\n",
            "Epoch 00081: val_loss did not improve from 0.67970\n",
            "Epoch 00081: early stopping\n",
            "fold #9 Log Loss: 0.6797006113405268\n",
            "Full Log loss 0.6764459017773456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T09:47:51.431975Z",
          "iopub.status.busy": "2021-09-30T09:47:51.429174Z",
          "iopub.status.idle": "2021-09-30T09:47:51.476465Z",
          "shell.execute_reply": "2021-09-30T09:47:51.476898Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.936064Z"
        },
        "id": "d300e381",
        "papermill": {
          "duration": 44.394383,
          "end_time": "2021-09-30T09:47:51.477041",
          "exception": false,
          "start_time": "2021-09-30T09:47:07.082658",
          "status": "completed"
        },
        "tags": [],
        "outputId": "a550112e-2325-4679-d7f5-168385e5acc9"
      },
      "source": [
        "print('NN Attention LOG LOSS :',log_loss(y_train,y_oof)) "
      ],
      "id": "d300e381",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NN Attention LOG LOSS : 0.6764458726404721\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T09:50:48.584598Z",
          "iopub.status.busy": "2021-09-30T09:50:48.583794Z",
          "iopub.status.idle": "2021-09-30T09:50:48.605946Z",
          "shell.execute_reply": "2021-09-30T09:50:48.606337Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.939863Z"
        },
        "id": "513a262a",
        "papermill": {
          "duration": 44.152329,
          "end_time": "2021-09-30T09:50:48.606473",
          "exception": false,
          "start_time": "2021-09-30T09:50:04.454144",
          "status": "completed"
        },
        "tags": [],
        "outputId": "40599105-d725-4abe-d8da-5304a35489f3"
      },
      "source": [
        "# In this part we format the DataFrame to have column names and order similar to the sample submission file. \n",
        "pred_df = pd.DataFrame(y_test)\n",
        "pred_df = pred_df.rename(columns={\n",
        "    0:'Crop_ID_1',\n",
        "    1:'Crop_ID_2', \n",
        "    2:'Crop_ID_3',\n",
        "    3:'Crop_ID_4',\n",
        "    4:'Crop_ID_5',\n",
        "    5:'Crop_ID_6',\n",
        "    6:'Crop_ID_7',\n",
        "    7:'Crop_ID_8',\n",
        "    8:'Crop_ID_9'\n",
        "})\n",
        "pred_df['field_id'] = Test['field_id'].astype('int').values\n",
        "pred_df = pred_df[['field_id', 'Crop_ID_1', 'Crop_ID_2', 'Crop_ID_3', 'Crop_ID_4', 'Crop_ID_5', 'Crop_ID_6', 'Crop_ID_7', 'Crop_ID_8', 'Crop_ID_9']]\n",
        "pred_df.head()"
      ],
      "id": "513a262a",
      "execution_count": null,
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>field_id</th>\n",
              "      <th>Crop_ID_1</th>\n",
              "      <th>Crop_ID_2</th>\n",
              "      <th>Crop_ID_3</th>\n",
              "      <th>Crop_ID_4</th>\n",
              "      <th>Crop_ID_5</th>\n",
              "      <th>Crop_ID_6</th>\n",
              "      <th>Crop_ID_7</th>\n",
              "      <th>Crop_ID_8</th>\n",
              "      <th>Crop_ID_9</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>30</td>\n",
              "      <td>0.041120</td>\n",
              "      <td>0.309569</td>\n",
              "      <td>0.009097</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>0.157301</td>\n",
              "      <td>0.451127</td>\n",
              "      <td>0.026184</td>\n",
              "      <td>2.467516e-03</td>\n",
              "      <td>0.003034</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>39</td>\n",
              "      <td>0.710284</td>\n",
              "      <td>0.205948</td>\n",
              "      <td>0.010874</td>\n",
              "      <td>0.023593</td>\n",
              "      <td>0.046379</td>\n",
              "      <td>0.002728</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>7.473998e-06</td>\n",
              "      <td>0.000033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>49</td>\n",
              "      <td>0.021883</td>\n",
              "      <td>0.448379</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000026</td>\n",
              "      <td>0.000337</td>\n",
              "      <td>0.498564</td>\n",
              "      <td>0.026004</td>\n",
              "      <td>4.699463e-03</td>\n",
              "      <td>0.000066</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>54</td>\n",
              "      <td>0.000009</td>\n",
              "      <td>0.000520</td>\n",
              "      <td>0.000347</td>\n",
              "      <td>0.998624</td>\n",
              "      <td>0.000455</td>\n",
              "      <td>0.000025</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>5.715823e-08</td>\n",
              "      <td>0.000017</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>56</td>\n",
              "      <td>0.837220</td>\n",
              "      <td>0.029454</td>\n",
              "      <td>0.002752</td>\n",
              "      <td>0.000264</td>\n",
              "      <td>0.003474</td>\n",
              "      <td>0.031217</td>\n",
              "      <td>0.090314</td>\n",
              "      <td>5.212712e-03</td>\n",
              "      <td>0.000092</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   field_id  Crop_ID_1  Crop_ID_2  Crop_ID_3  Crop_ID_4  Crop_ID_5  Crop_ID_6  \\\n",
              "0        30   0.041120   0.309569   0.009097   0.000099   0.157301   0.451127   \n",
              "1        39   0.710284   0.205948   0.010874   0.023593   0.046379   0.002728   \n",
              "2        49   0.021883   0.448379   0.000042   0.000026   0.000337   0.498564   \n",
              "3        54   0.000009   0.000520   0.000347   0.998624   0.000455   0.000025   \n",
              "4        56   0.837220   0.029454   0.002752   0.000264   0.003474   0.031217   \n",
              "\n",
              "   Crop_ID_7     Crop_ID_8  Crop_ID_9  \n",
              "0   0.026184  2.467516e-03   0.003034  \n",
              "1   0.000153  7.473998e-06   0.000033  \n",
              "2   0.026004  4.699463e-03   0.000066  \n",
              "3   0.000003  5.715823e-08   0.000017  \n",
              "4   0.090314  5.212712e-03   0.000092  "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "execution": {
          "iopub.execute_input": "2021-09-30T09:52:17.362031Z",
          "iopub.status.busy": "2021-09-30T09:52:17.361185Z",
          "iopub.status.idle": "2021-09-30T09:52:17.987966Z",
          "shell.execute_reply": "2021-09-30T09:52:17.986977Z",
          "shell.execute_reply.started": "2021-09-25T13:09:43.941611Z"
        },
        "id": "a2fffca4",
        "papermill": {
          "duration": 44.845692,
          "end_time": "2021-09-30T09:52:17.988115",
          "exception": false,
          "start_time": "2021-09-30T09:51:33.142423",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "# Write the predicted probabilites to a csv for submission\n",
        "pred_df.to_csv('S2_NNAttention.csv', index=False)"
      ],
      "id": "a2fffca4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4e7406c6",
        "papermill": {
          "duration": 44.562891,
          "end_time": "2021-09-30T09:53:46.550658",
          "exception": false,
          "start_time": "2021-09-30T09:53:01.987767",
          "status": "completed"
        },
        "tags": []
      },
      "source": [
        "np.save('S2_oof_NNAttention.npy',y_oof)"
      ],
      "id": "4e7406c6",
      "execution_count": null,
      "outputs": []
    }
  ]
}